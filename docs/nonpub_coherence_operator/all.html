<!-- pages/about/01_overview.js -->
<doc-view>

<h2 id="_get_going">Get Going</h2>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/about/03_quickstart"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-rocket</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Quick Start</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Quick start guide to running your first Coherence cluster using the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/installation/01_installation"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-save</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Install</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Installing and running the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/applications/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">cloud_upload</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Build and Deploy Applications</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Deploying Coherence Applications using the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/troubleshooting/01_trouble-shooting"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-question-circle</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Troubleshooting</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Hints and tips to troubleshoot common issues.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>

<h2 id="_in_depth">In Depth</h2>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/scaling/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-balance-scale</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Scaling</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Safe scaling of Coherence deployments the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/coherence/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-cogs</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Configure Coherence</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configuring Coherence behaviour.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-cog</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Configure the JVM</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configure the behaviour of the JVM.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/ports/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">control_camera</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Expose Ports & Services</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configure services to expose ports provided by the application.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/metrics/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">speed</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Metrics</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Enabling and working with Metrics in the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/management/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">fa-stethoscope</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Management and Diagnostics</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Management and Diagnostics in the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/logging/010_overview"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">find_in_page</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Logging</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Viewing and managing log files within using the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/about/04_coherence_spec"><div class="card__link-hover"/>
</router-link>
<v-layout align-center justify-center class="">
<v-avatar size="150px">
<v-icon class="xxx-large">widgets</v-icon>
</v-avatar>
</v-layout>
<div class="px-3">
<v-divider class="indigo lighten-4"/>
</div>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Coherence CRD Reference</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Coherence CRD reference guide.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</doc-view>
<!-- pages/about/02_introduction.js -->
<doc-view>

<h2 id="_what_is_the_coherence_operator">What is the Coherence Operator?</h2>
<div class="section">
<p>The Coherence Operator is a <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Kubernetes Operator</a> that
is used to manage <a id="" title="" target="_blank" href="https://oracle.github.io/coherence">Oracle Coherence</a> clusters in Kubernetes.
The Coherence Operator takes on the tasks of that human DevOps resource might carry out when managing Coherence clusters,
such as configuration, installation, safe scaling, management and metrics.</p>

<p>The Coherence Operator is a Go based application built using the <a id="" title="" target="_blank" href="https://github.com/operator-framework/operator-sdk">Operator SDK</a>.
It is distributed as a Docker image and Helm chart for easy installation and configuration.</p>

</div>

<h2 id="_why_use_the_coherence_kubernetes_operator">Why use the Coherence Kubernetes Operator</h2>
<div class="section">
<p>Using the Coherence Operator to manage Coherence clusters running in Kubernetes has many advantages over just deploying
and running clusters with the resources provided by Kubernetes.
Coherence can be treated as just another library that your application depends on and uses and hence, a Coherence
application can run in Kubernetes without requiring the Operator, but in this case there are
a number of things that the DevOps team for an application would need to build or do manually.</p>


<h3 id="_cluster_discovery">Cluster Discovery</h3>
<div class="section">
<p>JVMs that run as Coherence cluster members need to discover the other members of the cluster.
This is discussed in the <router-link to="/coherence/070_wka">Coherence Well Known Addressing</router-link> section of the documentation.
When using the Operator the well known addressing configuration for clusters is managed automatically to allow a Coherence
deployment to create its own cluster or to join with other deployments to form larger clusters.</p>

</div>

<h3 id="_better_fault_tolerant_data_distribution">Better Fault Tolerant Data Distribution</h3>
<div class="section">
<p>The Operator configures the Coherence site and rack properties for cluster members based on Kubernetes Node topology
labels. This allows Coherence to better distribute data across sites when a Kubernetes cluster spans availability domains.</p>

</div>

<h3 id="_safe_scaling">Safe Scaling</h3>
<div class="section">
<p>When scaling down a Coherence cluster, care must be taken to ensure that there will be no data loss.
This typically means scaling down by a single Pod at a time and waiting for the cluster to become "safe" before scaling
down the next Pod.
The Operator has built in functionality to do this, so scaling a Coherence cluster is as simple as scaling any other
Kubernetes Deployment or StatefulSet.</p>

</div>

<h3 id="_autoscaling">Autoscaling</h3>
<div class="section">
<p>Alongside safe scaling, because the Coherence CRD supports the Kubernetes scale sub-resource it is possible to configure
the Kubernetes Horizontal Pod Autoscaler to scale Coherence
clusters based on metrics.</p>

</div>

<h3 id="_readiness_probes">Readiness Probes</h3>
<div class="section">
<p>The Operator has an understanding of when a Coherence JVM is "ready", so it configures a readiness probe that k8s will
use to signal whether a Pod is ready or not.</p>

</div>

<h3 id="_persistence">Persistence</h3>
<div class="section">
<p>Using the Operator makes it simple to configure and use Coherence Persistence, storing data on Kubernetes Persistent
Volumes to allow state to be maintained between cluster restarts.</p>

</div>

<h3 id="_graceful_shutdown">Graceful Shutdown</h3>
<div class="section">
<p>When a Coherence cluster is deployed with persistence enabled, the Operator will gracefully shutdown a cluster by suspending
services before stopping all the Pods.
This ensures that all persistence files are properly closed and allows for quicker recovery and restart of the cluster.
Without the Operator, if a cluster is shutdown, typically by removing the controlling StatefulSet from Kubernetes then
the Pods will be shutdown but not all at the same time.
It is obviously impossible for k8s to kill all the Pods at the exact same instant in time. As some Pods die the remaining
storage enabled Pods will be trying to recover data for the lost Pods, this can cause a lot of needles work and moving of
data over the network. It is much cleaner to suspend all the services before shutdown.</p>

</div>

<h3 id="_simpler_configuration">Simpler Configuration</h3>
<div class="section">
<p>The Coherence CRD is designed to make the more commonly used configuration parameters for Coherence, and the JVM simpler
to configure. The Coherence CRD is simple to use, in fact none of its fields are mandatory, so an application can be
deployed with nothing more than a name, and a container image.</p>

</div>

<h3 id="_consistency">Consistency</h3>
<div class="section">
<p>By using the Operator to manage Coherence clusters all clusters are configured and managed the same way making it easier
for DevOps to manage multiple clusters and applications.</p>

</div>

<h3 id="_expertise">Expertise</h3>
<div class="section">
<p>The Operator has been built and tested by the Coherence engineering team, who understand Coherence and the various scenarios
and edge cases that can occur when managing Coherence clusters at scale in Kubernetes.</p>

</div>
</div>

<h2 id="_coherence_clusters">Coherence Clusters</h2>
<div class="section">
<p>A Coherence cluster is a number of distributed Java Virtual Machines (JVMs) that communicate to form a single coherent cluster.
In Kubernetes, this concept can be related to a number of Pods that form a single cluster.
In each <code>Pod</code> is a JVM running a Coherence <code>DefaultCacheServer</code>, or a custom application using Coherence.</p>

<p>The operator uses a Kubernetes Custom Resource Definition (CRD) to represent a group of members in a Coherence cluster.
Typically, a deployment would be used to configure one or more members of a specific role in a cluster.
Every field in the <code>Coherence</code> CRD <code>Spec</code> is optional, so a simple cluster can be defined in  yaml as:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: my-cluster <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">In this case the <code>metadata.name</code> field in the <code>Coherence</code> resource yaml will be used as the Coherence cluster name.</li>
</ul>
<p>The operator will use default values for fields that have not been entered, so the above yaml will create
a Coherence deployment using a <code>StatefulSet</code> with a replica count of three, which means that will be three storage
enabled Coherence <code>Pods</code>.</p>

<p>See the <router-link to="/about/04_coherence_spec">Coherence CRD spec</router-link> page for details of all the fields in the CRD.</p>

<p>In the above example no <code>spec.image</code> field has been set, so the Operator will use a publicly available Coherence CE
image as its default. These images are meant for demos, POCs and experimentation, but for a production application you
should build your own image.</p>

</div>

<h2 id="_using_commercial_coherence_versions">Using Commercial Coherence Versions</h2>
<div class="section">
<div class="admonition note">
<p class="admonition-inline">Whilst the Coherence CE version can be freely deployed anywhere, if your application image uses a commercial
version of Oracle Coherence then you are responsible for making sure your deployment has been properly licensed.</p>
</div>
<p>Oracle&#8217;s current policy is that a license will be required for each Kubernetes Node that images are to be pulled to.
While an image exists on a node it is effectively the same as having installed the software on that node.</p>

<p>One way to ensure that the Pods of a Coherence deployment only get scheduled onto nodes that meet the
license requirement is to configure Pod scheduling, for example a node selector. Node selectors, and other scheduling,
is simple to configure in the <code>Coherence</code> CRD, see the <router-link to="/other/090_pod_scheduling">scheduling documentation</router-link></p>

<p>For example, if a commercial Coherence license exists such that a sub-set of nodes in a Kubernetes cluster
have been covered by the license then those nodes could all be given a label, e.g. <code>coherenceLicense=true</code></p>

<p>When creating a <code>Coherence</code> deployment specify a node selector to match the label:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: my-app:1.0.0         <span class="conum" data-value="1" />
  nodeSelector:
    coherenceLicense: 'true'  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>my-app:1.0.0</code> image contains a commercial Coherence version.</li>
<li data-value="2">The <code>nodeSelector</code> will ensure Pods only get scheduled to nodes with the <code>coherenceLicense=true</code> label.</li>
</ul>
<p>There are other ways to configure Pod scheduling supported by the Coherence Operator (such as taints and tolerations)
and there are alternative ways to restrict nodes that Pods can be schedule to, for example a namespace in kubernetes
can be restricted to a sub-set of the cluster&#8217;s nodes. Using a node selector as described above is probably the
simplest approach.</p>

</div>
</doc-view>
<!-- pages/about/03_quickstart.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Quick Start</dt>
<dd slot="desc"><p>This guide is a simple set of steps to install the Coherence Operator and then use that
to install a simple Coherence cluster.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_prerequisites">Prerequisites</h2>
<div class="section">
<p>Ensure that the <router-link to="/installation/01_installation">Coherence Operator prerequisites</router-link> are available.</p>

</div>

<h2 id="_1_install_the_coherence_operator">1. Install the Coherence Operator</h2>
<div class="section">
<p>If you want the default Coherence Operator installation then the simplest solution is use <code>kubectl</code> to apply the manifests from the Operator release.</p>

<pre
lang="bash"

>kubectl apply -f https://github.com/oracle/coherence-operator/releases/download/v3.2.2/coherence-operator.yaml</pre>

<p>This will create a namespace called <code>coherence</code> and install the Operator into it along with all the required <code>ClusterRole</code> and <code>RoleBinding</code> resources. The <code>coherence</code> namespace can be changed by downloading and editing the yaml file.</p>

<div class="admonition note">
<p class="admonition-inline">Because the <code>coherence-operator.yaml</code> manifest also creates the namespace, the corresponding <code>kubectl delete</code> command will <em>remove the namespace and everything deployed to it</em>! If you do not want this behaviour you should edit the <code>coherence-operator.yaml</code> to remove the namespace section from the start of the file.</p>
</div>

<h3 id="_alternatively_install_using_helm">Alternatively Install Using Helm</h3>
<div class="section">
<p>Alternatively you can install the Operator using the Helm chart.</p>


<h4 id="_add_the_coherence_operator_helm_repository">Add the Coherence Operator Helm repository</h4>
<div class="section">
<p>Add the Coherence Operator Helm repo to your local Helm.</p>

<pre
lang="bash"

>helm repo add coherence https://oracle.github.io/coherence-operator/charts

helm repo update</pre>

<div class="admonition note">
<p class="admonition-inline">To avoid confusion, the URL <code><a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/charts">https://oracle.github.io/coherence-operator/charts</a></code> is a Helm repo, it is not a web site you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised.</p>
</div>
</div>

<h4 id="_install_the_coherence_operator_helm_chart">Install the Coherence Operator Helm chart</h4>
<div class="section">
<pre
lang="bash"
title="helm v3 install command"
>helm install  \
    --namespace &lt;namespace&gt; \
    &lt;release-name&gt; \
    coherence/coherence-operator</pre>

<p>e.g. if the Kubernetes namespace is <code>coherence-test</code> the command would be:</p>

<pre
lang="bash"
title="helm v3 install command"
>helm install --namespace coherence-test  operator coherence/coherence-operator</pre>

<p>or with Helm v2</p>

<pre
lang="bash"

>helm install --namespace coherence-test  --name operator coherence/coherence-operator</pre>

<p>See the <router-link to="/installation/01_installation">full install guide</router-link> for more details.</p>

</div>
</div>
</div>

<h2 id="_2_install_a_coherence_deployment">2. Install a Coherence Deployment</h2>
<div class="section">
<p>Ensure that the Coherence images can be pulled by the Kubernetes cluster,
see <router-link to="/installation/04_obtain_coherence_images">Obtain Coherence Images</router-link>.
By default, a <code>Coherence</code> resource will use the OSS Coherence CE image from Docker Hub.
If a different image is to be used the image name will need to be specified in the <code>Coherence</code> yaml,
see <router-link to="/applications/010_overview">Setting the Application Image</router-link> for documentation on how to
specify a different images to use.</p>


<h3 id="_2_1_install_a_coherence_resource_using_the_minimal_required_configuration">2.1 Install a Coherence resource using the minimal required configuration.</h3>
<div class="section">
<p>The minimal required yaml to create a <code>Coherence</code> resource is shown below.</p>

<pre
lang="yaml"
title="my-deployment.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: my-deployment <span class="conum" data-value="1" /></pre>

<p>The only required field is <code>metadata.name</code> which will be used as the Coherence cluster name, in this case <code>my-deployment</code></p>

<pre
lang="bash"

>kubectl -n &lt;namespace&gt; apply -f my-deployment.yaml</pre>

<div class="admonition note">
<p class="admonition-inline">Use the same namespace that the operator was installed into,
e.g. if the namespace is <code>coherence-test</code> the command would be
<code>kubectl -n coherence-test create -f my-deployment.yaml</code></p>
</div>
</div>

<h3 id="_2_2_list_the_coherence_resources">2.2 List the Coherence Resources</h3>
<div class="section">
<p>After installing the <code>my-deployment.yaml</code> above here should be a single <code>Coherence</code> resource  named <code>my-deployment</code> in the Coherence Operator namespace.</p>

<pre
lang="bash"

>kubectl -n &lt;namespace&gt; get coherence</pre>

<p>or alternatively using the <code>Coherence</code> CRD a short name of <code>coh</code></p>

<pre
lang="bash"

>kubectl -n &lt;namespace&gt; get coh</pre>

<p>e.g. if the namespace is <code>coherence-test</code> the command would be <code>kubectl -n coherence-test get coherence</code></p>

<pre
lang="bash"

>NAME                                                  AGE
coherence.coherence.oracle.com/my-deployment   19s</pre>

</div>

<h3 id="_2_3_list_all_of_the_pods_for_the_coherence_resource">2.3 List all of the <code>Pods</code> for the Coherence resource.</h3>
<div class="section">
<p>The Coherence Operator applies a <code>coherenceDeployment</code> label to all <code>Pods</code> so this label can be used with the <code>kubectl</code> command to find <code>Pods</code> for a <code>CoherenceCoherence</code> resource.</p>

<pre
lang="bash"

>kubectl -n &lt;namespace&gt; get pod -l coherenceDeployment=my-deployment</pre>

<p>e.g. if the namespace is <code>coherence</code> the command would be:
<code>kubectl -n coherence get pod -l coherenceDeployment=my-deployment</code></p>

<pre
lang="bash"

>NAME              READY   STATUS    RESTARTS   AGE
my-deployment-0   1/1     Running   0          2m58s
my-deployment-1   1/1     Running   0          2m58s
my-deployment-2   1/1     Running   0          2m58s</pre>

</div>

<h3 id="_2_3_list_all_the_pods_for_the_coherence_cluster">2.3 List all the <code>Pods</code> for the Coherence cluster.</h3>
<div class="section">
<p>The Coherence Operator applies a <code>coherenceCluster</code> label to all <code>Pods</code>, so this label can be used with the <code>kubectl</code>
command to find all <code>Pods</code> for a Coherence cluster, which will be made up of multiple <code>Coherence</code> resources.</p>

<pre
lang="bash"

>kubectl -n &lt;namespace&gt; get pod -l coherenceCluster=my-cluster</pre>

<p>e.g. If there is a cluster named <code>my-cluster</code> made up of two <code>Coherence</code> resources in the namespace
<code>coherence-test</code>, one named <code>storage</code> and one named <code>front-end</code>
then the <code>kubectl</code> command to list all Pods for the cluster would be:</p>

<pre
lang="bash"

>kubectl -n coherence-test get pod -l coherenceCluster=my-cluster</pre>

<p>The result of which might look something like this</p>

<pre
lang="bash"

>NAME          READY   STATUS    RESTARTS   AGE
storage-0     1/1     Running   0          2m58s
storage-1     1/1     Running   0          2m58s
storage-2     1/1     Running   0          2m58s
front-end-0   1/1     Running   0          2m58s
front-end-1   1/1     Running   0          2m58s
front-end-2   1/1     Running   0          2m58s</pre>

</div>
</div>

<h2 id="_3_scale_the_coherence_cluster">3. Scale the Coherence Cluster</h2>
<div class="section">

<h3 id="_3_1_use_kubectl_to_scale_up">3.1 Use kubectl to Scale Up</h3>
<div class="section">
<p>Using the <code>kubectl scale</code> command a specific <code>Coherence</code> resource can be scaled up or down.</p>

<pre
lang="bash"

>kubectl -n &lt;namespace&gt; scale coherence/my-deployment --replicas=6</pre>

<p>e.g. if the namespace is <code>coherence-test</code> the command would be:
<code>kubectl -n coherence scale coherence/my-deployment --replicas=6</code></p>

</div>
</div>
</doc-view>
<!-- pages/about/04_coherence_spec.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Coherence Operator API Docs</dt>
<dd slot="desc"><p>A reference guide to the Coherence Operator CRD types.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_coherence_operator_api_docs">Coherence Operator API Docs</h2>
<div class="section">
<p>This is a reference for the Coherence Operator API types.
These are all the types and fields that are used in the Coherence CRD.</p>

<div class="admonition tip">
<p class="admonition-inline">This document was generated from comments in the Go structs in the pkg/api/ directory.</p>
</div>

<h3 id="_table_of_contents">Table of Contents</h3>
<div class="section">
<ul class="ulist">
<li>
<p><router-link to="#_coherenceresourcespec" @click.native="this.scrollFix('#_coherenceresourcespec')">CoherenceResourceSpec</router-link></p>

</li>
<li>
<p><router-link to="#_applicationspec" @click.native="this.scrollFix('#_applicationspec')">ApplicationSpec</router-link></p>

</li>
<li>
<p><router-link to="#_cloudnativebuildpackspec" @click.native="this.scrollFix('#_cloudnativebuildpackspec')">CloudNativeBuildPackSpec</router-link></p>

</li>
<li>
<p><router-link to="#_coherencespec" @click.native="this.scrollFix('#_coherencespec')">CoherenceSpec</router-link></p>

</li>
<li>
<p><router-link to="#_coherencetracingspec" @click.native="this.scrollFix('#_coherencetracingspec')">CoherenceTracingSpec</router-link></p>

</li>
<li>
<p><router-link to="#_coherencewkaspec" @click.native="this.scrollFix('#_coherencewkaspec')">CoherenceWKASpec</router-link></p>

</li>
<li>
<p><router-link to="#_configmapvolumespec" @click.native="this.scrollFix('#_configmapvolumespec')">ConfigMapVolumeSpec</router-link></p>

</li>
<li>
<p><router-link to="#_imagespec" @click.native="this.scrollFix('#_imagespec')">ImageSpec</router-link></p>

</li>
<li>
<p><router-link to="#_jvmspec" @click.native="this.scrollFix('#_jvmspec')">JVMSpec</router-link></p>

</li>
<li>
<p><router-link to="#_jvmdebugspec" @click.native="this.scrollFix('#_jvmdebugspec')">JvmDebugSpec</router-link></p>

</li>
<li>
<p><router-link to="#_jvmgarbagecollectorspec" @click.native="this.scrollFix('#_jvmgarbagecollectorspec')">JvmGarbageCollectorSpec</router-link></p>

</li>
<li>
<p><router-link to="#_jvmjmxmpspec" @click.native="this.scrollFix('#_jvmjmxmpspec')">JvmJmxmpSpec</router-link></p>

</li>
<li>
<p><router-link to="#_jvmmemoryspec" @click.native="this.scrollFix('#_jvmmemoryspec')">JvmMemorySpec</router-link></p>

</li>
<li>
<p><router-link to="#_jvmoutofmemoryspec" @click.native="this.scrollFix('#_jvmoutofmemoryspec')">JvmOutOfMemorySpec</router-link></p>

</li>
<li>
<p><router-link to="#_localobjectreference" @click.native="this.scrollFix('#_localobjectreference')">LocalObjectReference</router-link></p>

</li>
<li>
<p><router-link to="#_namedportspec" @click.native="this.scrollFix('#_namedportspec')">NamedPortSpec</router-link></p>

</li>
<li>
<p><router-link to="#_networkspec" @click.native="this.scrollFix('#_networkspec')">NetworkSpec</router-link></p>

</li>
<li>
<p><router-link to="#_persistencespec" @click.native="this.scrollFix('#_persistencespec')">PersistenceSpec</router-link></p>

</li>
<li>
<p><router-link to="#_persistentstoragespec" @click.native="this.scrollFix('#_persistentstoragespec')">PersistentStorageSpec</router-link></p>

</li>
<li>
<p><router-link to="#_poddnsconfig" @click.native="this.scrollFix('#_poddnsconfig')">PodDNSConfig</router-link></p>

</li>
<li>
<p><router-link to="#_portspecwithssl" @click.native="this.scrollFix('#_portspecwithssl')">PortSpecWithSSL</router-link></p>

</li>
<li>
<p><router-link to="#_probe" @click.native="this.scrollFix('#_probe')">Probe</router-link></p>

</li>
<li>
<p><router-link to="#_probehandler" @click.native="this.scrollFix('#_probehandler')">ProbeHandler</router-link></p>

</li>
<li>
<p><router-link to="#_readinessprobespec" @click.native="this.scrollFix('#_readinessprobespec')">ReadinessProbeSpec</router-link></p>

</li>
<li>
<p><router-link to="#_resource" @click.native="this.scrollFix('#_resource')">Resource</router-link></p>

</li>
<li>
<p><router-link to="#_resources" @click.native="this.scrollFix('#_resources')">Resources</router-link></p>

</li>
<li>
<p><router-link to="#_sslspec" @click.native="this.scrollFix('#_sslspec')">SSLSpec</router-link></p>

</li>
<li>
<p><router-link to="#_scalingspec" @click.native="this.scrollFix('#_scalingspec')">ScalingSpec</router-link></p>

</li>
<li>
<p><router-link to="#_secretvolumespec" @click.native="this.scrollFix('#_secretvolumespec')">SecretVolumeSpec</router-link></p>

</li>
<li>
<p><router-link to="#_servicemonitorspec" @click.native="this.scrollFix('#_servicemonitorspec')">ServiceMonitorSpec</router-link></p>

</li>
<li>
<p><router-link to="#_servicespec" @click.native="this.scrollFix('#_servicespec')">ServiceSpec</router-link></p>

</li>
<li>
<p><router-link to="#_startquorum" @click.native="this.scrollFix('#_startquorum')">StartQuorum</router-link></p>

</li>
<li>
<p><router-link to="#_startquorumstatus" @click.native="this.scrollFix('#_startquorumstatus')">StartQuorumStatus</router-link></p>

</li>
<li>
<p><router-link to="#_coherence" @click.native="this.scrollFix('#_coherence')">Coherence</router-link></p>

</li>
<li>
<p><router-link to="#_coherencelist" @click.native="this.scrollFix('#_coherencelist')">CoherenceList</router-link></p>

</li>
<li>
<p><router-link to="#_coherenceresourcestatus" @click.native="this.scrollFix('#_coherenceresourcestatus')">CoherenceResourceStatus</router-link></p>

</li>
</ul>
</div>

<h3 id="_coherenceresourcespec">CoherenceResourceSpec</h3>
<div class="section">
<p>CoherenceResourceSpec defines the specification of a Coherence resource. A Coherence resource is typically one or more Pods that perform the same functionality, for example storage members.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>image</code></td>
<td class="">The name of the image. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images">https://kubernetes.io/docs/concepts/containers/images</a></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>imagePullPolicy</code></td>
<td class="">Image pull policy. One of Always, Never, IfNotPresent. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images#updating-images">https://kubernetes.io/docs/concepts/containers/images#updating-images</a></td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#pullpolicy-v1-core">corev1.PullPolicy</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>imagePullSecrets</code></td>
<td class="">ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. If specified, these secrets will be passed to individual puller implementations for them to use. For example, in the case of docker, only DockerConfig type secrets are honored. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod">https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod</a></td>
<td class=""><code>[]<router-link to="#_localobjectreference" @click.native="this.scrollFix('#_localobjectreference')">LocalObjectReference</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>replicas</code></td>
<td class="">The desired number of cluster members of this deployment. This is a pointer to distinguish between explicit zero and not specified. If not specified a default value of 3 will be used. This field cannot be negative.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>cluster</code></td>
<td class="">The optional name of the Coherence cluster that this Coherence resource belongs to. If this value is set the Pods controlled by this Coherence resource will form a cluster with other Pods controlled by Coherence resources with the same cluster name. If not set the Coherence resource&#8217;s name will be used as the cluster name.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>role</code></td>
<td class="">The name of the role that this deployment represents in a Coherence cluster. This value will be used to set the Coherence role property for all members of this role</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>appLabel</code></td>
<td class="">An optional app label to apply to resources created for this deployment. This is useful for example to apply an app label for use by Istio. This field follows standard Kubernetes label syntax.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>versionLabel</code></td>
<td class="">An optional version label to apply to resources created for this deployment. This is useful for example to apply an version label for use by Istio. This field follows standard Kubernetes label syntax.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>coherence</code></td>
<td class="">The optional settings specific to Coherence functionality.</td>
<td class=""><code>&#42;<router-link to="#_coherencespec" @click.native="this.scrollFix('#_coherencespec')">CoherenceSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>application</code></td>
<td class="">The optional application specific settings.</td>
<td class=""><code>&#42;<router-link to="#_applicationspec" @click.native="this.scrollFix('#_applicationspec')">ApplicationSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>jvm</code></td>
<td class="">The JVM specific options</td>
<td class=""><code>&#42;<router-link to="#_jvmspec" @click.native="this.scrollFix('#_jvmspec')">JVMSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>ports</code></td>
<td class="">Ports specifies additional port mappings for the Pod and additional Services for those ports.</td>
<td class=""><code>[]<router-link to="#_namedportspec" @click.native="this.scrollFix('#_namedportspec')">NamedPortSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>scaling</code></td>
<td class="">The configuration to control safe scaling.</td>
<td class=""><code>&#42;<router-link to="#_scalingspec" @click.native="this.scrollFix('#_scalingspec')">ScalingSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>suspendProbe</code></td>
<td class="">The configuration of the probe used to signal that services must be suspended before a deployment is stopped.</td>
<td class=""><code>&#42;<router-link to="#_probe" @click.native="this.scrollFix('#_probe')">Probe</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>suspendServicesOnShutdown</code></td>
<td class="">A flag controlling whether storage enabled cache services in this deployment will be suspended before the deployment is shutdown or scaled to zero. The action of suspending storage enabled services when the whole deployment is being stopped ensures that cache services with persistence enabled will shutdown cleanly without the possibility of Coherence trying to recover and re-balance partitions as Pods are stopped. The default value if not specified is true.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>suspendServiceTimeout</code></td>
<td class="">SuspendServiceTimeout sets the number of seconds to wait for the service suspend call to return (the default is 60 seconds)</td>
<td class=""><code>&#42;int</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>startQuorum</code></td>
<td class="">StartQuorum controls the start-up order of this Coherence resource in relation to other Coherence resources.</td>
<td class=""><code>[]<router-link to="#_startquorum" @click.native="this.scrollFix('#_startquorum')">StartQuorum</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>env</code></td>
<td class="">Env is additional environment variable mappings that will be passed to the Coherence container in the Pod. To specify extra variables add them as name value pairs the same as they would be added to a Pod containers spec.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#envvar-v1-core">corev1.EnvVar</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>labels</code></td>
<td class="">The extra labels to add to the all of the Pods in this deployments. Labels here will add to or override those defined for the cluster. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</a></td>
<td class=""><code>map[string]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>annotations</code></td>
<td class="">Annotations are free-form yaml that will be added to the store release as annotations Any annotations should be placed BELOW this annotations: key. For example if we wanted to include annotations for Prometheus it would look like this:<br>
<br>
annotations:<br>
  prometheus.io/scrape: \"true\" +<br>
  prometheus.io/port: \"2408\"<br></td>
<td class=""><code>map[string]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>initContainers</code></td>
<td class="">List of additional initialization containers to add to the deployment&#8217;s Pod. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</a></td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#container-v1-core">corev1.Container</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>sideCars</code></td>
<td class="">List of additional side-car containers to add to the deployment&#8217;s Pod.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#container-v1-core">corev1.Container</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>configMapVolumes</code></td>
<td class="">A list of ConfigMaps to add as volumes. Each entry in the list will be added as a ConfigMap Volume to the deployment&#8217;s Pods and as a VolumeMount to all of the containers and init-containers in the Pod.<br>
see: <router-link to="#misc_pod_settings/050_configmap_volumes.adoc" @click.native="this.scrollFix('#misc_pod_settings/050_configmap_volumes.adoc')">Add ConfigMap Volumes</router-link></td>
<td class=""><code>[]<router-link to="#_configmapvolumespec" @click.native="this.scrollFix('#_configmapvolumespec')">ConfigMapVolumeSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>secretVolumes</code></td>
<td class="">A list of Secrets to add as volumes. Each entry in the list will be added as a Secret Volume to the deployment&#8217;s Pods and as a VolumeMount to all of the containers and init-containers in the Pod.<br>
see: <router-link to="#misc_pod_settings/020_secret_volumes.adoc" @click.native="this.scrollFix('#misc_pod_settings/020_secret_volumes.adoc')">Add Secret Volumes</router-link></td>
<td class=""><code>[]<router-link to="#_secretvolumespec" @click.native="this.scrollFix('#_secretvolumespec')">SecretVolumeSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>volumes</code></td>
<td class="">Volumes defines extra volume mappings that will be added to the Coherence Pod.<br>
  The content of this yaml should match the normal k8s volumes section of a Pod definition +<br>
  as described in <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/storage/volumes/">https://kubernetes.io/docs/concepts/storage/volumes/</a><br></td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#volume-v1-core">corev1.Volume</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>volumeClaimTemplates</code></td>
<td class="">VolumeClaimTemplates defines extra PVC mappings that will be added to the Coherence Pod.<br>
  The content of this yaml should match the normal k8s volumeClaimTemplates section of a Pod definition +<br>
  as described in <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a><br></td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#persistentvolumeclaim-v1-core">corev1.PersistentVolumeClaim</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>volumeMounts</code></td>
<td class="">VolumeMounts defines extra volume mounts to map to the additional volumes or PVCs declared above<br>
  in store.volumes and store.volumeClaimTemplates<br></td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#volumemount-v1-core">corev1.VolumeMount</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>healthPort</code></td>
<td class="">The port that the health check endpoint will bind to.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>readinessProbe</code></td>
<td class="">The readiness probe config to be used for the Pods in this deployment. ref: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></td>
<td class=""><code>&#42;<router-link to="#_readinessprobespec" @click.native="this.scrollFix('#_readinessprobespec')">ReadinessProbeSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>livenessProbe</code></td>
<td class="">The liveness probe config to be used for the Pods in this deployment. ref: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></td>
<td class=""><code>&#42;<router-link to="#_readinessprobespec" @click.native="this.scrollFix('#_readinessprobespec')">ReadinessProbeSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>resources</code></td>
<td class="">Resources is the optional resource requests and limits for the containers<br>
 ref: <a id="" title="" target="_blank" href="http://kubernetes.io/docs/user-guide/compute-resources/">http://kubernetes.io/docs/user-guide/compute-resources/</a> +<br>
<br>
By default the cpu requests is set to zero and the cpu limit set to 32. This is because it appears that K8s defaults cpu to one and since Java 10 the JVM now correctly picks up cgroup cpu limits then the JVM will only see one cpu. By setting resources.requests.cpu=0 and resources.limits.cpu=32 it ensures that the JVM will see the either the number of cpus on the host if this is &#8656; 32 or the JVM will see 32 cpus if the host has &gt; 32 cpus. The limit is set to zero so that there is no hard-limit applied.<br>
<br>
No default memory limits are applied.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#resourcerequirements-v1-core">corev1.ResourceRequirements</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>affinity</code></td>
<td class="">Affinity controls Pod scheduling preferences.<br>
  ref: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity</a><br></td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#affinity-v1-core">corev1.Affinity</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>nodeSelector</code></td>
<td class="">NodeSelector is the Node labels for pod assignment<br>
  ref: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector</a><br></td>
<td class=""><code>map[string]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>tolerations</code></td>
<td class="">Tolerations is for nodes that have taints on them.<br>
  Useful if you want to dedicate nodes to just run the coherence container +<br>
For example:<br>
  tolerations: +<br>
  - key: \"key\" +<br>
    operator: \"Equal\" +<br>
    value: \"value\" +<br>
    effect: \"NoSchedule\" +<br>
<br>
  ref: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</a><br></td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#toleration-v1-core">corev1.Toleration</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>securityContext</code></td>
<td class="">SecurityContext is the PodSecurityContext that will be added to all of the Pods in this deployment. See: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">https://kubernetes.io/docs/tasks/configure-pod-container/security-context/</a></td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#podsecuritycontext-v1-core">corev1.PodSecurityContext</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>shareProcessNamespace</code></td>
<td class="">Share a single process namespace between all of the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set. Optional: Default to false.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>hostIPC</code></td>
<td class="">Use the host&#8217;s ipc namespace. Optional: Default to false.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>network</code></td>
<td class="">Configure various networks and DNS settings for Pods in this rolw.</td>
<td class=""><code>&#42;<router-link to="#_networkspec" @click.native="this.scrollFix('#_networkspec')">NetworkSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>coherenceUtils</code></td>
<td class="">The configuration for the Coherence utils image</td>
<td class=""><code>&#42;<router-link to="#_imagespec" @click.native="this.scrollFix('#_imagespec')">ImageSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>serviceAccountName</code></td>
<td class="">The name to use for the service account to use when RBAC is enabled The role bindings must already have been created as this chart does not create them it just sets the serviceAccountName value in the Pod spec.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>automountServiceAccountToken</code></td>
<td class="">Whether or not to auto-mount the Kubernetes API credentials for a service account</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>operatorRequestTimeout</code></td>
<td class="">The timeout to apply to REST requests made back to the Operator from Coherence Pods. These requests are typically to obtain site and rack information for the Pod.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>haBeforeUpdate</code></td>
<td class="">Whether or not to perform a StatusHA test on the cluster before performing an update or deletion. This field can be set to false to force through an update even when a Coherence deployment is in an unstable state. The default is true, to always check for StatusHA before updating a Coherence deployment.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_applicationspec">ApplicationSpec</h3>
<div class="section">
<p>ApplicationSpec is the specification of the application deployed into the Coherence.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>type</code></td>
<td class="">The application type to execute. This field would be set if using the Coherence Graal image and running a none-Java application. For example if the application was a Node application this field would be set to \"node\". The default is to run a plain Java application.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>main</code></td>
<td class="">Class is the Coherence container main class.  The default value is com.tangosol.net.DefaultCacheServer. If the application type is non-Java this would be the name of the corresponding language specific runnable, for example if the application type is \"node\" the main may be a Javascript file.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>args</code></td>
<td class="">Args is the optional arguments to pass to the main class.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>workingDir</code></td>
<td class="">The application folder in the custom artifacts Docker image containing application artifacts. This will effectively become the working directory of the Coherence container. If not set the application directory default value is \"/app\".</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>cloudNativeBuildPack</code></td>
<td class="">Optional settings that may be configured if using a Cloud Native Buildpack Image. For example an image build with the Spring Boot Maven/Gradle plugin. See: <a id="" title="" target="_blank" href="https://github.com/paketo-buildpacks/spring-boot">https://github.com/paketo-buildpacks/spring-boot</a> and <a id="" title="" target="_blank" href="https://buildpacks.io/">https://buildpacks.io/</a></td>
<td class=""><code>&#42;<router-link to="#_cloudnativebuildpackspec" @click.native="this.scrollFix('#_cloudnativebuildpackspec')">CloudNativeBuildPackSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>springBootFatJar</code></td>
<td class="">SpringBootFatJar is the full path name to the Spring Boot fat jar if the application image has been built by just adding a Spring Boot fat jar to the image. If this field is set then the application will be run by executing this jar. For example, if this field is \"/app/libs/foo.jar\" the command line will be \"java -jar app/libs/foo.jar\"</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_cloudnativebuildpackspec">CloudNativeBuildPackSpec</h3>
<div class="section">
<p>CloudNativeBuildPackSpec is the configuration when using a Cloud Native Buildpack Image. For example an image build with the Spring Boot Maven/Gradle plugin. See: <a id="" title="" target="_blank" href="https://github.com/paketo-buildpacks/spring-boot">https://github.com/paketo-buildpacks/spring-boot</a> and <a id="" title="" target="_blank" href="https://buildpacks.io/">https://buildpacks.io/</a></p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">Enable or disable buildpack detection. The operator will automatically detect Cloud Native Buildpack images but if this auto-detection fails to work correctly for a specific image then this field can be set to true to signify that the image is a buildpack image or false to signify that it is not a buildpack image.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>launcher</code></td>
<td class="">&#160;</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_coherencespec">CoherenceSpec</h3>
<div class="section">
<p>CoherenceSpec is the section of the CRD configures settings specific to Coherence.<br>
see: <router-link to="#coherence_settings/010_overview.adoc" @click.native="this.scrollFix('#coherence_settings/010_overview.adoc')">Coherence Configuration</router-link></p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>cacheConfig</code></td>
<td class="">CacheConfig is the name of the cache configuration file to use<br>
see: <router-link to="#coherence_settings/030_cache_config.adoc" @click.native="this.scrollFix('#coherence_settings/030_cache_config.adoc')">Configure Cache Config File</router-link></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>overrideConfig</code></td>
<td class="">OverrideConfig is name of the Coherence operational configuration override file, the default is tangosol-coherence-override.xml<br>
see: <router-link to="#coherence_settings/040_override_file.adoc" @click.native="this.scrollFix('#coherence_settings/040_override_file.adoc')">Configure Operational Config File</router-link></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>storageEnabled</code></td>
<td class="">A boolean flag indicating whether members of this deployment are storage enabled. This value will set the corresponding coherence.distributed.localstorage System property. If not specified the default value is true. This flag is also used to configure the ScalingPolicy value if a value is not specified. If the StorageEnabled field is not specified or is true the scaling will be safe, if StorageEnabled is set to false scaling will be parallel.<br>
see: <router-link to="#coherence_settings/050_storage_enabled.adoc" @click.native="this.scrollFix('#coherence_settings/050_storage_enabled.adoc')">Configure Storage Enabled</router-link></td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>persistence</code></td>
<td class="">Persistence values configure the on-disc data persistence settings. The bool Enabled enables or disabled on disc persistence of data.<br>
see: <router-link to="#coherence_settings/080_persistence.adoc" @click.native="this.scrollFix('#coherence_settings/080_persistence.adoc')">Configure Persistence</router-link></td>
<td class=""><code>&#42;<router-link to="#_persistencespec" @click.native="this.scrollFix('#_persistencespec')">PersistenceSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>logLevel</code></td>
<td class="">The Coherence log level, default being 5 (info level).<br>
see: <router-link to="#coherence_settings/060_log_level.adoc" @click.native="this.scrollFix('#coherence_settings/060_log_level.adoc')">Configure Coherence log level</router-link></td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>management</code></td>
<td class="">Management configures Coherence management over REST Note: Coherence management over REST will is available in Coherence version &gt;= 12.2.1.4.<br>
see: <router-link to="#management_and_diagnostics/010_overview.adoc" @click.native="this.scrollFix('#management_and_diagnostics/010_overview.adoc')">Management &amp; Diagnostics</router-link></td>
<td class=""><code>&#42;<router-link to="#_portspecwithssl" @click.native="this.scrollFix('#_portspecwithssl')">PortSpecWithSSL</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>metrics</code></td>
<td class="">Metrics configures Coherence metrics publishing Note: Coherence metrics publishing will is available in Coherence version &gt;= 12.2.1.4.<br>
see: <router-link to="/metrics/010_overview">Metrics</router-link></td>
<td class=""><code>&#42;<router-link to="#_portspecwithssl" @click.native="this.scrollFix('#_portspecwithssl')">PortSpecWithSSL</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>tracing</code></td>
<td class="">Tracing is used to configure Coherence distributed tracing functionality.</td>
<td class=""><code>&#42;<router-link to="#_coherencetracingspec" @click.native="this.scrollFix('#_coherencetracingspec')">CoherenceTracingSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>allowEndangeredForStatusHA</code></td>
<td class="">AllowEndangeredForStatusHA is a list of Coherence partitioned cache service names that are allowed to be in an endangered state when testing for StatusHA. Instances where a StatusHA check is performed include the readiness probe and when scaling a deployment. This field would not typically be used except in cases where a cache service is configured with a backup count greater than zero but it does not matter if caches in those services loose data due to member departure. Normally, such cache services would have a backup count of zero, which would automatically excluded them from the StatusHA check.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>excludeFromWKA</code></td>
<td class="">Exclude members of this deployment from being part of the cluster&#8217;s WKA list.<br>
see: <router-link to="#coherence_settings/070_wka.adoc" @click.native="this.scrollFix('#coherence_settings/070_wka.adoc')">Well Known Addressing</router-link></td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>wka</code></td>
<td class="">Specify an existing Coherence deployment to be used for WKA. If an existing deployment is to be used for WKA the ExcludeFromWKA is implicitly set to true.<br>
see: <router-link to="#coherence_settings/070_wka.adoc" @click.native="this.scrollFix('#coherence_settings/070_wka.adoc')">Well Known Addressing</router-link></td>
<td class=""><code>&#42;<router-link to="#_coherencewkaspec" @click.native="this.scrollFix('#_coherencewkaspec')">CoherenceWKASpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>skipVersionCheck</code></td>
<td class="">Certain features rely on a version check prior to starting the server, e.g. metrics requires &gt;= 12.2.1.4. The version check relies on the ability of the start script to find coherence.jar but if due to how the image has been built this check is failing then setting this flag to true will skip version checking and assume that the latest coherence.jar is being used.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>enableIpMonitor</code></td>
<td class="">Enables the Coherence IP Monitor feature. The Operator disables the IP Monitor by default.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_coherencetracingspec">CoherenceTracingSpec</h3>
<div class="section">
<p>CoherenceTracingSpec configures Coherence tracing.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>ratio</code></td>
<td class="">Ratio is the tracing sampling-ratio, which controls the likelihood of a tracing span being collected. For instance, a value of 1.0 will result in all tracing spans being collected, while a value of 0.1 will result in roughly 1 out of every 10 tracing spans being collected.<br>
<br>
A value of 0 indicates that tracing spans should only be collected if they are already in the context of another tracing span.  With such a configuration, Coherence will not initiate tracing on its own, and it is up to the application to start an outer tracing span, from which Coherence will then collect inner tracing spans.<br>
<br>
A value of -1 disables tracing completely.<br>
<br>
The Coherence default is -1 if not overridden. For values other than -1, numbers between 0 and 1 are acceptable.<br>
<br>
NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See <a id="" title="" target="_blank" href="https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity">https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity</a> for the different formats of number that may be entered.</td>
<td class=""><code>&#42;resource.Quantity</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_coherencewkaspec">CoherenceWKASpec</h3>
<div class="section">
<p>CoherenceWKASpec configures Coherence well-known-addressing to use an existing Coherence deployment for WKA.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>deployment</code></td>
<td class="">The name of the existing Coherence deployment to use for WKA.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>namespace</code></td>
<td class="">The optional namespace of the existing Coherence deployment to use for WKA if different from this deployment&#8217;s namespace.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_configmapvolumespec">ConfigMapVolumeSpec</h3>
<div class="section">
<p>ConfigMapVolumeSpec represents a ConfigMap that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers.<br>
see: <router-link to="#misc_pod_settings/050_configmap_volumes.adoc" @click.native="this.scrollFix('#misc_pod_settings/050_configmap_volumes.adoc')">Add ConfigMap Volumes</router-link></p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>name</code></td>
<td class="">The name of the ConfigMap to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>mountPath</code></td>
<td class="">Path within the container at which the volume should be mounted.  Must not contain ':'.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>volumeName</code></td>
<td class="">The optional name to use for the Volume added to the Pod. If not set, the ConfigMap name will be used as the VolumeName.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>readOnly</code></td>
<td class="">Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false.</td>
<td class=""><code>bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>subPath</code></td>
<td class="">Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \"\" (volume&#8217;s root).</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>mountPropagation</code></td>
<td class="">mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#mountpropagationmode-v1-core">corev1.MountPropagationMode</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>subPathExpr</code></td>
<td class="">Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \"\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>items</code></td>
<td class="">If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#keytopath-v1-core">corev1.KeyToPath</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>defaultMode</code></td>
<td class="">Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>optional</code></td>
<td class="">Specify whether the ConfigMap or its keys must be defined</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_imagespec">ImageSpec</h3>
<div class="section">
<p>ImageSpec defines the settings for a Docker image</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>image</code></td>
<td class="">The image name. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images">https://kubernetes.io/docs/concepts/containers/images</a></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>imagePullPolicy</code></td>
<td class="">Image pull policy. One of Always, Never, IfNotPresent. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images#updating-images">https://kubernetes.io/docs/concepts/containers/images#updating-images</a></td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#pullpolicy-v1-core">corev1.PullPolicy</a></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_jvmspec">JVMSpec</h3>
<div class="section">
<p>JVMSpec is the JVM configuration.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>classpath</code></td>
<td class="">Classpath specifies additional items to add to the classpath of the JVM.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>args</code></td>
<td class="">Args specifies the options (System properties, -XX: args etc) to pass to the JVM.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>debug</code></td>
<td class="">The settings for enabling debug mode in the JVM.</td>
<td class=""><code>&#42;<router-link to="#_jvmdebugspec" @click.native="this.scrollFix('#_jvmdebugspec')">JvmDebugSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>useContainerLimits</code></td>
<td class="">If set to true Adds the  -XX:+UseContainerSupport JVM option to ensure that the JVM respects any container resource limits. The default value is true</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>gc</code></td>
<td class="">Set JVM garbage collector options.</td>
<td class=""><code>&#42;<router-link to="#_jvmgarbagecollectorspec" @click.native="this.scrollFix('#_jvmgarbagecollectorspec')">JvmGarbageCollectorSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>diagnosticsVolume</code></td>
<td class="">&#160;</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#volumesource-v1-core">corev1.VolumeSource</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>memory</code></td>
<td class="">Configure the JVM memory options.</td>
<td class=""><code>&#42;<router-link to="#_jvmmemoryspec" @click.native="this.scrollFix('#_jvmmemoryspec')">JvmMemorySpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>jmxmp</code></td>
<td class="">Configure JMX using JMXMP.</td>
<td class=""><code>&#42;<router-link to="#_jvmjmxmpspec" @click.native="this.scrollFix('#_jvmjmxmpspec')">JvmJmxmpSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>useJibClasspath</code></td>
<td class="">A flag indicating whether to automatically add the default classpath for images created by the JIB tool <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib">https://github.com/GoogleContainerTools/jib</a> If true then the /app/lib/* /app/classes and /app/resources entries are added to the JVM classpath. The default value fif not specified is true.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_jvmdebugspec">JvmDebugSpec</h3>
<div class="section">
<p>JvmDebugSpec the JVM Debug specific configuration.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">Enabled is a flag to enable or disable running the JVM in debug mode. Default is disabled.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>suspend</code></td>
<td class="">A boolean true if the target VM is to be suspended immediately before the main class is loaded; false otherwise. The default value is false.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>attach</code></td>
<td class="">Attach specifies the address of the debugger that the JVM should attempt to connect back to instead of listening on a port.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>port</code></td>
<td class="">The port that the debugger will listen on; the default is 5005.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_jvmgarbagecollectorspec">JvmGarbageCollectorSpec</h3>
<div class="section">
<p>JvmGarbageCollectorSpec is options for managing the JVM garbage collector.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>collector</code></td>
<td class="">The name of the JVM garbage collector to use. G1 - adds the -XX:+UseG1GC option CMS - adds the -XX:+UseConcMarkSweepGC option Parallel - adds the -XX:+UseParallelGC Default - use the JVMs default collector The field value is case insensitive If not set G1 is used. If set to a value other than those above then the default collector for the JVM will be used.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>args</code></td>
<td class="">Args specifies the GC options to pass to the JVM.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>logging</code></td>
<td class="">Enable the following GC logging args  -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime Default is true</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_jvmjmxmpspec">JvmJmxmpSpec</h3>
<div class="section">
<p>JvmJmxmpSpec is options for configuring JMX using JMXMP.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">If set to true the JMXMP support will be enabled. Default is false</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>port</code></td>
<td class="">The port tht the JMXMP MBeanServer should bind to. If not set the default port is 9099</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_jvmmemoryspec">JvmMemorySpec</h3>
<div class="section">
<p>JvmMemorySpec is options for managing the JVM memory.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>heapSize</code></td>
<td class="">HeapSize sets both the initial and max heap size values for the JVM. This will set both the -XX:InitialHeapSize and -XX:MaxHeapSize JVM options to the same value (the equivalent of setting -Xms and -Xmx to the same value).<br>
<br>
The format should be the same as that used when specifying these JVM options.<br>
<br>
If not set the JVM defaults are used.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>initialHeapSize</code></td>
<td class="">InitialHeapSize sets the initial heap size value for the JVM. This will set the -XX:InitialHeapSize JVM option (the equivalent of setting -Xms).<br>
<br>
The format should be the same as that used when specifying this JVM options.<br>
<br>
NOTE: If the HeapSize field is set it will override this field.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>maxHeapSize</code></td>
<td class="">MaxHeapSize sets the maximum heap size value for the JVM. This will set the -XX:MaxHeapSize JVM option (the equivalent of setting -Xmx).<br>
<br>
The format should be the same as that used when specifying this JVM options.<br>
<br>
NOTE: If the HeapSize field is set it will override this field.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>maxRAM</code></td>
<td class="">Sets the JVM option <code>-XX:MaxRAM=N</code> which sets the maximum amount of memory used by the JVM to <code>n</code>, where <code>n</code> is expressed in terms of megabytes (for example, <code>100m</code>) or gigabytes (for example <code>2g</code>).</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>percentage</code></td>
<td class="">Percentage sets the initial and maximum and minimum heap percentage sizes to the same value, This will set the -XX:InitialRAMPercentage -XX:MinRAMPercentage and -XX:MaxRAMPercentage JVM options to the same value.<br>
<br>
The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set.<br>
<br>
Valid values are decimal numbers between 0 and 100.<br>
<br>
NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See <a id="" title="" target="_blank" href="https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity">https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity</a> for the different formats of number that may be entered.<br>
<br>
NOTE: This field maps to the -XX:InitialRAMPercentage -XX:MinRAMPercentage and -XX:MaxRAMPercentage JVM options and will be incompatible with some JVMs that do not have this option (e.g. Java 8).</td>
<td class=""><code>&#42;resource.Quantity</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>initialRAMPercentage</code></td>
<td class="">Set initial heap size as a percentage of total memory.<br>
<br>
The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set.<br>
<br>
Valid values are decimal numbers between 0 and 100.<br>
<br>
NOTE: If the Percentage field is set it will override this field.<br>
<br>
NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See <a id="" title="" target="_blank" href="https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity">https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity</a> for the different formats of number that may be entered.<br>
<br>
NOTE: This field maps to the -XX:InitialRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8).</td>
<td class=""><code>&#42;resource.Quantity</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>maxRAMPercentage</code></td>
<td class="">Set maximum heap size as a percentage of total memory.<br>
<br>
The JVM will ignore this option if any of the HeapSize, InitialHeapSize or MaxHeapSize options have been set.<br>
<br>
Valid values are decimal numbers between 0 and 100.<br>
<br>
NOTE: If the Percentage field is set it will override this field.<br>
<br>
NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See <a id="" title="" target="_blank" href="https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity">https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity</a> for the different formats of number that may be entered.<br>
<br>
NOTE: This field maps to the -XX:MaxRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8).</td>
<td class=""><code>&#42;resource.Quantity</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>minRAMPercentage</code></td>
<td class="">Set the minimal JVM Heap size as a percentage of the total memory.<br>
<br>
This option will be ignored if HeapSize is set.<br>
<br>
Valid values are decimal numbers between 0 and 100.<br>
<br>
NOTE: This field is a k8s resource.Quantity value as CRDs do not support decimal numbers. See <a id="" title="" target="_blank" href="https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity">https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity</a> for the different formats of number that may be entered.<br>
<br>
NOTE: This field maps the the -XX:MinRAMPercentage JVM option and will be incompatible with some JVMs that do not have this option (e.g. Java 8).</td>
<td class=""><code>&#42;resource.Quantity</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>stackSize</code></td>
<td class="">StackSize is the stack size value to pass to the JVM. The format should be the same as that used for Java&#8217;s -Xss JVM option. If not set the JVM defaults are used.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>metaspaceSize</code></td>
<td class="">MetaspaceSize is the min/max metaspace size to pass to the JVM. This sets the -XX:MetaspaceSize and -XX:MaxMetaspaceSize=size JVM options. If not set the JVM defaults are used.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>directMemorySize</code></td>
<td class="">DirectMemorySize sets the maximum total size (in bytes) of the New I/O (the java.nio package) direct-buffer allocations. This value sets the -XX:MaxDirectMemorySize JVM option. If not set the JVM defaults are used.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>nativeMemoryTracking</code></td>
<td class="">Adds the -XX:NativeMemoryTracking=mode  JVM options where mode is on of \"off\", \"summary\" or \"detail\", the default is \"summary\" If not set to \"off\" also add -XX:+PrintNMTStatistics</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>onOutOfMemory</code></td>
<td class="">Configure the JVM behaviour when an OutOfMemoryError occurs.</td>
<td class=""><code>&#42;<router-link to="#_jvmoutofmemoryspec" @click.native="this.scrollFix('#_jvmoutofmemoryspec')">JvmOutOfMemorySpec</router-link></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_jvmoutofmemoryspec">JvmOutOfMemorySpec</h3>
<div class="section">
<p>JvmOutOfMemorySpec is options for managing the JVM behaviour when an OutOfMemoryError occurs.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>exit</code></td>
<td class="">If set to true the JVM will exit when an OOM error occurs. Default is true</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>heapDump</code></td>
<td class="">If set to true adds the -XX:+HeapDumpOnOutOfMemoryError JVM option to cause a heap dump to be created when an OOM error occurs. Default is true</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_localobjectreference">LocalObjectReference</h3>
<div class="section">
<p>LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>name</code></td>
<td class="">Name of the referent. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names">https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names</a></td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_namedportspec">NamedPortSpec</h3>
<div class="section">
<p>NamedPortSpec defines a named port for a Coherence component</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>name</code></td>
<td class="">Name specifies the name of the port.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>port</code></td>
<td class="">Port specifies the port used.</td>
<td class=""><code>int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>protocol</code></td>
<td class="">Protocol for container port. Must be UDP or TCP. Defaults to \"TCP\"</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#protocol-v1-core">corev1.Protocol</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>appProtocol</code></td>
<td class="">The application protocol for this port. This field follows standard Kubernetes label syntax. Un-prefixed names are reserved for IANA standard service names (as per RFC-6335 and <a id="" title="" target="_blank" href="http://www.iana.org/assignments/service-names">http://www.iana.org/assignments/service-names</a>). Non-standard protocols should use prefixed names such as mycompany.com/my-custom-protocol.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>nodePort</code></td>
<td class="">The port on each node on which this service is exposed when type=NodePort or LoadBalancer. Usually assigned by the system. If specified, it will be allocated to the service if unused or else creation of the service will fail. If set, this field must be in the range 30000 - 32767 inclusive. Default is to auto-allocate a port if the ServiceType of this Service requires one. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport</a></td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>hostPort</code></td>
<td class="">Number of port to expose on the host. If specified, this must be a valid port number, 0 &lt; x &lt; 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>hostIP</code></td>
<td class="">What host IP to bind the external port to.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>service</code></td>
<td class="">Service configures the Kubernetes Service used to expose the port.</td>
<td class=""><code>&#42;<router-link to="#_servicespec" @click.native="this.scrollFix('#_servicespec')">ServiceSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>serviceMonitor</code></td>
<td class="">The specification of a Prometheus ServiceMonitor resource that will be created for the Service being exposed for this port.</td>
<td class=""><code>&#42;<router-link to="#_servicemonitorspec" @click.native="this.scrollFix('#_servicemonitorspec')">ServiceMonitorSpec</router-link></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_networkspec">NetworkSpec</h3>
<div class="section">
<p>NetworkSpec configures various networking and DNS settings for Pods in a deployment.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>dnsConfig</code></td>
<td class="">Specifies the DNS parameters of a pod. Parameters specified here will be merged to the generated DNS configuration based on DNSPolicy.</td>
<td class=""><code>&#42;<router-link to="#_poddnsconfig" @click.native="this.scrollFix('#_poddnsconfig')">PodDNSConfig</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>dnsPolicy</code></td>
<td class="">Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#dnspolicy-v1-core">corev1.DNSPolicy</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>hostAliases</code></td>
<td class="">HostAliases is an optional list of hosts and IPs that will be injected into the pod&#8217;s hosts file if specified. This is only valid for non-hostNetwork pods.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#hostalias-v1-core">corev1.HostAlias</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>hostNetwork</code></td>
<td class="">Host networking requested for this pod. Use the host&#8217;s network namespace. If this option is set, the ports that will be used must be specified. Default to false.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>hostname</code></td>
<td class="">Specifies the hostname of the Pod If not specified, the pod&#8217;s hostname will be set to a system-defined value.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_persistencespec">PersistenceSpec</h3>
<div class="section">
<p>PersistenceSpec is the spec for Coherence persistence.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>mode</code></td>
<td class="">The persistence mode to use. Valid choices are \"on-demand\", \"active\", \"active-async\". This field will set the coherence.distributed.persistence-mode System property to \"default-\" + Mode.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>persistentVolumeClaim</code></td>
<td class="">PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#persistentvolumeclaimspec-v1-core">corev1.PersistentVolumeClaimSpec</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>volume</code></td>
<td class="">Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#volumesource-v1-core">corev1.VolumeSource</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>snapshots</code></td>
<td class="">Snapshot values configure the on-disc persistence data snapshot (backup) settings. These settings enable a different location for persistence snapshot data. If not set then snapshot files will be written to the same volume configured for persistence data in the Persistence section.</td>
<td class=""><code>&#42;<router-link to="#_persistentstoragespec" @click.native="this.scrollFix('#_persistentstoragespec')">PersistentStorageSpec</router-link></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_persistentstoragespec">PersistentStorageSpec</h3>
<div class="section">
<p>PersistentStorageSpec defines the persistence settings for the Coherence</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>persistentVolumeClaim</code></td>
<td class="">PersistentVolumeClaim allows the configuration of a normal k8s persistent volume claim for persistence data.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#persistentvolumeclaimspec-v1-core">corev1.PersistentVolumeClaimSpec</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>volume</code></td>
<td class="">Volume allows the configuration of a normal k8s volume mapping for persistence data instead of a persistent volume claim. If a value is defined for store.persistence.volume then no PVC will be created and persistence data will instead be written to this volume. It is up to the deployer to understand the consequences of this and how the guarantees given when using PVCs differ to the storage guarantees for the particular volume type configured here.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#volumesource-v1-core">corev1.VolumeSource</a></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_poddnsconfig">PodDNSConfig</h3>
<div class="section">
<p>PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>nameservers</code></td>
<td class="">A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>searches</code></td>
<td class="">A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>options</code></td>
<td class="">A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#poddnsconfigoption-v1-core">corev1.PodDNSConfigOption</a></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_portspecwithssl">PortSpecWithSSL</h3>
<div class="section">
<p>PortSpecWithSSL defines a port with SSL settings for a Coherence component</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">Enable or disable flag.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>port</code></td>
<td class="">The port to bind to.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>ssl</code></td>
<td class="">SSL configures SSL settings for a Coherence component</td>
<td class=""><code>&#42;<router-link to="#_sslspec" @click.native="this.scrollFix('#_sslspec')">SSLSpec</router-link></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_probe">Probe</h3>
<div class="section">
<p>Probe is the handler that will be used to determine how to communicate with a Coherence deployment for operations like StatusHA checking and service suspension. StatusHA checking is primarily used during scaling of a deployment, a deployment must be in a safe Phase HA state before scaling takes place. If StatusHA handler is disabled for a deployment (by specifically setting Enabled to false then no check will take place and a deployment will be assumed to be safe).</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>timeoutSeconds</code></td>
<td class="">Number of seconds after which the handler times out (only applies to http and tcp handlers). Defaults to 1 second. Minimum value is 1.</td>
<td class=""><code>&#42;int</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_probehandler">ProbeHandler</h3>
<div class="section">
<p>ProbeHandler is the definition of a probe handler.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>exec</code></td>
<td class="">One and only one of the following should be specified. Exec specifies the action to take.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#execaction-v1-core">corev1.ExecAction</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>httpGet</code></td>
<td class="">HTTPGet specifies the http request to perform.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#httpgetaction-v1-core">corev1.HTTPGetAction</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>tcpSocket</code></td>
<td class="">TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#tcpsocketaction-v1-core">corev1.TCPSocketAction</a></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_readinessprobespec">ReadinessProbeSpec</h3>
<div class="section">
<p>ReadinessProbeSpec defines the settings for the Coherence Pod readiness probe</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>exec</code></td>
<td class="">One and only one of the following should be specified. Exec specifies the action to take.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#execaction-v1-core">corev1.ExecAction</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>httpGet</code></td>
<td class="">HTTPGet specifies the http request to perform.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#httpgetaction-v1-core">corev1.HTTPGetAction</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>tcpSocket</code></td>
<td class="">TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#tcpsocketaction-v1-core">corev1.TCPSocketAction</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>initialDelaySeconds</code></td>
<td class="">Number of seconds after the container has started before liveness probes are initiated. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes">https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes</a></td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>timeoutSeconds</code></td>
<td class="">Number of seconds after which the probe times out. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes">https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes</a></td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>periodSeconds</code></td>
<td class="">How often (in seconds) to perform the probe.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>successThreshold</code></td>
<td class="">Minimum consecutive successes for the probe to be considered successful after having failed.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>failureThreshold</code></td>
<td class="">Minimum consecutive failures for the probe to be considered failed after having succeeded.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_resource">Resource</h3>
<div class="section">
<p>Resource is a structure holding a resource to be managed</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>kind</code></td>
<td class="">&#160;</td>
<td class=""><code>ResourceType</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>name</code></td>
<td class="">&#160;</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>spec</code></td>
<td class="">&#160;</td>
<td class=""><code>client.Object</code></td>
<td class="">true</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_resources">Resources</h3>
<div class="section">
<p>Resources is a cloolection of resources to be managed.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>version</code></td>
<td class="">&#160;</td>
<td class=""><code>int32</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>items</code></td>
<td class="">&#160;</td>
<td class=""><code>[]<router-link to="#_resource" @click.native="this.scrollFix('#_resource')">Resource</router-link></code></td>
<td class="">true</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_sslspec">SSLSpec</h3>
<div class="section">
<p>SSLSpec defines the SSL settings for a Coherence component over REST endpoint.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">Enabled is a boolean flag indicating whether enables or disables SSL on the Coherence management over REST endpoint, the default is false (disabled).</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>secrets</code></td>
<td class="">Secrets is the name of the k8s secrets containing the Java key stores and password files.<br>
  This value MUST be provided if SSL is enabled on the Coherence management over ReST endpoint.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>keyStore</code></td>
<td class="">Keystore is the name of the Java key store file in the k8s secret to use as the SSL keystore<br>
  when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>keyStorePasswordFile</code></td>
<td class="">KeyStorePasswordFile is the name of the file in the k8s secret containing the keystore<br>
  password when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>keyPasswordFile</code></td>
<td class="">KeyStorePasswordFile is the name of the file in the k8s secret containing the key<br>
  password when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>keyStoreAlgorithm</code></td>
<td class="">KeyStoreAlgorithm is the name of the keystore algorithm for the keystore in the k8s secret<br>
  used when configuring component over REST to use SSL. If not set the default is SunX509<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>keyStoreProvider</code></td>
<td class="">KeyStoreProvider is the name of the keystore provider for the keystore in the k8s secret<br>
  used when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>keyStoreType</code></td>
<td class="">KeyStoreType is the name of the Java keystore type for the keystore in the k8s secret used<br>
  when configuring component over REST to use SSL. If not set the default is JKS.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>trustStore</code></td>
<td class="">TrustStore is the name of the Java trust store file in the k8s secret to use as the SSL<br>
  trust store when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>trustStorePasswordFile</code></td>
<td class="">TrustStorePasswordFile is the name of the file in the k8s secret containing the trust store<br>
  password when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>trustStoreAlgorithm</code></td>
<td class="">TrustStoreAlgorithm is the name of the keystore algorithm for the trust store in the k8s<br>
  secret used when configuring component over REST to use SSL.  If not set the default is SunX509.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>trustStoreProvider</code></td>
<td class="">TrustStoreProvider is the name of the keystore provider for the trust store in the k8s<br>
  secret used when configuring component over REST to use SSL.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>trustStoreType</code></td>
<td class="">TrustStoreType is the name of the Java keystore type for the trust store in the k8s secret<br>
  used when configuring component over REST to use SSL. If not set the default is JKS.<br></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>requireClientCert</code></td>
<td class="">RequireClientCert is a boolean flag indicating whether the client certificate will be<br>
  authenticated by the server (two-way SSL) when configuring component over REST to use SSL. +<br>
  If not set the default is false<br></td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_scalingspec">ScalingSpec</h3>
<div class="section">
<p>ScalingSpec is the configuration to control safe scaling.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>policy</code></td>
<td class="">ScalingPolicy describes how the replicas of the deployment will be scaled. The default if not specified is based upon the value of the StorageEnabled field. If StorageEnabled field is not specified or is true the default scaling will be safe, if StorageEnabled is set to false the default scaling will be parallel.</td>
<td class=""><code>&#42;ScalingPolicy</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>probe</code></td>
<td class="">The probe to use to determine whether a deployment is Phase HA. If not set the default handler will be used. In most use-cases the default handler would suffice but in advanced use-cases where the application code has a different concept of Phase HA to just checking Coherence services then a different handler may be specified.</td>
<td class=""><code>&#42;<router-link to="#_probe" @click.native="this.scrollFix('#_probe')">Probe</router-link></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_secretvolumespec">SecretVolumeSpec</h3>
<div class="section">
<p>SecretVolumeSpec represents a Secret that will be added to the deployment&#8217;s Pods as an additional Volume and as a VolumeMount in the containers.<br>
see: <router-link to="#misc_pod_settings/020_secret_volumes.adoc" @click.native="this.scrollFix('#misc_pod_settings/020_secret_volumes.adoc')">Add Secret Volumes</router-link></p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>name</code></td>
<td class="">The name of the Secret to mount. This will also be used as the name of the Volume added to the Pod if the VolumeName field is not set.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>mountPath</code></td>
<td class="">Path within the container at which the volume should be mounted.  Must not contain ':'.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>volumeName</code></td>
<td class="">The optional name to use for the Volume added to the Pod. If not set, the Secret name will be used as the VolumeName.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>readOnly</code></td>
<td class="">Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false.</td>
<td class=""><code>bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>subPath</code></td>
<td class="">Path within the volume from which the container&#8217;s volume should be mounted. Defaults to \"\" (volume&#8217;s root).</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>mountPropagation</code></td>
<td class="">mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#mountpropagationmode-v1-core">corev1.MountPropagationMode</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>subPathExpr</code></td>
<td class="">Expanded path within the volume from which the container&#8217;s volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container&#8217;s environment. Defaults to \"\" (volume&#8217;s root). SubPathExpr and SubPath are mutually exclusive.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>items</code></td>
<td class="">If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#keytopath-v1-core">corev1.KeyToPath</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>defaultMode</code></td>
<td class="">Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>optional</code></td>
<td class="">Specify whether the Secret or its keys must be defined</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_servicemonitorspec">ServiceMonitorSpec</h3>
<div class="section">
<p>ServiceMonitorSpec the ServiceMonitor spec for a port service.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">Enabled is a flag to enable or disable creation of a Prometheus ServiceMonitor for a port. If Prometheus ServiceMonitor CR is not installed no ServiceMonitor then even if this flag is true no ServiceMonitor will be created.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>labels</code></td>
<td class="">Additional labels to add to the ServiceMonitor. More info: <a id="" title="" target="_blank" href="http://kubernetes.io/docs/user-guide/labels">http://kubernetes.io/docs/user-guide/labels</a></td>
<td class=""><code>map[string]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>jobLabel</code></td>
<td class="">The label to use to retrieve the job name from. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec">https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>targetLabels</code></td>
<td class="">TargetLabels transfers labels on the Kubernetes Service onto the target. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec">https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec</a></td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>podTargetLabels</code></td>
<td class="">PodTargetLabels transfers labels on the Kubernetes Pod onto the target. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec">https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec</a></td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>sampleLimit</code></td>
<td class="">SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec">https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec</a></td>
<td class=""><code>uint64</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>path</code></td>
<td class="">HTTP path to scrape for metrics. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>scheme</code></td>
<td class="">HTTP scheme to use for scraping. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>params</code></td>
<td class="">Optional HTTP URL parameters See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>map[string][]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>interval</code></td>
<td class="">Interval at which metrics should be scraped See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>scrapeTimeout</code></td>
<td class="">Timeout after which the scrape is ended See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>tlsConfig</code></td>
<td class="">TLS configuration to use when scraping the endpoint See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>&#42;monitoringv1.TLSConfig</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>bearerTokenFile</code></td>
<td class="">File to read bearer token for scraping targets. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>bearerTokenSecret</code></td>
<td class="">Secret to mount to read bearer token for scraping targets. The secret needs to be in the same namespace as the service monitor and accessible by the Prometheus Operator. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code><a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#secretkeyselector-v1-core">corev1.SecretKeySelector</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>honorLabels</code></td>
<td class="">HonorLabels chooses the metric labels on collisions with target labels. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>honorTimestamps</code></td>
<td class="">HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>basicAuth</code></td>
<td class="">BasicAuth allow an endpoint to authenticate over basic authentication More info: <a id="" title="" target="_blank" href="https://prometheus.io/docs/operating/configuration/#endpoints">https://prometheus.io/docs/operating/configuration/#endpoints</a> See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>&#42;monitoringv1.BasicAuth</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>metricRelabelings</code></td>
<td class="">MetricRelabelings to apply to samples before ingestion. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>[]&#42;monitoringv1.RelabelConfig</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>relabelings</code></td>
<td class="">Relabelings to apply to samples before scraping. More info: <a id="" title="" target="_blank" href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config</a> See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>[]&#42;monitoringv1.RelabelConfig</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>proxyURL</code></td>
<td class="">ProxyURL eg <a id="" title="" target="_blank" href="http://proxyserver:2195">http://proxyserver:2195</a> Directs scrapes to proxy through this endpoint. See <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint</a></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_servicespec">ServiceSpec</h3>
<div class="section">
<p>ServiceSpec defines the settings for a Service</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>enabled</code></td>
<td class="">Enabled controls whether to create the service yaml or not</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>name</code></td>
<td class="">An optional name to use to override the generated service name.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>portName</code></td>
<td class="">An optional name to use to override the port name.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>port</code></td>
<td class="">The service port value</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>type</code></td>
<td class="">Kind is the K8s service type (typically ClusterIP or LoadBalancer) The default is \"ClusterIP\".</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#servicetype-v1-core">corev1.ServiceType</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>externalIPs</code></td>
<td class="">externalIPs is a list of IP addresses for which nodes in the cluster will also accept traffic for this service.  These IPs are not managed by Kubernetes.  The user is responsible for ensuring that traffic arrives at a node with this IP.  A common example is external load-balancers that are not part of the Kubernetes system.</td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>clusterIP</code></td>
<td class="">clusterIP is the IP address of the service and is usually assigned randomly by the master. If an address is specified manually and is not in use by others, it will be allocated to the service; otherwise, creation of the service will fail. This field can not be changed through updates. Valid values are \"None\", empty string (\"\"), or a valid IP address. \"None\" can be specified for headless services when proxying is not required. Only applies to types ClusterIP, NodePort, and LoadBalancer. Ignored if type is ExternalName. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies</a></td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>clusterIPs</code></td>
<td class="">ClusterIPs is a list of IP addresses assigned to this service, and are usually assigned randomly.  If an address is specified manually, is in-range (as per system configuration), and is not in use, it will be allocated to the service; otherwise creation of the service will fail. This field may not be changed through updates unless the type field is also being changed to ExternalName (which requires this field to be empty) or the type field is being changed from ExternalName (in which case this field may optionally be specified, as describe above).  Valid values are \"None\", empty string (\"\"), or a valid IP address.  Setting this to \"None\" makes a \"headless service\" (no virtual IP), which is useful when direct endpoint connections are preferred and proxying is not required.  Only applies to types ClusterIP, NodePort, and LoadBalancer. If this field is specified when creating a Service of type ExternalName, creation will fail. This field will be wiped when updating a Service to type ExternalName.  If this field is not specified, it will be initialized from the clusterIP field.  If this field is specified, clients must ensure that clusterIPs[0] and clusterIP have the same value.<br>
<br>
Unless the \"IPv6DualStack\" feature gate is enabled, this field is limited to one value, which must be the same as the clusterIP field.  If the feature gate is enabled, this field may hold a maximum of two entries (dual-stack IPs, in either order).  These IPs must correspond to the values of the ipFamilies field. Both clusterIPs and ipFamilies are governed by the ipFamilyPolicy field. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies</a></td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>loadBalancerIP</code></td>
<td class="">LoadBalancerIP is the IP address of the load balancer</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>labels</code></td>
<td class="">The extra labels to add to the service. More info: <a id="" title="" target="_blank" href="http://kubernetes.io/docs/user-guide/labels">http://kubernetes.io/docs/user-guide/labels</a></td>
<td class=""><code>map[string]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>annotations</code></td>
<td class="">Annotations is free form yaml that will be added to the service annotations</td>
<td class=""><code>map[string]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>sessionAffinity</code></td>
<td class="">Supports \"ClientIP\" and \"None\". Used to maintain session affinity. Enable client IP based session affinity. Must be ClientIP or None. Defaults to None. More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies</a></td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#serviceaffinity-v1-core">corev1.ServiceAffinity</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>loadBalancerSourceRanges</code></td>
<td class="">If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.\" More info: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/">https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/</a></td>
<td class=""><code>[]string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>externalName</code></td>
<td class="">externalName is the external reference that kubedns or equivalent will return as a CNAME record for this service. No proxying will be involved. Must be a valid RFC-1123 hostname (<a id="" title="" target="_blank" href="https://tools.ietf.org/html/rfc1123">https://tools.ietf.org/html/rfc1123</a>) and requires Kind to be ExternalName.</td>
<td class=""><code>&#42;string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>externalTrafficPolicy</code></td>
<td class="">externalTrafficPolicy denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. \"Local\" preserves the client source IP and avoids a second hop for LoadBalancer and Nodeport type services, but risks potentially imbalanced traffic spreading. \"Cluster\" obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#serviceexternaltrafficpolicytype-v1-core">corev1.ServiceExternalTrafficPolicyType</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>healthCheckNodePort</code></td>
<td class="">healthCheckNodePort specifies the healthcheck nodePort for the service. If not specified, HealthCheckNodePort is created by the service api backend with the allocated nodePort. Will use user-specified nodePort value if specified by the client. Only effects when Kind is set to LoadBalancer and ExternalTrafficPolicy is set to Local.</td>
<td class=""><code>&#42;int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>publishNotReadyAddresses</code></td>
<td class="">publishNotReadyAddresses, when set to true, indicates that DNS implementations must publish the notReadyAddresses of subsets for the Endpoints associated with the Service. The default value is false. The primary use case for setting this field is to use a StatefulSet&#8217;s Headless Service to propagate SRV records for its Pods without respect to their readiness for purpose of peer discovery.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>sessionAffinityConfig</code></td>
<td class="">sessionAffinityConfig contains the configurations of session affinity.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#sessionaffinityconfig-v1-core">corev1.SessionAffinityConfig</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>ipFamilies</code></td>
<td class="">IPFamilies is a list of IP families (e.g. IPv4, IPv6) assigned to this service, and is gated by the \"IPv6DualStack\" feature gate.  This field is usually assigned automatically based on cluster configuration and the ipFamilyPolicy field. If this field is specified manually, the requested family is available in the cluster, and ipFamilyPolicy allows it, it will be used; otherwise creation of the service will fail.  This field is conditionally mutable: it allows for adding or removing a secondary IP family, but it does not allow changing the primary IP family of the Service.  Valid values are \"IPv4\" and \"IPv6\".  This field only applies to Services of types ClusterIP, NodePort, and LoadBalancer, and does apply to \"headless\" services.  This field will be wiped when updating a Service to type ExternalName.<br>
<br>
This field may hold a maximum of two entries (dual-stack families, in either order).  These families must correspond to the values of the clusterIPs field, if specified. Both clusterIPs and ipFamilies are governed by the ipFamilyPolicy field.</td>
<td class=""><code>[]<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#ipfamily-v1-core">corev1.IPFamily</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>ipFamilyPolicy</code></td>
<td class="">IPFamilyPolicy represents the dual-stack-ness requested or required by this Service, and is gated by the \"IPv6DualStack\" feature gate.  If there is no value provided, then this field will be set to SingleStack. Services can be \"SingleStack\" (a single IP family), \"PreferDualStack\" (two IP families on dual-stack configured clusters or a single IP family on single-stack clusters), or \"RequireDualStack\" (two IP families on dual-stack configured clusters, otherwise fail). The ipFamilies and clusterIPs fields depend on the value of this field.  This field will be wiped when updating a service to type ExternalName.</td>
<td class=""><code>&#42;<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#ipfamilypolicytype-v1-core">corev1.IPFamilyPolicyType</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>allocateLoadBalancerNodePorts</code></td>
<td class="">allocateLoadBalancerNodePorts defines if NodePorts will be automatically allocated for services with type LoadBalancer.  Default is \"true\". It may be set to \"false\" if the cluster load-balancer does not rely on NodePorts. allocateLoadBalancerNodePorts may only be set for services with type LoadBalancer and will be cleared if the type is changed to any other type. This field is alpha-level and is only honored by servers that enable the ServiceLBNodePortControl feature.</td>
<td class=""><code>&#42;bool</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_startquorum">StartQuorum</h3>
<div class="section">
<p>StartQuorum defines the order that deployments will be started in a Coherence cluster made up of multiple deployments.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>deployment</code></td>
<td class="">The name of deployment that this deployment depends on.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>namespace</code></td>
<td class="">The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>podCount</code></td>
<td class="">The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment.</td>
<td class=""><code>int32</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_startquorumstatus">StartQuorumStatus</h3>
<div class="section">
<p>StartQuorumStatus tracks the state of a deployment&#8217;s start quorums.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>deployment</code></td>
<td class="">The name of deployment that this deployment depends on.</td>
<td class=""><code>string</code></td>
<td class="">true</td>
</tr>
<tr>
<td class=""><code>namespace</code></td>
<td class="">The namespace that the deployment that this deployment depends on is installed into. Default to the same namespace as this deployment</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>podCount</code></td>
<td class="">The number of the Pods that should have been started before this deployments will be started, defaults to all Pods for the deployment.</td>
<td class=""><code>int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>ready</code></td>
<td class="">Whether this quorum&#8217;s condition has been met</td>
<td class=""><code>bool</code></td>
<td class="">true</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_coherence">Coherence</h3>
<div class="section">
<p>Coherence is the Schema for the Coherence API.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>metadata</code></td>
<td class="">&#160;</td>
<td class=""><code><a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#objectmeta-v1-meta">metav1.ObjectMeta</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>spec</code></td>
<td class="">&#160;</td>
<td class=""><code><router-link to="#_coherenceresourcespec" @click.native="this.scrollFix('#_coherenceresourcespec')">CoherenceResourceSpec</router-link></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>status</code></td>
<td class="">&#160;</td>
<td class=""><code><router-link to="#_coherenceresourcestatus" @click.native="this.scrollFix('#_coherenceresourcestatus')">CoherenceResourceStatus</router-link></code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_coherencelist">CoherenceList</h3>
<div class="section">
<p>CoherenceList contains a list of Coherence resources.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>metadata</code></td>
<td class="">&#160;</td>
<td class=""><code><a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#listmeta-v1-meta">metav1.ListMeta</a></code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>items</code></td>
<td class="">&#160;</td>
<td class=""><code>[]<router-link to="#_coherence" @click.native="this.scrollFix('#_coherence')">Coherence</router-link></code></td>
<td class="">true</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>

<h3 id="_coherenceresourcestatus">CoherenceResourceStatus</h3>
<div class="section">
<p>CoherenceResourceStatus defines the observed state of Coherence resource.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 7.692%;">
<col style="width: 76.923%;">
<col style="width: 7.692%;">
<col style="width: 7.692%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
<th>Type</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>phase</code></td>
<td class="">The phase of a Coherence resource is a simple, high-level summary of where the Coherence resource is in its lifecycle. The conditions array, the reason and message fields, and the individual container status arrays contain more detail about the pod&#8217;s status. There are eight possible phase values:<br>
<br>
Initialized:    The deployment has been accepted by the Kubernetes system. Created:        The deployments secondary resources, (e.g. the StatefulSet, Services etc) have been created. Ready:          The StatefulSet for the deployment has the correct number of replicas and ready replicas. Waiting:        The deployment&#8217;s start quorum conditions have not yet been met. Scaling:        The number of replicas in the deployment is being scaled up or down. RollingUpgrade: The StatefulSet is performing a rolling upgrade. Stopped:        The replica count has been set to zero. Failed:         An error occurred reconciling the deployment and its secondary resources.</td>
<td class=""><code>status.ConditionType</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>coherenceCluster</code></td>
<td class="">The name of the Coherence cluster that this deployment is part of.</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>replicas</code></td>
<td class="">Replicas is the desired number of members in the Coherence deployment represented by the Coherence resource.</td>
<td class=""><code>int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>currentReplicas</code></td>
<td class="">CurrentReplicas is the current number of members in the Coherence deployment represented by the Coherence resource.</td>
<td class=""><code>int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>readyReplicas</code></td>
<td class="">ReadyReplicas is the number of number of members in the Coherence deployment represented by the Coherence resource that are in the ready state.</td>
<td class=""><code>int32</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>role</code></td>
<td class="">The effective role name for this deployment. This will come from the Spec.Role field if set otherwise the deployment name will be used for the role name</td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>selector</code></td>
<td class="">label query over deployments that should match the replicas count. This is same as the label selector but in the string format to avoid introspection by clients. The string will be in the same format as the query-param syntax. More info about label selectors: <a id="" title="" target="_blank" href="http://kubernetes.io/docs/user-guide/labels#label-selectors">http://kubernetes.io/docs/user-guide/labels#label-selectors</a></td>
<td class=""><code>string</code></td>
<td class="">false</td>
</tr>
<tr>
<td class=""><code>conditions</code></td>
<td class="">The status conditions.</td>
<td class=""><code>status.Conditions</code></td>
<td class="">false</td>
</tr>
</tbody>
</table>
</div>
<p><router-link to="#_table_of_contents" @click.native="this.scrollFix('#_table_of_contents')">Back to TOC</router-link></p>

</div>
</div>
</doc-view>
<!-- pages/about/05_upgrade.js -->
<doc-view>

<h2 id="_upgrading_from_operator_v2">Upgrading from Operator v2</h2>
<div class="section">
<p>Version 3 of the Coherence Operator is very different to version 2.
There is only a single CRD named <code>Coherence</code> instead of the three CRDs used by v2,
and the operator no longer uses Helm internally to install the Kubernetes resources.</p>

<p>In terms of usage and concepts, the biggest change is that there are no longer clusters and roles.
The <code>Coherence</code> CRD represents what would previously in v2 have been a role. A Coherence cluster that is made up
of multiple roles will just require multiple <code>Coherence</code> resources deploying to Kubernetes.
The simplification of the operator, and consequently the better reliability, far outweigh any advantage of being able
to put multiple roles in a single yaml file. If this is desire just put multiple <code>Coherence</code> resource definitions in
a single yaml file with the <code>---</code> separator.</p>

<p>For example:</p>

<p>In Operator v2 a cluster may have been defined with two roles, <code>storage</code> and <code>proxy</code> like this:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: my-cluster
spec:
  roles:
    - role: storage
      replicas: 3
    - role: proxy
      replicas: 2</pre>

<p>In Operator v3 this needs to be two separate`Coherence` resources.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: my-cluster-storage
spec:
  - role: storage
    replicas: 3
    cluster: my-cluster
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: my-cluster-proxy
spec:
  - role: proxy
    replicas: 2
    cluster: my-cluster</pre>

<div class="admonition note">
<p class="admonition-inline">To make both <code>Coherence</code> resources part of the same cluster the <code>cluster</code> field must now be set in both
resources to the same value, in this case <code>my-cluster</code>.</p>
</div>
</div>

<h2 id="_applications">Applications</h2>
<div class="section">
<p>Coherence applications in Operator v2 worked by application resources (jar files etc) being provided in an image
that was loaded as an init-container in the <code>Pod</code>, and the application artifacts copied to the classpath of the Coherence
container.
In version 3 of the Operator there is only one image required that should contain all of the resources required for the
application, including Coherence jar. This gives the application developer much more control over how the image is built
and what resources it contains, as well as making it more obvious what is going to be run when the container starts.</p>


<h3 id="_images">Images</h3>
<div class="section">
<p>In Operator v2 there were multiple images defined, one for Coherence and one used to provide application artifacts.
Because of the application changes described only a single image now needs to be specified in the <code>image</code> field
of the <code>CRD</code> spec.</p>

<p>See the <router-link to="/applications/010_overview">Applications</router-link> section of the doecumentation for more details.</p>

</div>
</div>

<h2 id="_crd_differences">CRD Differences</h2>
<div class="section">
<p>A lot of the fields in the <code>Coherence</code> CRD are the same as when defining a role in version 2.
Whilst a number of new fields and features have been added in version 3, a handful of fields have moved,
and a small number, that no longer made sense, have been removed.
The <router-link to="/about/04_coherence_spec">Coherence Spec</router-link> page documents the full <code>Coherence</code> CRD, so it is
simple to locate where a field might have moved to.</p>

</div>

<h2 id="_logging_and_fluentd">Logging and Fluentd</h2>
<div class="section">
<p>Version 3 of the operator no longer has fields to configure a Fluentd side-car container.
There are a lot of different ways to configure Fluentd and making the Operator accomodate all of these was becoming
too much of a head-ache to do in a backwards compatible way.
If a Fluentd side-car is required it can just be added to the <code>Coherence</code> resource spec as an additional container,
so there is no limitation on the Fluentd configuration.
See the <router-link to="/logging/010_overview">Logging documentation</router-link> for more examples.</p>

</div>

<h2 id="_prometheus_and_elasticsearch">Prometheus and Elasticsearch</h2>
<div class="section">
<p>Version 3 of the Operator no  longer comes with the option to install Prometheus and/or Elasticsearch.
This feature was only ever intended to make it easier to demo features that required Prometheus and Elasticsearch and
keeping this up to date was a headache nobody needed.
Both Prometheus and Elasticsearch have operators of their own which make installing them simple and importing the
dashboards provided by the Coherence Operator simple too.</p>

</div>
</doc-view>
<!-- pages/applications/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>A typical Coherence deployment contains custom application code that runs with Coherence.
To run custom application code in a <code>Coherence</code> resource that code needs to be packaged into an image that the
deployment will use.</p>


<h3 id="_building_and_deploying_applications">Building and Deploying Applications</h3>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/applications/020_build_application"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Build Custom Application Images</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Building custom Coherence application images for use with the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/applications/030_deploy_application"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Deploy Custom Application Images</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Deploying custom application images using the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>

<h3 id="_configuring_applications">Configuring Applications</h3>
<div class="section">
<p>There are many settings in a <code>Coherence</code> resource that control the behaviour of Coherence, the JVM and
the application code. Some of the application specific settings are shown below:</p>

<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/020_classpath"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Setting the Classpath</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Setting a custom classpath for the application.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/applications/040_application_main"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Setting a Main Class</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Setting a custom main class to run.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/applications/050_application_args"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Setting Application Arguments</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Setting arguments to pass to the main class.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/applications/060_application_working_dir"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Working Directory</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Setting the application&#8217;s working directory.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</div>
</doc-view>
<!-- pages/applications/020_build_application.js -->
<doc-view>

<h2 id="_build_custom_application_images">Build Custom Application Images</h2>
<div class="section">
<p>To deploy a Coherence application using the operator the application code must be packaged into an image that the
Coherence container in the Pods will run. This image can be any image that contains a JVM as well as the application&#8217;s
jar files, including obviously <code>coherence.jar</code>.</p>

<p>There are many ways to build an image for a Java application so it would be of little value to document the exact steps
for one of them here that might turn out to be used by very few people. One of the simplest ways to build a Java image
is to use <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib/blob/master/README.md">JIB</a>.
The Operator supports JIB images automatically but any image that meets the requirements of having a JVM and <code>coherence.jar</code>
will be supported. Any version of Java which works with the version of <code>coherence.jar</code> in the image will be suitable.
This can be a JRE, it does not need to be a full JDK.</p>

<p>At a bare minimum the directories in an image might look like this example
(obviously there would be more O/S related files and more JVM files, but they are not relevant for the example):</p>

<pre


>/
|-- app
|    |-- libs                      <span class="conum" data-value="1" />
|         |-- application.jar
|         |-- coherence.jar
|-- usr
     |-- bin
     |    |-- java                 <span class="conum" data-value="2" />
     |
     |-- lib
          |-- jvm
               |-- java-11-openjdk <span class="conum" data-value="3" /></pre>

<ul class="colist">
<li data-value="1">The <code>/app/libs</code> directory contains the application jar files. This will be the classpath used to run the application.</li>
<li data-value="2">The <code>/usr/bin/java</code> file is the Java executable and on the <code>PATH</code> in the image (this would be a link to the actual
Java executable location, in this example to <code>/usr/lib/jvm/java-11-openjdk/bin/java</code>.</li>
<li data-value="3">The <code>/usr/lib/jvm/java-11-openjdk/</code> is the actual JVM install location.</li>
</ul>

<h3 id="_image_entrypoint_what_does_the_operator_run">Image <code>EntryPoint</code> - What Does the Operator Run?</h3>
<div class="section">
<p>The image does not need to have an <code>EntryPoint</code> or command specified, it does not need to actually be executable.
If the image does have an <code>EntryPoint</code>, it will just be ignored.</p>

<p>The Coherence Operator actually injects its own <code>runner</code> executable into the container which the container runs and which
in turn builds the Java command line to execute. The <code>runner</code> process looks at arguments and environment variables configured
for the Coherence container and from these constructs a Java command line that it then executes.</p>

<p>The default command might look something like this:</p>

<pre
lang="bash"

>java -cp `/app/resources:/app/classes:/app/libs/*` \
    &lt;JVM args&gt; \
    &lt;System Properties&gt; \
    com.tangosol.net.DefaultCacheServer</pre>

<p>The <code>runner</code> will work out the JVM&#8217;s classpath, args and system properties to add to the command line
and execute the main class <code>com.tangosol.net.DefaultCacheServer</code>.
All these are configurable in the <code>Coherence</code> resource spec.</p>

</div>

<h3 id="_optional_classpath_environment_variable">Optional <code>CLASSPATH</code> Environment Variable</h3>
<div class="section">
<p>If the <code>CLASSPATH</code> environment variable has been set in an image that classpath will be used when running the Coherence
container. Other elements may also be added to the classpath depending on the configuration of the <code>Coherence</code> resource.</p>

</div>

<h3 id="_setting_the_classpath">Setting the Classpath</h3>
<div class="section">
<p>An application image contains <code>.jar</code> files (at least <code>coherence.jar</code>), possibly Java class files, also possibly
other ad-hoc files, all of which need to be on the application&#8217;s classpath.
There are certain classpath values that the operator supports out of the box without needing any extra configuration,
but for occasions where the location of files in the image does not match the defaults a classpath can be specified.</p>

<p>Images built with <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib/blob/master/README.md">JIB</a>
have a default classpath of <code>/app/resources:/app/classes:/app/libs/*</code>.
When the Coherence container starts if the directories <code>/app/resources</code>, <code>/app/classes</code> or <code>/app/libs/</code> exist in the
image they will automatically be added to the classpath of the JVM. In this way the Operator supports standard JIB
images without requiring additional configuration.</p>

<p>If the image is not a JIB image, or is a JIB image without the standard classpath but one or more of the
<code>/app/resources</code>, <code>/app/classes</code> or <code>/app/libs/</code> directories exist they will still be added to the classpath.
This may be desired or in some cases it may cause issues. It is possible to disable automatically adding these
directories in the <code>Coherence</code> resource spec by setting the <code>jvm.useJibClasspath</code> field to <code>false</code> (the default
value of the field is <code>true</code>).</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    useJibClasspath: false  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>useJibClasspath</code> is set to <code>false</code>. Even if any of the the <code>/app/resources</code>, <code>/app/classes</code> or <code>/app/libs/</code>
directories exist in the image they will not be added to the classpath.</li>
</ul>
<p>If the image is not a JIB image, or is a JIB image without the standard classpath, then additional classpath entries
can be configured as described in the <router-link to="/jvm/020_classpath">setting the classpath</router-link> documentation.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    classpath:          <span class="conum" data-value="1" />
      - "/data/libs/*"  <span class="conum" data-value="2" />
      - "/data/config"</pre>

<ul class="colist">
<li data-value="1">The <code>jvm.classpath</code> field will be used to add additional items to the classpath, the field is a list of strings.</li>
<li data-value="2">Each entry in the <code>jvm.classpath</code> will be appended to the classpath exactly as it is declared, so in this case
the classpath will be <code>/data/libs/*:/data/config</code></li>
</ul>
</div>

<h3 id="_optional_java_home_environment_variable">Optional <code>JAVA_HOME</code> Environment Variable</h3>
<div class="section">
<p>The <code>JAVA_HOME</code> environment variable does not have to be set in the image. If it is set the JVM at that location will
be used to run the application. If it is not set then the <code>java</code> executable <strong>must</strong> be on the <code>PATH</code> in the image.</p>

</div>

<h3 id="_optional_coherence_home_environment_variable">Optional <code>COHERENCE_HOME</code> Environment Variable</h3>
<div class="section">
<p>The <code>COHERENCE_HOME</code> environment variable does not have to be set in an image.
Typically, all the jar files, including <code>coherence.jar</code> would be packaged into a single directory which is then used as
the classpath.
It is possible to run official Coherence images published by Oracle, which have <code>COHERENCE_HOME</code> set, which is then used
by the Operator to set the classpath.</p>

<p>If the <code>COHERENCE_HOME</code> environment variable is set in an image the following entries will be added to the end of the
classpath:</p>

<ul class="ulist">
<li>
<p><code>$COHERENCE_HOME/lib/coherence.jar</code></p>

</li>
<li>
<p><code>$COHERENCE_HOME/conf</code></p>

</li>
</ul>
</div>

<h3 id="_additional_data_volumes">Additional Data Volumes</h3>
<div class="section">
<p>If the application requires access to external storage volumes in Kubernetes it is possible to add additional <code>Volumes</code>
and <code>VolumeMappings</code> to the Pod and containers.</p>

<p>There are three ways to add additional volumes:</p>

<ul class="ulist">
<li>
<p>ConfigMaps - easily add a <code>ConfigMap</code> volume and volume mapping see: <router-link to="/other/050_configmap_volumes">Add ConfigMap Volumes</router-link></p>

</li>
<li>
<p>Secrets - easily add a <code>Secret</code> volume and volume mapping see: <router-link to="/other/060_secret_volumes">Add Secret Volumes</router-link></p>

</li>
<li>
<p>Volumes - easily add any additional volume and volume mapping see: <router-link to="/other/070_add_volumes">Add Volumes</router-link></p>

</li>
</ul>
<p>Both of <code>ConfigMaps</code> and <code>Secrets</code> have been treated as a special case because they are quite commonly used to provide
configurations to Pods, so the <code>Coherence</code> spec provides a simpler way to declare them than for ad-hoc <code>Volumes</code>.</p>

</div>
</div>
</doc-view>
<!-- pages/applications/030_deploy_application.js -->
<doc-view>

<h2 id="_deploy_coherence_application_images">Deploy Coherence Application Images</h2>
<div class="section">
<p>Once a custom application image has been built (as described in <router-link to="/applications/020_build_application">Build Custom Application Images</router-link>)
a <code>Coherence</code> resource can be configured to use that image.</p>


<h3 id="_specify_the_image_to_use">Specify the Image to Use</h3>
<div class="section">
<p>To specify the image to use set the <code>image</code> field in the <code>Coherence</code> spec to the name of the image.</p>

<p>For example if there was an application image called <code>catalogue:1.0.0</code> it can be specified like this:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>catalogue:1.0.0</code> will be used in the <code>coherence</code> container in the deployment&#8217;s Pods.</li>
</ul>
<p>The example above would assume that the <code>catalogue:1.0.0</code> has a JVM on the <code>PATH</code> and all the required <code>.jar</code> files,
or Java classes, in the default classpath locations used by the Operator.</p>

</div>

<h3 id="_image_pull_secrets">Image Pull Secrets</h3>
<div class="section">
<p>If your image needs to be pulled from a private registry you may need to provide
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">image pull secrets</a> for this.</p>

<p>For example, supposing the application image is <code>repo.acme.com/catalogue:1.0.0</code> and that <code>repo.acme.com</code> is a private registry; we might a <code>Secret</code> to the k8s namespace named <code>repo-acme-com-secrets</code>. We can then specify that these secrets are used in the <code>Coherence</code> resource by setting the <code>imagePullSecrets</code> fields. The <code>imagePullSecrets</code> field is a list of secret names, the same format as that used when specifying secrets for a Pod spec.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: repo.acme.com/catalogue:1.0.0  <span class="conum" data-value="1" />
  imagePullSecrets:
    - name: repo-acme-com-secrets       <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>repo.acme.com/catalogue:1.0.0</code> image will be used for the application image</li>
<li data-value="2">The <code>Secret</code> named <code>repo-acme-com-secrets</code> will be used to pull images.</li>
</ul>
<p>Multiple secrets can be specified in the case where different images used by different containers are pulled from different registries.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: repo.acme.com/catalogue:1.0.0
  imagePullSecrets:
    - name: repo-acme-com-secrets               <span class="conum" data-value="1" />
    - name: oracle-container-registry-secrets</pre>

<ul class="colist">
<li data-value="1">The example above has two image pull secrets, <code>repo-acme-com-secrets</code> and <code>oracle-container-registry-secrets</code></li>
</ul>
</div>

<h3 id="_more_application_configuration">More Application Configuration</h3>
<div class="section">
<p>Additional configuration can be added to specify other application settings, these include:</p>

<ul class="ulist">
<li>
<p>setting the <router-link to="/jvm/020_classpath">classpath</router-link></p>

</li>
<li>
<p>specifying the <router-link to="/applications/040_application_main">application main</router-link></p>

</li>
<li>
<p>specifying <router-link to="/applications/050_application_args">application arguments</router-link></p>

</li>
<li>
<p>specifying the <router-link to="/applications/060_application_working_dir">working directory</router-link></p>

</li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/applications/040_application_main.js -->
<doc-view>

<h2 id="_set_the_application_main">Set the Application Main</h2>
<div class="section">
<p>The Coherence container in the deployment&#8217;s Pods will, by default, run <code>com.tangosol.net.DefaultCacheServer</code> as the Java main class.
It is possible to change this when running a custom application that requires a different main.</p>

<p>The name of the main is set in the <code>application.main</code> field in the <code>Coherence</code> spec.</p>

<p>For example, if the deployment is using a custom image <code>catalogue:1.0.0</code> that requires a custom main class
called <code>com.acme.Catalogue</code> the <code>Coherence</code> resource would look like this:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    main: com.acme.Catalogue <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>com.acme.Catalogue</code> will be run as the main class.</li>
</ul>
<p>The example would be equivalent to the Coherence container running:</p>

<pre
lang="bash"

>$ java com.acme.Catalogue</pre>

</div>
</doc-view>
<!-- pages/applications/050_application_args.js -->
<doc-view>

<h2 id="_set_application_arguments">Set Application Arguments</h2>
<div class="section">
<p>When running a custom application there may be a requirement to pass arguments to the application&#8217;s main class.
By default, there are no application arguments but any arguments required can be specified in the <code>application.args</code> list
in the <code>Coherence</code> resource spec.</p>

<p>The <code>application.args</code> is a list of string values, each value in the list is passed as an argument, in the order
that they are specified in the list.</p>

<p>For example, a deployment uses a custom image <code>catalogue:1.0.0</code> that requires a custom main class
called <code>com.acme.Catalogue</code>, and that class takes additional arguments.
In this example we&#8217;ll use two fictitious arguments such as a name and a language for the catalogue.
the <code>Coherence</code> resource would look like this:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    main: com.acme.Catalogue <span class="conum" data-value="1" />
    args:                    <span class="conum" data-value="2" />
      - "--name=Books"
      - "--language=en_GB"</pre>

<ul class="colist">
<li data-value="1">The <code>com.acme.Catalogue</code> will be run as the main class.</li>
<li data-value="2">The arguments passed to the <code>com.acme.Catalogue</code> class will be <code>--name=Books</code> and <code>--language=en_GB</code></li>
</ul>
<p>The example would be equivalent to the Coherence container running:</p>

<pre
lang="bash"

>$ java com.acme.Catalogue --name=Books --language=en_GB</pre>

</div>

<h2 id="_environment_variable_expansion">Environment Variable Expansion</h2>
<div class="section">
<p>The Operator supports environment variable expansion in program arguments.
The runner in the Coherence container will replace <code>${var}</code> or <code>$var</code> in the program arguments with the corresponding environment variable name.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    main: com.acme.Catalogue
    args:
      - "${HOSTNAME}"  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The argument passed to the <code>com.acme.Catalogue</code> main method will resolve to the value of the <code>HOSTNAME</code> environment variable.</li>
</ul>
<p>Any environment variable that is present when the Coherence container starts can be used, this would include variables created as part of the image and variables specified in the Coherence yaml.</p>

</div>
</doc-view>
<!-- pages/applications/060_application_working_dir.js -->
<doc-view>

<h2 id="_set_the_working_directory">Set the Working Directory</h2>
<div class="section">
<p>When running a custom application there may be a requirement to run in a specific working directory.
The working directory can be specified in the <code>application.workingDir</code> field in the <code>Coherence</code> spec.</p>

<p>For example, a deployment uses a custom image <code>catalogue:1.0.0</code> that requires a custom main class
called <code>com.acme.Catalogue</code>, and that class takes additional arguments.
In this example we&#8217;ll use two fictitious arguments such as a name and a language for the catalogue.
the <code>Coherence</code> resource would look like this:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0          <span class="conum" data-value="1" />
  application:
    workingDir: "/apps/catalogue" <span class="conum" data-value="2" />
    main: "com.acme.Catalogue"    <span class="conum" data-value="3" /></pre>

<ul class="colist">
<li data-value="1">The <code>catalogue:1.0.0</code> image will be used.</li>
<li data-value="2">The Java command will be executed in the <code>/apps/catalogue</code> working directory.</li>
<li data-value="3">The Java main class executed will be <code>com.acme.Catalogue</code></li>
</ul>
<p>The example would be equivalent to the Coherence container running:</p>

<pre
lang="bash"

>$ cd /apps/catalogue
$ java com.acme.Catalogue</pre>

</div>
</doc-view>
<!-- pages/applications/070_spring.js -->
<doc-view>

<h2 id="_spring_boot_applications">Spring Boot Applications</h2>
<div class="section">
<p>The Coherence Operator supports running images that contain Spring Boot applications.
Exactly how easy this is depends on how the image has been built.</p>

<p>When the operator runs an image it overrides the default image entrypoint and uses its own launcher.
This allows the operator to properly configure various Coherence properties that the launcher then uses to build the
command line to actually run your application. With some types of image this is not a straight forward Java command line
so the Operator requires a bit more information adding to the <code>Coherence</code> deployment yaml.</p>


<h3 id="_using_jib_images">Using JIB Images</h3>
<div class="section">
<p>The simplest way to build an application image to run with the Coherence Operator (including Spring Boot applications)
is to use the <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib/blob/master/README.md">JIB</a> tool.
JIB images will work out of the box with the operator, even for a Spring Boot application, as described in
<router-link to="/applications/020_build_application">Building Applications</router-link> and
<router-link to="/applications/030_deploy_application">Deploying Applications</router-link>.</p>

<p>If you have used the Spring Maven or Gradle plugins to build the application into a fat jar, but you then build the image
using the <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib/blob/master/README.md">JIB</a> plugin then JIB will detect the fat
jar and package the image in an exploded form that will run out of the box with the operator.</p>

</div>

<h3 id="_using_an_exploded_spring_boot_image">Using an Exploded Spring Boot Image</h3>
<div class="section">
<p>Another way to build a Spring Boot image is to explode the Spring Boot jar into a directory structure in the image.</p>

<p>For example, if a Spring Boot jar has been exploded into a directory called <code>/spring</code>, the image contents might look
like the diagram below; where you can see the <code>/spring</code> directory contains the Spring Boot application.</p>

<pre


>&#9500;&#9472;&#9472; bin
&#9500;&#9472;&#9472; boot
&#9500;&#9472;&#9472; dev
&#9500;&#9472;&#8853; etc
&#9500;&#9472;&#8853; home
&#9500;&#9472;&#8853; lib
&#9500;&#9472;&#8853; lib64
&#9500;&#9472;&#9472; proc
&#9500;&#9472;&#9472; root
&#9500;&#9472;&#9472; run
&#9500;&#9472;&#9472; sbin
&#9500;&#9472;&#9472; spring
&#9474;   &#9500;&#9472;&#9472; BOOT-INF
&#9474;   &#9474;   &#9500;&#9472;&#8853; classes
&#9474;   &#9474;   &#9500;&#9472;&#9472; classpath.idx
&#9474;   &#9474;   &#9492;&#9472;&#8853; lib
&#9474;   &#9500;&#9472;&#9472; META-INF
&#9474;   &#9474;   &#9500;&#9472;&#9472; MANIFEST.MF
&#9474;   &#9474;   &#9492;&#9472;&#8853; maven
&#9474;   &#9492;&#9472;&#9472; org
&#9474;       &#9492;&#9472;&#9472; springframework
&#9474;           &#9492;&#9472;&#8853; boot
&#9500;&#9472;&#9472; sys
&#9500;&#9472;&#9472; tmp
&#9500;&#9472;&#8853; usr
&#9492;&#9472;&#8853; var</pre>

<p>This type of image can be run by the Coherence Operator by specifying an application type of <code>spring</code> in the
<code>spec.application.type</code> field and by setting the working directory to the exploded directory, for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: my-spring-app:1.0.0
  application:
    type: spring         <span class="conum" data-value="1" />
    workingDir: /spring  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>type</code> field set to <code>spring</code> tells the Operator that this is a Spring Boot application.</li>
<li data-value="2">The working directory has been set to the directory containing the exploded Spring Boot application.</li>
</ul>
<p>When the Operator starts the application it will then run a command equivalent to:</p>

<pre
lang="bash"

>cd /spring &amp;&amp; java org.springframework.boot.loader.PropertiesLauncher</pre>

</div>

<h3 id="_using_a_spring_boot_fat_jar">Using a Spring Boot Fat Jar</h3>
<div class="section">
<p>It is not recommended to build images containing fat jars for various reasons which can easily be found on the internet.
If you feel that you must build your application as a Spring Boot fat jar then this can still work with the Coherence Operator.</p>

<p>The Java command line to run a Spring Boot fat jar needs to be something like <code>java -jar my-app.jar</code>
where <code>my-app.jar</code> is the fat jar.
This means that the Operator&#8217;s launcher needs to know the location of the fat jar in the image, so this must
be provided in the <code>Coherence</code> deployment yaml.</p>

<p>For example, suppose that an application has been built into a fat jar names <code>catalogue-1.0.0.jar</code> which is in the
<code>/app/libs</code> directory in the image, so the full path to the jar is <code>/app/libs/catalogue-1.0.0.jar</code>.
This needs to be set in the <code>spec.applicaton.springBootFatJar</code> field of the <code>Coherence</code> yaml.</p>

<p>The <code>spec.application.type</code> field also needs to be set to <code>spring</code> so that the Operator knows that this is a
Spring Boot application</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    type: spring                                      <span class="conum" data-value="1" />
    springBootFatJar: /app/libs/catalogue-1.0.0.jar   <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>type</code> field set to <code>spring</code> tells the Operator that this is a Spring Boot application.</li>
<li data-value="2">The location of the Spring Boot jar has been set.</li>
</ul>
<p>When the Operator starts the application it will then run a command equivalent to:</p>

<pre
lang="bash"

>java -cp /app/libs/catalogue-1.0.0.jar org.springframework.boot.loader.PropertiesLauncher</pre>

<div class="admonition note">
<p class="admonition-inline">The Operator does not run the fat jar using the <code>java -jar</code> command because it needs to add various other
JVM arguments and append to the classpath, so it has to run the <code>org.springframework.boot.loader.PropertiesLauncher</code>
class as opposed to the <code>org.springframework.boot.loader.JarLauncher</code> that <code>java -jar</code> would run.</p>
</div>
</div>

<h3 id="_using_could_native_buildpacks">Using Could Native Buildpacks</h3>
<div class="section">
<p>If the Spring Boot Maven or Gradle plugin has been used to produce an image using
<a id="" title="" target="_blank" href="https://spring.io/blog/2020/01/27/creating-docker-images-with-spring-boot-2-3-0-m1">Cloud Native Buildpacks</a>
these images can work with the Coherence Operator.</p>

<div class="admonition warning">
<p class="admonition-textlabel">Warning</p>
<p ><p>Due to limitation on the way that arguments can be passed to the JVM when using Buildpacks images the Coherence
operator will only work with images containing a JVM greater than Java 11.
Although the Buildpacks launcher will honour the <code>JAVA_OPTS</code> or <code>JAVA_TOOL_OPTIONS</code> environment variables there appear
to be size limitations for the values of these variables that make it impractical for the Operator to use them.
The Operator therefore creates a JVM arguments file to pass the arguments to the JVM.
At the time of writing these docs, Java 8 (which is the default version of Java used by the Spring Boot plugin) does not
support the use of argument files for the JVM.</p>

<p>It is simple to configure the version of the JVM used by the Spring Boot plugin, for example in Maven:</p>

<pre
lang="xml"

>&lt;plugin&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
  &lt;configuration&gt;
    &lt;image&gt;
      &lt;env&gt;
        &lt;BP_JVM_VERSION&gt;11.*&lt;/BP_JVM_VERSION&gt;
      &lt;/env&gt;
    &lt;/image&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;</pre>
</p>
</div>
<p>When creating a <code>Coherence</code> deployment for a Spring Boot Buildpacks image The application type must be set to <code>spring</code>.
The Operator&#8217;s launcher will automatically detect that the image is a Buildpacks image and launch the application using
the Buildpacks launcher.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    type: spring <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The application type has been set to <code>spring</code> so that the operator knows that this is a Spring Boot application,
and the fact that the image is a Buildpacks image will be auto-discovered.</li>
</ul>
<p>When the Operator starts the application it will then run the buildpacks launcher with a command equivalent
to this:</p>

<pre
lang="bash"

>/cnb/lifecycle/launcher java @jvm-args-file org.springframework.boot.loader.PropertiesLauncher</pre>


<h4 id="_buildpacks_detection">Buildpacks Detection</h4>
<div class="section">
<p>If for some reason buildpacks auto-detection does not work properly the <code>Coherence</code>
CRD contains a filed to force buildpacks to be enabled or disabled.</p>

<p>The <code>boolean</code> field <code>spec.application.cloudNativeBuildPack.enabled</code> can be set to <code>true</code> to enable buildpacks or false
to disable buildpack.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    type: spring            <span class="conum" data-value="1" />
    cloudNativeBuildPack:
      enabled: true         <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The application type has been set to <code>spring</code> so that the operator knows that this is a Spring Boot application</li>
<li data-value="2">The <code>cloudNativeBuildPack.enabled</code> field has been set to <code>true</code> to force the Operator to use the Buildpacks launcher.</li>
</ul>
</div>

<h4 id="_specify_the_buildpacks_launcher">Specify the Buildpacks Launcher</h4>
<div class="section">
<p>A Cloud Native Buildpacks image uses a launcher mechanism to run the executable(s) in the image. The Coherence Operator
launcher will configure the application and then invoke the same buildpacks launcher.
The Coherence Operator assumes that the buildpacks launcher is in the image in the location <code>/cnb/lifecycle/launcher</code>.
If a buildpacks image has been built with the launcher in a different location then the <code>Coherence</code> CRD contains
a field to set the new location.</p>

<p>The <code>spec.application.cloudNativeBuildPack.enabled</code> field.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  image: catalogue:1.0.0
  application:
    type: spring                    <span class="conum" data-value="1" />
    cloudNativeBuildPack:
      launcher: /buildpack/launcher <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The application type has been set to <code>spring</code> so that the operator knows that this is a Spring Boot application</li>
<li data-value="2">The buildpacks launcher that the Operator will invoke is located at <code>/buildpack/launcher</code>.</li>
</ul>
</div>

<h4 id="_buildpack_jvm_arguments">Buildpack JVM Arguments</h4>
<div class="section">
<p>A typical Spring Boot buildpack launcher will attempt to configure options such as heap size based on the container
resource limits configured, so this must be taken into account if using any of the memory options available in the
<code>Coherence</code> CRD as there may be conflicting configurations.</p>

</div>
</div>
</div>
</doc-view>
<!-- pages/coherence/010_overview.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Overview</dt>
<dd slot="desc"><p>The <code>Coherence</code> resource has a number of fields to configure the behaviour of <code>Coherence</code>,
these fields are in the <code>spec.coherence</code> section of the CRD.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_configuring_coherence">Configuring Coherence</h2>
<div class="section">
<p>The <code>Coherence</code> CRD has specific fields to configure the most common Coherence settings.
Any other settings can be configured by adding system properties to the <router-link to="/jvm/010_overview">JVM Settings</router-link>.</p>

<p>The following Coherence features can be directly specified in the <code>Coherence</code> spec.</p>

<ul class="ulist">
<li>
<p><router-link to="/coherence/020_cluster_name">Cluster Name</router-link></p>

</li>
<li>
<p><router-link to="/coherence/030_cache_config">Cache Configuration File</router-link></p>

</li>
<li>
<p><router-link to="/coherence/040_override_file">Operational Configuration File</router-link> (aka, the override file)</p>

</li>
<li>
<p><router-link to="/coherence/050_storage_enabled">Storage Enabled</router-link> or disabled deployments</p>

</li>
<li>
<p><router-link to="/coherence/060_log_level">Log Level</router-link></p>

</li>
<li>
<p><router-link to="/coherence/070_wka">Well Known Addressing</router-link> and cluster discovery</p>

</li>
<li>
<p><router-link to="/coherence/080_persistence">Persistence</router-link></p>

</li>
<li>
<p><router-link to="/management/010_overview">Management over REST</router-link></p>

</li>
<li>
<p><router-link to="/metrics/010_overview">Metrics</router-link></p>

</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">The Coherence settings in the <code>Coherence</code> CRD spec typically set system property values that will
be passed through to the Coherence JVM command line, which in turn configure Coherence.
This is the same behaviour that would occur when running Coherence outside of containers.
Whether these system properties actually apply or not depends on the application code. For example,
it is simple to override the Coherence operational configuration file in a jar file deployed as part of an
application&#8217;s image in such a way that will cause all the normal Coherence system properties to be ignored.
If that is done then the Coherence settings discussed in this documentation will not apply.<br>
For example, adding a <code>tangosol-coherence-override.xml</code> file to a jar on the application&#8217;s classpath that contains
an overridden <code>&lt;configurable-cache-factory-config&gt;</code> section with a hard coded cache configuration file name would
mean that the <code>Coherence</code> CRD <code>spec.coherence.cacheConfig</code> field, that sets the <code>coherence.cacheconfig</code> system
property, would be ignored.<br>
It is, therefore, entirely at the application developer&#8217;s discretion whether they use the fields of the <code>Coherence</code> CRD
to configure Coherence, or they put those settings into configuration files, either hard coded into jar files or
picked up at runtime from files mapped from Kubernetes volumes, config maps, secrets, etc.</p>
</div>
</div>
</doc-view>
<!-- pages/coherence/020_cluster_name.js -->
<doc-view>

<h2 id="_set_coherence_cluster_name">Set Coherence Cluster Name</h2>
<div class="section">
<p>The name of the Coherence cluster that a <code>Coherence</code> resource is part of can be set with the <code>cluster</code> field
in the <code>Coherence.Spec</code>. The cluster name is used to set the <code>coherence.cluster</code> system property in the JVM in the Coherence container.</p>


<h3 id="_default_cluster_name">Default Cluster Name</h3>
<div class="section">
<p>The default Coherence cluster name, used when the <code>cluster</code> field is empty, will be the same as the name of the <code>Coherence</code> resource, for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The name of this <code>Coherence</code> resource is <code>test</code>, which will also be used as the Coherence cluster name, effectively passing <code>-Dcoherence.cluster=test</code> to the JVM in the Coherence container.</li>
</ul>
</div>

<h3 id="_specify_a_cluster_name">Specify a Cluster Name</h3>
<div class="section">
<p>In a use case where multiple <code>Coherence</code> resources will be created to form a single Coherence cluster, the <code>cluster</code>
field in all the <code>Coherence</code> resources needs to be set to the same value.</p>

<pre
lang="yaml"
title="cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  cluster: test-cluster
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: front-end
spec:
  cluster: test-cluster</pre>

<p>The yaml above contains two <code>Coherence</code> resources, one named <code>storage</code> and one named <code>front-end</code>.
Both of these <code>Coherence</code> resources have the same value for the <code>cluster</code> field, <code>test-cluster</code>,
so the Pods in both deployments will form a single Coherence cluster named <code>test</code>.</p>

</div>
</div>
</doc-view>
<!-- pages/coherence/021_member_identity.js -->
<doc-view>

<h2 id="_member_identity">Member Identity</h2>
<div class="section">
<p>Each JVM in a Coherence cluster has an identity. This is made up of a number of values for <code>site</code>, <code>rack</code>, <code>member</code>,
<code>machine</code> and <code>node-id</code>.
The <code>node-id</code> is assigned by Coherence when a node joins a cluster.
The other values can be assigned using system properties, or will have defaults assigned by Coherence if not set.
The Coherence Operator will configure properties for these values.</p>

<ul class="ulist">
<li>
<p>The member name is set to the Pod name.</p>

</li>
<li>
<p>The machine name is set to the name of the Node that the Pod has been scheduled onto.</p>

</li>
<li>
<p>The rack name is taken from the <code>topology.kubernetes.io/region</code> label on the Node that the Pod has been scheduled onto.
If the <code>topology.kubernetes.io/region</code> label is not set then the deprecated <code>failure-domain.beta.kubernetes.io/region</code>
label will be tried.
If neither of these labels are set then the site will be set to the same value as the site name.</p>

</li>
<li>
<p>The site name is taken from the <code>topology.kubernetes.io/zone</code> label on the Node that the Pod has been scheduled onto.
If the <code>topology.kubernetes.io/zone</code> label is not set then the deprecated <code>failure-domain.beta.kubernetes.io/zone</code> label
will be tried.
If neither of these labels are set then the site and rack will be unset, and the cache services may not reach site safe.</p>

</li>
</ul>
</div>

<h2 id="_status_ha_values">Status HA Values</h2>
<div class="section">
<p>As well as identifying cluster members, these values are also used by the partitioned cache service to distribute data
as widely (safely) as possible in the cluster. The backup owner will be as far away as possible from the primary owner.
Ideally this would be on a member with a different site; failing that, a different rack, machine and finally member.</p>

</div>

<h2 id="_changing_site_and_rack_values">Changing Site and Rack Values</h2>
<div class="section">
<p>You should not usually need to change the default values applied for the <code>member</code> and <code>machine</code> names, but you may need
to change the values used for the site, or rack. The labels used for the <code>site</code> and <code>rack</code> are standard k8s labels but
the k8s cluster being used may not have these labels set</p>


<h3 id="_apply_node_labels">Apply Node Labels</h3>
<div class="section">
<p>One solution to missing site and rack values is to apply the required labels to the Nodes in the k8s cluster.</p>

<p>For example the command below labels the node in Docker dDesktop on MacOS to "twighlight-zone".</p>

<pre
lang="bash"

>kubectl label node docker-desktop topology.kubernetes.io/zone=twighlight-zone</pre>

</div>

<h3 id="_specify_site_and_rack_using_system_properties">Specify Site and Rack Using System Properties</h3>
<div class="section">
<p>The site and rack values can be specified as system properties as part of the Coherence deployment yaml.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: my-cluster
spec:
  jvm:
    args:
      - "-Dcoherence.site=foo"
      - "-Dcoherence.rack=fbar"</pre>

<p>In the deployment above the site name is set to "foo" using the <code>coherence.site</code> system property.
The rack name is set to "bar" using the <code>coherence.rack</code> system property.</p>

</div>

<h3 id="_configure_the_operator_to_use_different_labels">Configure the Operator to Use Different Labels</h3>
<div class="section">
<p>The Operator can be configured to use different labels to obtain values for the site and rack names.
This will obviously apply to all Coherence deployments managed by the Operator, but is useful if the Nodes in the
k8s cluster do not have the normal k8s labels.
The <code>SITE_LABEL</code> and <code>RACK_LABEL</code> environment variables are used to specify different labels to use.
How these environment variables are set depends on how you are installing the Operator.</p>


<h4 id="_using_helm">Using Helm</h4>
<div class="section">
<p>If the Operator is installed using the Helm chart then the site and rack labels can be set using the
<code>siteLabel</code> and <code>rackLabel</code> values;
for example:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    --set siteLabel=identity/site \
    --set siteLabel=identity/rack \
    coherence-operator \
    coherence/coherence-operator</pre>

<p>In the example above the Node label used by the Operator to get the value for the site will be <code>identity/site</code>,
and the Node label used to get the value for the rack will be <code>identity/rack</code>.</p>

</div>

<h4 id="_using_kubectl_or_kustomize">Using Kubectl or Kustomize</h4>
<div class="section">
<p>If using <code>kubectl</code> or <code>kustomize</code> as described in the <router-link to="/installation/01_installation">Installation Guide</router-link>
the additional environment variables can be applied using <code>kustomize</code> commands.</p>

<pre
lang="bash"

>cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal SITE_LABEL='identity/site'</pre>

<pre
lang="bash"

>cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal RACK_LABEL='identity/rack'</pre>

</div>
</div>
</div>
</doc-view>
<!-- pages/coherence/030_cache_config.js -->
<doc-view>

<h2 id="_set_the_cache_configuration_file_name">Set the Cache Configuration File Name</h2>
<div class="section">
<p>The name of the Coherence cache configuration file that the Coherence processes in a <code>Coherence</code> resource will
use can be set with the <code>spec.coherence.cacheConfig</code> field. By setting this field the <code>coherence.cacheconfig</code> system
property will be set in the Coherence JVM.</p>

<p>When the <code>spec.coherence.cacheConfig</code> is blank or not specified, Coherence use its default behaviour to find the
cache configuration file to use. Typically, this is to use the first occurrence of <code>coherence-cache-config.xml</code> that is
found on the classpath
(consult the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/develop-applications/understanding-configuration.html#GUID-360B798E-2120-44A9-8B09-1FDD9AB40EB5">Coherence documentation</a>
for an explanation of the default behaviour).</p>

<p>To set a specific cache configuration file to use set the <code>spec.coherence.cacheConfig</code> field, for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    cacheConfig: storage-cache-config.xml <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>spec.coherence.cacheConfig</code> field has been set to <code>storage-cache-config.xml</code> which will effectively pass
<code>-Dcoherence.cacheconfig=storage-cache-config.xml</code> to the JVM command line.</li>
</ul>
</div>
</doc-view>
<!-- pages/coherence/040_override_file.js -->
<doc-view>

<h2 id="_set_the_operational_configuration_file_name">Set the Operational Configuration File Name</h2>
<div class="section">
<p>The name of the Coherence operations configuration file (commonly called the overrides file) that the Coherence processes
in a <code>Coherence</code> resource will use can be set with the <code>spec.coherence.overrideConfig</code> field.
By setting this field the <code>coherence.override</code> system property will be set in the Coherence JVM.</p>

<p>When the <code>spec.coherence.overrideConfig</code> is blank or not specified, Coherence use its default behaviour to find the
operational configuration file to use. Typically, this is to use the first occurrence of <code>tangosol-coherence-override.xml</code>
that is found on the classpath
(consult the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/develop-applications/understanding-configuration.html#GUID-360B798E-2120-44A9-8B09-1FDD9AB40EB5">Coherence documentation</a>
for an explanation of the default behaviour).</p>

<p>To set a specific operational configuration file to use set the <code>spec.coherence.overrideConfig</code> field, for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    overrideConfig: test-override.xml <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>spec.coherence.overrideConfig</code> field has been set to <code>test-override.xml</code> which will effectively pass
<code>-Dcoherence.override=test-override.xml</code> to the JVM command line.</li>
</ul>
</div>
</doc-view>
<!-- pages/coherence/050_storage_enabled.js -->
<doc-view>

<h2 id="_storage_enabled_or_disabled_deployments">Storage Enabled or Disabled Deployments</h2>
<div class="section">
<p>Partitioned cache services that manage Coherence caches are configured as storage enabled or storage disabled.
Whilst it is possible to configure individual services to be storage enabled or disabled in the cache configuration file
and have a mixture of modes in a single JVM, typically all the services in a JVM share the same mode by setting the
<code>coherence.distributed.localstorage</code> system property to <code>true</code> for storage enabled members and to <code>false</code> for
storage disabled members. The <code>Coherence</code> CRD allows this property to be set by specifying the
<code>spec.coherence.storageEnabled</code> field to either true or false. The default value when nothing is specified is <code>true</code>.</p>

<pre
lang="yaml"
title="storage enabled"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    storageEnabled: true  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>Coherence</code> resource specifically sets <code>coherence.distributed.localstorage</code> to <code>true</code></li>
</ul>
<pre
lang="yaml"
title="storage disabled"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    storageEnabled: false  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>Coherence</code> resource specifically sets <code>coherence.distributed.localstorage</code> to <code>false</code></li>
</ul>
</div>
</doc-view>
<!-- pages/coherence/060_log_level.js -->
<doc-view>

<h2 id="_set_the_coherence_log_level">Set the Coherence Log Level</h2>
<div class="section">
<p>Logging granularity in Coherence is controlled by a log level, that is a number between one and nine,
where the higher the number the more debug logging is produced. The <code>Coherence</code> CRD has a field
<code>spec.coherence.logLevel</code> that allows the log level to be configured by setting the <code>coherence.log.level</code>
system property.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    logLevel: 9  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>Coherence</code> spec sets the log level to 9, effectively passing <code>-Dcoherence.log.level=9</code> to the Coherence
JVM&#8217;s command line.</li>
</ul>
</div>
</doc-view>
<!-- pages/coherence/070_wka.js -->
<doc-view>

<h2 id="_well_known_addressing_and_cluster_discovery">Well Known Addressing and Cluster Discovery</h2>
<div class="section">
<p>A Coherence cluster is made up of one or more JVMs. In order for these JVMs to form a cluster they need to be able to
discover other cluster members. The default mechanism for discovery is multicast broadcast but this does not work in
most container environments. Coherence provides an alternative mechanism where the addresses of the hosts where the
members of the cluster will run is provided in the form of a
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/develop-applications/setting-cluster.html#GUID-E8CC7C9A-5739-4D12-B88E-A3575F20D63B">"well known address" (or WKA) list</a>.
This address list is then used by Coherence when it starts in a JVM to discover other cluster members running on the
hosts in the WKA list.</p>

<p>When running in containers each container is effectively a host and has its own host name and IP address (or addresses)
and in Kubernetes it is the <code>Pod</code> that is effectively a host. When starting a container it is usually not possible to
know in advance what the host names of the containers or <code>Pods</code> will be so there needs to be another solution to
providing the WKA list.</p>

<p>Coherence processes a WKA list it by performing a DNS lookup for each host name in the list. If a host name resolves
to more than one IP address then <em>all</em> of those IP addresses will be used in cluster discovery. This feature of Coherence
when combined with Kubernetes <code>Services</code> allows discovery of cluster members without resorting to a custom discovery
mechanism.</p>

<p>A Kubernetes <code>Service</code> has a DNS name and that name will resolve to all the IP addresses of the <code>Pods</code> that match
that <code>Service</code> selector. This means that a Coherence JVM only needs to be given the DNS name of a <code>Service</code> as the
single host name in its WKA list so that it will form a cluster with any other JVM using in a Pod matching the selector.</p>

<p>When the Coherence Operator creates reconciles a <code>Coherence</code> CRD configuration to create a running set of <code>Pods</code>
it creates a headless service specifically for the purposes of WKA for that <code>Coherence</code> resource with a selector that
matches any Pod with the same cluster name.</p>

<p>For example, if a <code>Coherence</code> resource is created with the following yaml:</p>

<pre
lang="yaml"
title="test-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  cluster: test-cluster <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">In this yaml the <code>Coherence</code> resource has a cluster name of <code>test-cluster</code></li>
</ul>
<p>The Operator will create a <code>Service</code> for the <code>Coherence</code> resource using the same name as the deployment
with a <code>-wka</code> suffix.
So in the example above the Operator would create a <code>Service</code> with the name <code>storage-wka</code>.</p>

<p>The yaml for the WKA <code>Service</code> would look like the following:</p>

<pre
lang="yaml"
title="wka-service.yaml"
>apiVersion: v1
kind: Service
metadata:
  name: storage-wka                                                  <span class="conum" data-value="1" />
  labels:
    coherenceCluster: test-cluster
    component: coherenceWkaService
spec:
  clusterIP: None                                                    <span class="conum" data-value="2" />
  publishNotReadyAddresses: true                                     <span class="conum" data-value="3" />
  ports:
    - name: coherence                                                <span class="conum" data-value="4" />
      protocol: TCP
      port: 7
      targetPort: 7
  selector:
    coherenceCluster: test-cluster                                   <span class="conum" data-value="5" />
    component: coherencePod</pre>

<ul class="colist">
<li data-value="1">The <code>Service</code> name is made up of the cluster name with the suffix <code>-wka</code> so in this case <code>storage-wka</code></li>
<li data-value="2">The service has a <code>clusterIP</code> of <code>None</code> so it is headless</li>
<li data-value="3">The <code>Service</code> is configured to allow unready <code>Pods</code> so that all <code>Pods</code> matching the selector will be resolved as
members of this service regardless of their ready state. This is important so that Coherence JVMs can discover other
members before they are fully ready.</li>
<li data-value="4">A single port is exposed, in this case the echo port (7), even though nothing in the Coherence <code>Pods</code> binds to this
port. Ideally no port would be included, but a Kubernetes service has to have at least one port defined.</li>
<li data-value="5">The selector will match all <code>Pods</code> with the labels <code>coherenceCluster=test-cluster</code> and <code>component=coherencePod</code>
which are labels that the Coherence Operator will assign to all <code>Pods</code> in this cluster</li>
</ul>
<p>Because this <code>Service</code> is created in the same <code>Namespace</code> as the deployment&#8217;s <code>Pods</code> the JVMs can use
the raw <code>Service</code> name as the WKA list, in the example above the WKA list would just be <code>test-cluster-wka</code>.</p>

</div>

<h2 id="_exclude_a_deployment_from_wka">Exclude a Deployment From WKA</h2>
<div class="section">
<p>In some situations it may be desirable to exclude the Pods belonging to certain deployments in the cluster from being
members of the well known address list. For example certain K8s network configurations such as host networking can
cause issues with WKA if other deployments in the cluster are using host networking.</p>

<p>A role can be excluded from the WKA list by setting the <code>excludeFromWKA</code> field of the <code>coherence</code> section of the
deployment&#8217;s spec to <code>true</code>.</p>

<pre
lang="yaml"
title="test-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-client
spec:
  cluster: `my-cluster`    <span class="conum" data-value="1" />
  coherence:
    excludeFromWKA: true   <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>cluster</code> field is set to the name of the Coherence cluster that this deployment wil be part of (there is no
point in excluding a deployment from WKA unless it is part of a wider cluster).</li>
<li data-value="2">The <code>excludeFromWKA</code> field is <code>true</code> so that <code>Pods</code> in the <code>test-client</code> deployment will not form part of the WKA
list for the Coherence cluster.</li>
</ul>
<div class="admonition warning">
<p class="admonition-inline">The operator does not validate the <code>excludeFromWKA</code> field for a deployment so it is possible to try to create
a cluster where all of the deployment have <code>excludeFromWKA</code> set to <code>true</code> which will cause the cluster fail to start.</p>
</div>
<div class="admonition warning">
<p class="admonition-inline">When excluding a deployment from WKA it is important that at least one deployment that is part of the WKA list
has been started first otherwise the non-WKA role members cannot start.Eventually the K8s readiness probe for these Pods
would time-out causing K8s to restart them but this would not be a desirable way to start a cluster.
The start-up order can be controlled by configuring the deployment&#8217;s <code>startQuorum</code> list, as described in the documentation
section on <router-link to="/ordering/010_overview">deployment start-up ordering</router-link>.</p>
</div>
</div>

<h2 id="_multi_namespace_clusters">Multi-Namespace Clusters</h2>
<div class="section">
<p>It is possible to configure a Coherence cluster made up of multiple <code>Coherence</code> deployments that are deployed into
different namespaces in the same Kubernetes cluster (with some caveats).</p>

<p>The <code>coherence.wka</code> section of the Coherence CRD spec can be used to override the default WKA behaviour.</p>

<p>For example, suppose that there is a <code>Coherence</code> deployment named <code>data</code> that is the storage enabled cluster members
holding data for an online store. This <code>data</code> deployment will be deployed into the <code>back-end</code> namespace in a Kubernetes
cluster.<br>
Another <code>Coherence</code> deployment of storage disabled members will provide the front end REST API for the online store.
This will be named <code>web-store</code> and deployed in the <code>front-end</code> namespace.<br>
Although both the <code>data</code> and <code>web-store</code> deployments are in different namespaces they need to form a single Coherence
cluster.</p>

<pre
lang="yaml"
title="data-deployment.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: data
  namespace: back-end      <span class="conum" data-value="1" />
spec:
  cluster: `shop`          <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>data</code> deployment is deployed into the <code>back-end</code> namespace</li>
<li data-value="2">The Coherence cluster name is set to <code>shop</code></li>
</ul>
<pre
lang="yaml"
title="web-store-deployment.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: web-store
  namespace: front-end     <span class="conum" data-value="1" />
spec:
  cluster: `shop`          <span class="conum" data-value="2" />
  coherence:
    wka:                   <span class="conum" data-value="3" />
      deployment: data
      namespace: back-end</pre>

<ul class="colist">
<li data-value="1">The <code>web-store</code> deployment is deployed into the <code>front-end</code> namespace.</li>
<li data-value="2">The Coherence cluster name is set to <code>shop</code> to match the <code>data</code> deployment</li>
<li data-value="3">The <code>coherence.wka</code> section specifies the name of the <code>Coherence</code> deployment to use for WKA so in this
case the <code>data</code> deployment in the <code>back-end</code> namespace.</li>
</ul>
<p>As described already above the <code>data</code> deployment will have a headless <code>Service</code> created for <code>WKA</code> named <code>data-wka</code>,
which will be in the <code>back-end</code> namespace.
The full name of this <code>Service</code> in Kubernetes will be <code>data-wka.back-end.svc.cluster.local</code> and this will be the
name that the members of the <code>web-store</code> deployment will be configured to use for WKA.</p>

<div class="admonition warning">
<p class="admonition-inline">When using WKA in this way the <code>Coherence</code> deployment that is providing the WKA <code>Service</code> should be running before
any deployment that depends on it is deployed.</p>
</div>
</div>
</doc-view>
<!-- pages/coherence/080_persistence.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Coherence Persistence</dt>
<dd slot="desc"><p>Coherence persistence is a set of tools and technologies that manage the persistence and recovery of Coherence
distributed caches. Cached data can be persisted so that it can be quickly recovered after a catastrophic failure
or after a cluster restart due to planned maintenance. Persistence and federated caching can be used together
as required.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_configure_coherence_persistence">Configure Coherence Persistence</h2>
<div class="section">
<p>The <code>Coherence</code> CRD allows the default persistence mode, and the storage location of persistence data to be
configured. Persistence can be configured in the <code>spec.coherence.persistence</code> section of the CRD.
See the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/administer/persisting-caches.html#GUID-3DC46E44-21E4-4DC4-9D12-231DE57FE7A1">Coherence Persistence</a>
documentation for more details of how persistence works and its configuration.</p>

</div>

<h2 id="_persistence_mode">Persistence Mode</h2>
<div class="section">
<p>There are three default persistence modes available, <code>active</code>, <code>active-async</code> and <code>on-demand</code>; the default mode is <code>on-demand</code>.
The persistence mode will be set using the <code>spec.coherence.persistence,mode</code> field in the CRD. The value of this field will be
used to set the <code>coherence.distributed.persistence-mode</code> system property in the Coherence JVM.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    persistence:
      mode: active  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The example above sets the persistence mode to <code>active</code> which will effectively pass
<code>-Dcoherence.distributed.persistence-mode=active</code> to the Coherence JVM&#8217;s command line.</li>
</ul>
</div>

<h2 id="_persistence_storage">Persistence Storage</h2>
<div class="section">
<p>The purpose of persistence in Coherence is to store data on disc so that it is available outside of the lifetime of the
JVMs that make up the cluster. In a containerised environment like Kubernetes this means storing that data in storage that
also lives outside of the containers.</p>

<p>When persistence storage has been configured a <code>VolumeMount</code> will be added to the Coherence container mounted at <code>/persistence</code>,
and the <code>coherence.distributed.persistence.base.dir</code> system property will be configured to point to the storage location.</p>


<h3 id="_using_a_persistentvolumeclaim">Using a PersistentVolumeClaim</h3>
<div class="section">
<p>The Coherence Operator creates a <code>StatefulSet</code> for each <code>Coherence</code> resource, so the
logical place to store persistence data is in a <code>PersistentVolumeClaim</code>.</p>

<p>The PVC used for persistence can be configured in the <code>spec.coherence.persistence.persistentVolumeClaim</code> section
of the CRD.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    persistence:
      persistentVolumeClaim:     <span class="conum" data-value="1" />
        storageClassName: "SSD"
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi</pre>

<ul class="colist">
<li data-value="1">The example above configures a 50GB PVC with a storage class name of "SSD"
(assuming the Kubernetes cluster has a storage class of that name configured).</li>
</ul>
<p>The configuration under the <code>spec.coherence.persistence.persistentVolumeClaim</code> section is exactly the same as
configuring a PVC for a normal Kubernetes Pod and all the possible options are beyond the scope of this document.
For more details on configuring PVC, see the Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a> documentation.</p>

</div>

<h3 id="_using_a_normal_volume">Using a Normal Volume</h3>
<div class="section">
<p>An alternative to a PVC is to use a normal Kubernetes Volume to store the persistence data.
An example of this use-case could be when the Kubernetes Nodes that the Coherence Pods are scheduled onto have locally
attached fast SSD drives, which is ideal storage for persistence.
In this case a normal Volume can be configured in the <code>spec.coherence.persistence.volume</code> section of the CRD.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    persistence:                                 <span class="conum" data-value="1" />
      volume:
        hostPath:
          path: /mnt/ssd/coherence/persistence</pre>

<ul class="colist">
<li data-value="1">In the example above a Volume has been configured for persistence, in this case a <code>HostPath</code> volume pointing to
the <code>/mnt/ssd/coherence/persistence</code> directory on the Node.</li>
</ul>
<p>The configuration under the <code>spec.coherence.persistence.volume</code> section is a normal Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#volume-v1-core">VolumeSource</a>
so any valid <code>VolumeSource</code> configuration can be used.
See the Kubernetes <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a> documentation for more details.</p>

</div>
</div>

<h2 id="_snapshot_storage">Snapshot Storage</h2>
<div class="section">
<p>Coherence allows on-demand snapshots to be taken of cache data. With the default configuration the snapshot files will
be stored under the same persistence root location as active persistence data.
The <code>Coherence</code> spec allows a different location to be specified for storage of snapshot files so that active data
and snapshot data can be stored in different locations and/or on different storage types in Kubernetes.</p>

<p>The same two options are available for snapshot storage that are available for persistence storage, namely PVCs and
normal Volumes. The <code>spec.coherence.persistence.snapshots</code> section is used to configure snapshot storage.
When this is used a <code>VolumeMount</code> will be added to the Coherence container with a mount path of <code>/snapshots</code>,
and the <code>coherence.distributed.persistence.snapshot.dir</code> system property will be set to point to this location.</p>


<h3 id="_snapshots_using_a_persistentvolumeclaim">Snapshots Using a PersistentVolumeClaim</h3>
<div class="section">
<p>A PVC can be configured for persistence snapshot data as shown below.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    persistence:                                 <span class="conum" data-value="1" />
      volume:
        hostPath:
          path: /mnt/ssd/coherence/persistence
      snapshots:
        persistentVolumeClaim:                   <span class="conum" data-value="2" />
          resources:
            requests:
              storage: 50Gi</pre>

<ul class="colist">
<li data-value="1">Active persistence data will be stored on a normal Volume using a HostPath volume source.</li>
<li data-value="2">Snapshot data will be stored in a 50GB PVC.</li>
</ul>
</div>

<h3 id="_snapshots_using_a_normal_volumes">Snapshots Using a Normal Volumes</h3>
<div class="section">
<p>A normal volume can be configured for snapshot data as shown below.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    persistence:                                 <span class="conum" data-value="1" />
      volume:
        hostPath:
          path: /mnt/ssd/coherence/persistence
      snapshots:
        volume:
          hostPath:
            path: /mnt/ssd/coherence/snapshots   <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">Active persistence data will be stored on a normal Volume using a HostPath volume source.</li>
<li data-value="2">Snapshot data will be stored on a normal Volume using a different HostPath volume source.</li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/coherence/090_ipmonitor.js -->
<doc-view>

<h2 id="_coherence_ipmonitor">Coherence IPMonitor</h2>
<div class="section">
<p>The Coherence IPMonitor is a failure detection mechanism used by Coherence to detect machine failures. It does this by pinging the echo port, (port 7) on remote hosts that other cluster members are running on. When running in Kubernetes, every Pod has its own IP address, so it looks to Coherence like every member is on a different host. Failure detection using IPMonitor is less useful in Kubernetes than it is on physical machines or VMs, so the Operator disables the IPMonitor by default. This is configurable though and if it is felt that using IPMonitor is useful to an application, it can be re-enabled.</p>

<p>To re-enable IPMonitor set the boolean flag <code>enableIpMonitor</code> in the <code>coherence</code> section of the Coherence resource yaml:</p>

<pre
lang="yaml"
title="coherence-storage.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    enableIpMonitor: true</pre>

<p>Setting <code>enableIpMonitor</code> will disable the IPMonitor, which is the default behaviour when <code>enableIpMonitor</code> is not specified in the yaml.</p>

</div>
</doc-view>
<!-- pages/examples/010_overview.js -->
<doc-view>

<h2 id="_examples_overview">Examples Overview</h2>
<div class="section">
<p>There are a number of examples which show you how to build and deploy applications for the Coherence Operator.</p>

<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/examples/020_deployment"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Deployment</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>This example showcases how to deploy Coherence applications using the Coherence Operator.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/examples/030_federation"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Federation</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>This is a simple Coherence federation example. The federation feature requires Coherence Grid Edition.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/examples/100_tls"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">TLS</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Securing Coherence clusters using TLS.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/examples/500_autoscaler"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Autoscaling</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Scaling Coherence clusters using the horizontal Pod autoscaler.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/examples/800_istio"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Istio</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Istio Support</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/examples/900_demo"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Coherene Demo App</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Deploying the Coherence demo application.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</doc-view>
<!-- pages/examples/020_deployment.js -->
<doc-view>

<h2 id="_coherence_operator_deployment_example">Coherence Operator Deployment Example</h2>
<div class="section">
<p>This example showcases how to deploy Coherence applications using the Coherence Operator.</p>

<p>This example shows how to use the Kubernetes Horizontal Pod Autoscaler to scale Coherence clusters.
You can find the source code in the <a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/tree/master/examples/deployment">Operator GitHub Repo</a></p>

<p>The following scenarios are covered:</p>

<ol style="margin-left: 15px;">
<li>
Installing the Coherence Operator

</li>
<li>
Installing a Coherence cluster

</li>
<li>
Deploying a Proxy tier

</li>
<li>
Deploying a storage-disabled application

</li>
<li>
Enabling Active Persistence

</li>
<li>
Viewing Metrics via Grafana

</li>
</ol>
<p>After the initial installation of the Coherence cluster, the following examples
build on the previous ones by issuing a <code>kubectl apply</code> to modify
the installation adding additional roles.</p>

<p>You can use <code>kubectl create</code> for any of the examples to install that one directly.</p>

<ul class="ulist">
<li>
<p><router-link to="#pre" @click.native="this.scrollFix('#pre')">Prerequisites</router-link></p>
<ul class="ulist">
<li>
<p><router-link to="#create-the-example-namespace" @click.native="this.scrollFix('#create-the-example-namespace')">Create the example namespace</router-link></p>

</li>
<li>
<p><router-link to="#clone-the-github-repository" @click.native="this.scrollFix('#clone-the-github-repository')">Clone the GitHub repository</router-link></p>

</li>
<li>
<p><router-link to="#install-operator" @click.native="this.scrollFix('#install-operator')">Install the Coherence Operator</router-link></p>

</li>
</ul>
</li>
<li>
<p><router-link to="#examples" @click.native="this.scrollFix('#examples')">Run the Examples</router-link></p>
<ul class="ulist">
<li>
<p><router-link to="#ex1" @click.native="this.scrollFix('#ex1')">Example 1 - Coherence cluster only</router-link></p>

</li>
<li>
<p><router-link to="#ex2" @click.native="this.scrollFix('#ex2')">Example 2 - Adding a Proxy role</router-link></p>

</li>
<li>
<p><router-link to="#ex3" @click.native="this.scrollFix('#ex3')">Example 3 - Adding a User application role</router-link></p>

</li>
<li>
<p><router-link to="#ex4" @click.native="this.scrollFix('#ex4')">Example 4 - Enabling Persistence</router-link></p>

</li>
<li>
<p><router-link to="#metrics" @click.native="this.scrollFix('#metrics')">View Cluster Metrics via Grafana</router-link></p>

</li>
<li>
<p><router-link to="#grafana" @click.native="this.scrollFix('#grafana')">Port Forward and Access Grafana</router-link></p>

</li>
</ul>
</li>
<li>
<p><router-link to="#cleaning-up" @click.native="this.scrollFix('#cleaning-up')">Cleaning Up</router-link></p>

</li>
</ul>
</div>

<h2 id="pre">Prerequisites</h2>
<div class="section">
<p>Ensure you have the following software installed:</p>

<ul class="ulist">
<li>
<p>Java 11+ JDK either [OpenJDK](<a id="" title="" target="_blank" href="https://adoptopenjdk.net/">https://adoptopenjdk.net/</a>) or [Oracle JDK](<a id="" title="" target="_blank" href="https://www.oracle.com/java/technologies/javase-downloads.html">https://www.oracle.com/java/technologies/javase-downloads.html</a>)</p>

</li>
<li>
<p><a id="" title="" target="_blank" href="https://docs.docker.com/install/">Docker</a> version 17.03+.</p>

</li>
<li>
<p>Access to a Kubernetes v1.14.0+ cluster.</p>

</li>
<li>
<p><a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> version matching your Kubernetes cluster.</p>

</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">This example requires Java 11+ because it creates a Helidon web application and Helidon requires Java 11+. Coherence and running Coherence in Kubernetes only requires Java 8+.</p>
</div>
</div>

<h2 id="create-the-example-namespace">Create the example namespace</h2>
<div class="section">
<p>You need to create the namespace for the first time to run any of the examples. Create your target namespace:</p>

<pre
lang="bash"

>kubectl create namespace coherence-example

namespace/coherence-example created</pre>

<div class="admonition note">
<p class="admonition-textlabel">Note</p>
<p ><p>In the examples, a Kubernetes namespace called <code>coherence-example</code> is used.
If you want to change this namespace, ensure that you change any references to this namespace
to match your selected namespace when running the examples.</p>
</p>
</div>
</div>

<h2 id="clone-the-github-repository">Clone the GitHub repository</h2>
<div class="section">
<p>This examples exist in the <code>examples/deployment</code> directory in the
<a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator">Coherence Operator GitHub repository</a>.</p>

<p>Clone the repository:</p>

<pre
lang="bash"

>git clone https://github.com/oracle/coherence-operator

cd coherence-operator/examples/deployment</pre>

<p>Ensure you have Docker running and JDK 11+ build environment set and use the
following command from the deployment example directory to build the project and associated Docker image:</p>

<pre
lang="bash"

>./mvnw package jib:dockerBuild</pre>

<div class="admonition note">
<p class="admonition-textlabel">Note</p>
<p ><p>If you are running behind a corporate proxy and receive the following message building the Docker image:
<code>Connect to gcr.io:443 [gcr.io/172.217.212.82] failed: connect timed out</code> you must modify the build command
to add the proxy hosts and ports to be used by the <code>jib-maven-plugin</code> as shown below:</p>

<pre
lang="bash"

>mvn package jib:dockerBuild -Dhttps.proxyHost=host \
    -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80</pre>
</p>
</div>
<p>This will result in the following Docker image being created which contains the configuration and server-side
artifacts to be use by all deployments.</p>

<pre


>deployment-example:1.0.0</pre>

<div class="admonition note">
<p class="admonition-textlabel">Note</p>
<p ><p>If you are running against a remote Kubernetes cluster, you need to tag and
push the Docker image to your repository accessible to that cluster.
You also need to prefix the image name in the <code>yaml</code> files below.</p>
</p>
</div>
</div>

<h2 id="install-operator">Install the Coherence Operator</h2>
<div class="section">
<p>Install the Coherence Operator using your preferred method in the Operator
<a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/docs/latest/#/installation/01_installation">Installation Guide</a></p>

<p>Confirm the operator is running, for example:</p>

<pre
lang="bash"

>kubectl get pods -n coherence-example

NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-controller-manager-74d49cd9f9-sgzjr   1/1     Running   1          27s</pre>

</div>

<h2 id="examples">Run the Examples</h2>
<div class="section">
<p>Ensure you are in the <code>examples/deployment</code> directory to run the following commands.</p>


<h3 id="ex1">Example 1 - Coherence cluster only</h3>
<div class="section">
<p>The first example uses the yaml file <code>src/main/yaml/example-cluster.yaml</code>, which
defines a single role <code>storage</code> which will store cluster data.</p>

<div class="admonition note">
<p class="admonition-inline">If you have pushed your Docker image to a remote repository, ensure you update the above file to prefix the image.</p>
</div>

<h4 id="_1_install_the_coherence_cluster_storage_role">1. Install the Coherence cluster <code>storage</code> role</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example create -f src/main/yaml/example-cluster.yaml

coherence.coherence.oracle.com/example-cluster-storage created</pre>

</div>

<h4 id="_2_list_the_created_coherence_cluster">2. List the created Coherence cluster</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example get coherence

NAME                      CLUSTER           ROLE                      REPLICAS   READY   PHASE
example-cluster-storage   example-cluster   example-cluster-storage   3                  Created

NAME                                                         AGE
coherencerole.coherence.oracle.com/example-cluster-storage   18s</pre>

</div>

<h4 id="_3_view_the_running_pods">3. View the running pods</h4>
<div class="section">
<p>Run the following command to view the Pods:</p>

<pre
lang="bash"

>kubectl -n coherence-example get pods</pre>

<pre
lang="bash"

>NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-controller-manager-74d49cd9f9-sgzjr   1/1     Running   1          6m46s
example-cluster-storage-0                                0/1     Running   0          119s
example-cluster-storage-1                                1/1     Running   0          119s
example-cluster-storage-2                                0/1     Running   0          118s</pre>

</div>

<h4 id="_connect_to_the_coherence_console_inside_the_cluster_to_add_data">Connect to the Coherence Console inside the cluster to add data</h4>
<div class="section">
<p>Since we cannot yet access the cluster via Coherence*Extend, we will connect via Coherence console to add data.</p>

<pre
lang="bash"

>kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console</pre>

<p>At the prompt type the following to create a cache called <code>test</code>:</p>

<pre
lang="bash"

>cache test</pre>

<p>Use the following to create 10,000 entries of 100 bytes:</p>

<pre
lang="bash"

>bulkput 10000 100 0 100</pre>

<p>Lastly issue the command <code>size</code> to verify the cache entry count.</p>

<p>Type <code>bye</code> to exit the console.</p>

</div>

<h4 id="_scale_the_storage_role_to_6_members">Scale the <code>storage</code> role to 6 members</h4>
<div class="section">
<p>To scale up the cluster the <code>kubectl scale</code> command can be used:</p>

<pre
lang="bash"

>kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=6</pre>

<p>Use the following to verify all 6 nodes are Running and READY before continuing.</p>

<pre
lang="bash"

>kubectl -n coherence-example get pods</pre>

<pre
lang="bash"

>NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-controller-manager-74d49cd9f9-sgzjr   1/1     Running   1          53m
example-cluster-storage-0                                1/1     Running   0          49m
example-cluster-storage-1                                1/1     Running   0          49m
example-cluster-storage-2                                1/1     Running   0          49m
example-cluster-storage-3                                1/1     Running   0          54s
example-cluster-storage-4                                1/1     Running   0          54s
example-cluster-storage-5                                1/1     Running   0          54s</pre>

</div>

<h4 id="_confirm_the_cache_count">Confirm the cache count</h4>
<div class="section">
<p>Re-run step 3 above and just use the <code>cache test</code> and <code>size</code> commands to confirm the number of entries is still 10,000.</p>

<p>This confirms that the scale-out was done in a <code>safe</code> manner ensuring no data loss.</p>

</div>
</div>

<h3 id="_scale_the_storage_role_back_to_3_members">Scale the <code>storage</code> role back to 3 members</h3>
<div class="section">
<p>To scale back doewn to three members run the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-example scale coherence/example-cluster-storage --replicas=3</pre>

<p>By using the following, you will see that the number of members will gradually scale back to
3 during which the is done in a <code>safe</code> manner ensuring no data loss.</p>

<pre
lang="bash"

>kubectl -n coherence-example get pods</pre>

<pre
lang="bash"

>NAME                        READY   STATUS        RESTARTS   AGE
example-cluster-storage-0   1/1     Running       0          19m
example-cluster-storage-1   1/1     Running       0          19m
example-cluster-storage-2   1/1     Running       0          19m
example-cluster-storage-3   1/1     Running       0          3m41s
example-cluster-storage-4   0/1     Terminating   0          3m41s</pre>

</div>

<h3 id="ex2">Example 2 - Adding a Proxy role</h3>
<div class="section">
<p>The second example uses the yaml file <code>src/main/yaml/example-cluster-proxy.yaml</code>, which
adds a proxy server <code>example-cluster-proxy</code> to allow for Coherence*Extend connections via a Proxy server.</p>

<p>The additional yaml added below shows:</p>

<ul class="ulist">
<li>
<p>A port called <code>proxy</code> being exposed on 20000</p>

</li>
<li>
<p>The role being set as storage-disabled</p>

</li>
<li>
<p>A different cache config being used which will start a Proxy Server. See [here](src/main/resources/proxy-cache-config.xml) for details</p>

</li>
</ul>
<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: example-cluster-proxy
spec:
  cluster: example-cluster
  jvm:
    memory:
      heapSize: 512m
  ports:
    - name: metrics
      port: 9612
      serviceMonitor:
        enabled: true
    - name: proxy
      port: 20000
  coherence:
    cacheConfig: proxy-cache-config.xml
    storageEnabled: false
    metrics:
      enabled: true
  image: deployment-example:1.0.0
  imagePullPolicy: Always
  replicas: 1</pre>


<h4 id="_install_the_proxy_role">Install the <code>proxy</code> role</h4>
<div class="section">
<pre
lang="bash"

>  kubectl -n coherence-example apply -f src/main/yaml/example-cluster-proxy.yaml

  kubectl get coherence -n coherence-example

  NAME                      CLUSTER           ROLE                      REPLICAS   READY   PHASE
  example-cluster-proxy     example-cluster   example-cluster-proxy     1          1       Ready
  example-cluster-storage   example-cluster   example-cluster-storage   3          3       Ready</pre>

</div>

<h4 id="_view_the_running_pods">View the running pods</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example get pods

NAME                                  READY   STATUS    RESTARTS   AGE
coherence-operator-578497bb5b-w89kt   1/1     Running   0          68m
example-cluster-proxy-0               1/1     Running   0          2m41s
example-cluster-storage-0             1/1     Running   0          29m
example-cluster-storage-1             1/1     Running   0          29m
example-cluster-storage-2             1/1     Running   0          2m43s</pre>

<p>Ensure the <code>example-cluster-proxy-0</code> pod is Running and READY before continuing.</p>

</div>

<h4 id="_port_forward_the_proxy_port">Port forward the proxy port</h4>
<div class="section">
<pre>In a separate terminal, run the following:</pre>
<pre
lang="bash"

>    kubectl port-forward -n coherence-example example-cluster-proxy-0 20000:20000</pre>

</div>

<h4 id="_connect_via_cohql_and_add_data">Connect via CohQL and add data</h4>
<div class="section">
<p>In a separate terminal, change to the <code>examples/deployments</code> directory and run the following to
start Coherence Query Language (CohQL):</p>

<pre
lang="bash"

>    mvn exec:java

    Coherence Command Line Tool

    CohQL&gt;</pre>

<p>Run the following <code>CohQL</code> commands to view and insert data into the cluster.</p>

<pre


>CohQL&gt; select count() from 'test';

Results
10000

CohQL&gt; insert into 'test' key('key-1') value('value-1');

CohQL&gt; select key(), value() from 'test' where key() = 'key-1';
Results
["key-1", "value-1"]

CohQL&gt; select count() from 'test';
Results
10001

CohQL&gt; quit</pre>

<p>The above results will show that you can see the data previously inserted and
can add new data into the cluster using Coherence*Extend.</p>

</div>
</div>

<h3 id="ex3">Example 3 - Adding a User application role</h3>
<div class="section">
<p>The third example uses the yaml file <code>src/main/yaml/example-cluster-app.yaml</code>, which
adds a new role <code>rest</code>. This role defines a user application which uses <a id="" title="" target="_blank" href="https://helidon.io/">Helidon</a> to create a <code>/query</code> endpoint allowing the user to send CohQL commands via this endpoint.</p>

<p>The additional yaml added below shows:</p>

<ul class="ulist">
<li>
<p>A port called <code>http</code> being exposed on 8080 for the application</p>

</li>
<li>
<p>The role being set as storage-disabled</p>

</li>
<li>
<p>Using the storage-cache-config.xml but as storage-disabled</p>

</li>
<li>
<p>An alternate main class to run - <code>com.oracle.coherence.examples.Main</code></p>

</li>
</ul>
<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: example-cluster-rest
spec:
  cluster: example-cluster
  jvm:
    memory:
      heapSize: 512m
  ports:
    - name: metrics
      port: 9612
      serviceMonitor:
        enabled: true
    - name: http
      port: 8080
  coherence:
    cacheConfig: storage-cache-config.xml
    storageEnabled: false
    metrics:
      enabled: true
  image: deployment-example:1.0.0
  imagePullPolicy: Always
  application:
    main: com.oracle.coherence.examples.Main</pre>


<h4 id="_install_the_rest_role">Install the <code>rest</code> role</h4>
<div class="section">
<p>Install the yaml with the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-example apply -f src/main/yaml/example-cluster-app.yaml

kubectl get coherence -n coherence-example

NAME                      CLUSTER           ROLE                      REPLICAS   READY   PHASE
example-cluster-proxy     example-cluster   example-cluster-proxy     1          1       Ready
example-cluster-rest      example-cluster   example-cluster-rest      1          1       Ready
example-cluster-storage   example-cluster   example-cluster-storage   3          3       Ready</pre>

</div>

<h4 id="_view_the_running_pods_2">View the running pods</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example get pods

NAME                              READY   STATUS    RESTARTS   AGE
coherence-operator-578497bb5b-w89kt   1/1     Running   0          90m
example-cluster-proxy-0               1/1     Running   0          3m57s
example-cluster-rest-0                1/1     Running   0          3m57s
example-cluster-storage-0             1/1     Running   0          3m59s
example-cluster-storage-1             1/1     Running   0          3m58s
example-cluster-storage-2             1/1     Running   0          3m58s</pre>

</div>

<h4 id="_port_forward_the_application_port">Port forward the application port</h4>
<div class="section">
<p>In a separate terminal, run the following:</p>

<pre
lang="bash"

>kubectl port-forward -n coherence-example example-cluster-rest-0 8080:8080</pre>

</div>

<h4 id="_access_the_custom_query_endpoint">Access the custom <code>/query</code> endpoint</h4>
<div class="section">
<p>Use the various <code>CohQL</code> commands via the <code>/query</code> endpoint to access, and mutate data in the Coherence cluster.</p>

<pre
lang="bash"

>curl -i -w '\n' -X PUT http://127.0.0.1:8080/query -d '{"query":"create cache foo"}'</pre>

<pre
lang="bash"

>HTTP/1.1 200 OK
Date: Fri, 19 Jun 2020 06:29:40 GMT
transfer-encoding: chunked
connection: keep-alive</pre>

<pre
lang="bash"

>curl -i -w '\n' -X PUT http://127.0.0.1:8080/query -d '{"query":"insert into foo key(\"foo\") value(\"bar\")"}'</pre>

<pre
lang="bash"

>HTTP/1.1 200 OK
Date: Fri, 19 Jun 2020 06:29:44 GMT
transfer-encoding: chunked
connection: keep-alive</pre>

<pre
lang="bash"

>curl -i -w '\n' -X PUT http://127.0.0.1:8080/query -d '{"query":"select key(),value() from foo"}'</pre>

<pre
lang="bash"

>HTTP/1.1 200 OK
Content-Type: application/json
Date: Fri, 19 Jun 2020 06:29:55 GMT
transfer-encoding: chunked
connection: keep-alive

{"result":"{foo=[foo, bar]}"}</pre>

<pre
lang="bash"

>curl -i -w '\n' -X PUT http://127.0.0.1:8080/query -d '{"query":"create cache test"}'</pre>

<pre
lang="bash"

>HTTP/1.1 200 OK
Date: Fri, 19 Jun 2020 06:30:00 GMT
transfer-encoding: chunked
connection: keep-alive</pre>

<pre
lang="bash"

>curl -i -w '\n' -X PUT http://127.0.0.1:8080/query -d '{"query":"select count() from test"}'</pre>

<pre
lang="bash"

>HTTP/1.1 200 OK
Content-Type: application/json
Date: Fri, 19 Jun 2020 06:30:20 GMT
transfer-encoding: chunked
connection: keep-alive

{"result":"10001"}</pre>

</div>
</div>

<h3 id="ex4">Example 4 - Enabling Persistence</h3>
<div class="section">
<p>The fourth example uses the yaml file <code>src/main/yaml/example-cluster-persistence.yaml</code>, which
enabled Active Persistence for the <code>storage</code> role by adding a <code>persistence:</code> element.</p>

<p>The additional yaml added to the storage role below shows:</p>

<ul class="ulist">
<li>
<p>Active Persistence being enabled via <code>persistence.enabled=true</code></p>

</li>
<li>
<p>Various Persistence Volume Claim (PVC) values being set under <code>persistentVolumeClaim</code></p>

</li>
</ul>
<pre
lang="yaml"

>  coherence:
    cacheConfig: storage-cache-config.xml
    metrics:
      enabled: true
    persistence:
      enabled: true
      persistentVolumeClaim:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi</pre>

<p>NOTE:By default, when you enable Coherence Persistence, the required infrastructure in terms of persistent volumes (PV) and persistent volume claims (PVC) is set up automatically. Also, the persistence-mode is set to <code>active</code>. This allows the Coherence cluster to be restarted, and the data to be retained.</p>


<h4 id="_delete_the_existing_deployment">Delete the existing deployment</h4>
<div class="section">
<p>We must first delete the existing deployment as we need to redeploy to enable Active Persistence.</p>

<pre
lang="bash"

>kubectl -n coherence-example delete -f src/main/yaml/example-cluster-app.yaml</pre>

<p>Ensure all the pods have terminated before you continue.</p>

</div>

<h4 id="_install_the_cluster_with_persistence_enabled">Install the cluster with Persistence enabled</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml</pre>

</div>

<h4 id="_view_the_running_pods_and_pvcs">View the running pods and PVC&#8217;s</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example get pods</pre>

<pre
lang="bash"

>NAME                            READY   STATUS    RESTARTS   AGE
example-cluster-rest-0          1/1     Running   0          5s
example-cluster-proxy-0         1/1     Running   0          5m1s
example-cluster-storage-0       1/1     Running   0          5m3s
example-cluster-storage-1       1/1     Running   0          5m3s
example-cluster-storage-2       1/1     Running   0          5m3s</pre>

<p>Check the Persistent Volumes and PVC are automatically created.</p>

<pre
lang="bash"

>kubectl get pvc -n coherence-example</pre>

<pre
lang="bash"

>NAME                                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistence-volume-example-cluster-storage-0   Bound    pvc-15b46996-eb35-11e9-9b4b-025000000001   1Gi        RWO            hostpath       55s
persistence-volume-example-cluster-storage-1   Bound    pvc-15bd99e9-eb35-11e9-9b4b-025000000001   1Gi        RWO            hostpath       55s
persistence-volume-example-cluster-storage-2   Bound    pvc-15e55b6b-eb35-11e9-9b4b-025000000001   1Gi        RWO            hostpath       55s</pre>

<p>Wait until all  nodes are Running and READY before continuing.</p>

</div>

<h4 id="_check_active_persistence_is_enabled">Check Active Persistence is enabled</h4>
<div class="section">
<p>Use the following to view the logs of the <code>example-cluster-storage-0</code> pod and validate that Active Persistence is enabled.</p>

<pre
lang="bash"

>kubectl logs example-cluster-storage-0 -c coherence -n coherence-example | grep 'Created persistent'</pre>

<pre
lang="bash"

>...
019-10-10 04:52:00.179/77.023 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/126-2-16db40199bc-4
2019-10-10 04:52:00.247/77.091 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=4): Created persistent store /persistence/active/example-cluster/PartitionedCache/127-2-16db40199bc-4
...</pre>

<p>If you see output similar to above then Active Persistence is enabled.</p>

</div>

<h4 id="_connect_to_the_coherence_console_to_add_data">Connect to the Coherence Console to add data</h4>
<div class="section">
<pre
lang="bash"

>kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console</pre>

<p>At the prompt type the following to create a cache called <code>test</code>:</p>

<pre
lang="bash"

>cache test</pre>

<p>Use the following to create 10,000 entries of 100 bytes:</p>

<pre
lang="bash"

>bulkput 10000 100 0 100</pre>

<p>Lastly issue the command <code>size</code> to verify the cache entry count.</p>

<p>Type <code>bye</code> to exit the console.</p>

</div>

<h4 id="_delete_the_cluster">Delete the cluster</h4>
<div class="section">
<div class="admonition note">
<p class="admonition-inline">This will not delete the PVC&#8217;s.</p>
</div>
<pre
lang="bash"

>kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml</pre>

<p>Use <code>kubectl get pods -n coherence-example</code> to confirm the pods have terminated.</p>

</div>

<h4 id="_confirm_the_pvcs_are_still_present">Confirm the PVC&#8217;s are still present</h4>
<div class="section">
<pre
lang="bash"

>kubectl get pvc -n coherence-example</pre>

<pre
lang="bash"

>NAME                                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistence-volume-example-cluster-storage-0   Bound    pvc-730f86fe-eb19-11e9-9b4b-025000000001   1Gi        RWO            hostpath       116s
persistence-volume-example-cluster-storage-1   Bound    pvc-73191751-eb19-11e9-9b4b-025000000001   1Gi        RWO            hostpath       116s
persistence-volume-example-cluster-storage-2   Bound    pvc-73230889-eb19-11e9-9b4b-025000000001   1Gi        RWO            hostpath       116s</pre>

</div>

<h4 id="_re_install_the_cluster">Re-install the cluster</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example create -f src/main/yaml/example-cluster-persistence.yaml</pre>

</div>

<h4 id="_follow_the_logs_for_persistence_messages">Follow the logs for Persistence messages</h4>
<div class="section">
<pre
lang="bash"

>kubectl logs example-cluster-storage-0 -c coherence -n coherence-example -f</pre>

<p>You should see a message regarding recovering partitions, similar to the following:</p>

<pre
lang="bash"

>2019-10-10 05:00:14.255/32.206 Oracle Coherence GE 12.2.1.4.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=1): Recovering 86 partitions
...
2019-10-10 05:00:17.417/35.368 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1): Created persistent store /persistence/active/example-cluster/PartitionedCache/50-3-16db409d035-1 from SafeBerkeleyDBStore(50-2-16db40199bc-4, /persistence/active/example-cluster/PartitionedCache/50-2-16db40199bc-4)
...</pre>

<p>Finally, you should see the following indicating active recovery has completed.</p>

<pre
lang="bash"

>2019-10-10 08:18:04.870/59.565 Oracle Coherence GE 12.2.1.4.0 &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=1):
   Recovered PartitionSet{172..256} from active persistent store</pre>

</div>

<h4 id="_confirm_the_data_has_been_recovered">Confirm the data has been recovered</h4>
<div class="section">
<pre
lang="bash"

>kubectl exec -it -n coherence-example example-cluster-storage-0 /coherence-operator/utils/runner console</pre>

<p>At the prompt type the following to create a cache called <code>test</code>:</p>

<pre
lang="bash"

>cache test</pre>

<p>Lastly issue the command <code>size</code> to verify the cache entry count is 10,000 meaning the data has been recovered.</p>

<p>Type <code>bye</code> to exit the console.</p>

</div>
</div>

<h3 id="metrics">View Cluster Metrics via Grafana</h3>
<div class="section">
<p>If you wish to view metrics via Grafana, you must carry out the following steps <strong>before</strong> you
install any of the examples above.</p>


<h4 id="_install_prometheus_operator">Install Prometheus Operator</h4>
<div class="section">
<p>Add the <code>stable</code> helm repository</p>

<pre
lang="bash"

>helm repo add stable https://charts.helm.sh/stable

helm repo update</pre>

</div>

<h4 id="_create_prometheus_pre_requisites">Create Prometheus pre-requisites</h4>
<div class="section">
<pre
lang="bash"

>    kubectl apply -f src/main/yaml/prometheus-rbac.yaml</pre>

</div>

<h4 id="_create_config_maps_for_datasource_and_dashboards">Create Config Maps for datasource and dashboards</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example create -f src/main/yaml/grafana-datasource-config.yaml

kubectl -n coherence-example label configmap demo-grafana-datasource grafana_datasource=1

kubectl -n coherence-example create -f https://oracle.github.io/coherence-operator/dashboards/latest/coherence-grafana-dashboards.yaml

kubectl -n coherence-example label configmap coherence-grafana-dashboards grafana_dashboard=1</pre>

</div>

<h4 id="_install_prometheus_operator_2">Install Prometheus Operator</h4>
<div class="section">
<div class="admonition note">
<p class="admonition-inline">If you have already installed Prometheus Operator before on this Kubernetes Cluster then set <code>--set prometheusOperator.createCustomResource=false</code>.</p>
</div>
<p>Issue the following command to install the Prometheus Operator using Helm:</p>

<pre
lang="bash"

>helm install --namespace coherence-example --version 8.13.9 \
    --set grafana.enabled=true \
    --set prometheusOperator.createCustomResource=true \
    --values src/main/yaml/prometheus-values.yaml prometheus stable/prometheus-operator</pre>

<div class="admonition tip">
<p class="admonition-textlabel">Tip</p>
<p ><p>Note: for Helm version 2, use the following:</p>

<pre
lang="bash"

>helm install --namespace coherence-example --version 8.13.9 \
    --set grafana.enabled=true --name prometheus \
    --set prometheusOperator.createCustomResource=true \
    --values src/main/yaml/prometheus-values.yaml stable/prometheus-operator</pre>
</p>
</div>
<p>Use the following to view the installed pods:</p>

<pre
lang="bash"

>kubectl get pods -n coherence-example</pre>

<pre
lang="bash"

>NAME                                                   READY   STATUS    RESTARTS   AGE
coherence-operator-578497bb5b-w89kt                    1/1     Running   0          136m
prometheus-grafana-6bb6d86f86-rgsm6                    2/2     Running   0          85s
prometheus-kube-state-metrics-5496457bd-vjqgd          1/1     Running   0          85s
prometheus-prometheus-node-exporter-29lrp              1/1     Running   0          85s
prometheus-prometheus-node-exporter-82b5w              1/1     Running   0          85s
prometheus-prometheus-node-exporter-mbj2k              1/1     Running   0          85s
prometheus-prometheus-oper-operator-6bc97bc4d7-67qjp   2/2     Running   0          85s
prometheus-prometheus-prometheus-oper-prometheus-0     3/3     Running   1          68s</pre>

<p>Remember to now install one of the examples above.  If you have already had the examples installed,
you must delete and re-install once you have installed Prometheus operator.</p>

</div>

<h4 id="grafana">Port Forward and Access Grafana</h4>
<div class="section">
<p>Port-forward the ports for these components using the scripts
in the <code>examples/deployment/scripts/</code> directory.</p>

<ul class="ulist">
<li>
<p>Port-forward Grafana for viewing metrics</p>

</li>
</ul>
<pre
lang="bash"

>./port-forward-grafana.sh coherence-example</pre>

<pre
lang="bash"

>Port-forwarding coherence-operator-grafana-8454698bcf-5dqvm in coherence-example
Open the following URL: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main
Forwarding from 127.0.0.1:3000 -&gt; 3000
Forwarding from [::1]:3000 -&gt; 3000</pre>

<p>The default username is <code>admin</code> and the password is <code>prom-operator</code>.</p>

<ul class="ulist">
<li>
<p>Port-forward Kibana for viewing log messages</p>

</li>
</ul>
<pre
lang="bash"

>./port-forward-kibana.sh coherence-example</pre>

<pre
lang="bash"

>Port-forwarding kibana-67c4f74ffb-nspwz in coherence-example
Open the following URL: http://127.0.0.1:5601/
Forwarding from 127.0.0.1:5601 -&gt; 5601
Forwarding from [::1]:5601 -&gt; 5601</pre>

<ul class="ulist">
<li>
<p>Port-forward Prometheus (for troubleshooting only)</p>

</li>
</ul>
<pre
lang="bash"

>./port-forward-prometheus.sh coherence-example</pre>

<pre
lang="bash"

>Port-forwarding prometheus-prometheus-prometheus-oper-prometheus-0 in coherence-example
Open the following URL: http://127.0.0.1:9090/targets
Forwarding from 127.0.0.1:9090 -&gt; 9090
Forwarding from [::1]:9090 -&gt; 9090</pre>

<ul class="ulist">
<li>
<p>Open the URLS described above.</p>

</li>
</ul>
</div>

<h4 id="_troubleshooting">Troubleshooting</h4>
<div class="section">
<ul class="ulist">
<li>
<p>It may take up to 5 minutes for data to start appearing in Grafana.</p>

</li>
<li>
<p>If you are not seeing data after 5 minutes, access the Prometheus endpoint as described above.
Ensure that the endpoints named <code>coherence-example/example-cluster-storage-metrics/0 (3/3 up)</code> are up.</p>
<pre>If the endpoints are not up then wait 60 seconds and refresh the browser.</pre>
</li>
<li>
<p>If you do not see any values in the <code>Cluster Name</code> dropdown in Grafana, ensure the endpoints are up as  described above and click on <code>Manage Alerts</code> and then <code>Back to Main Dashboard</code>. This will re-query the data and load the list of clusters.</p>

</li>
</ul>
</div>
</div>

<h3 id="cleaning-up">Cleaning Up</h3>
<div class="section">

<h4 id="_delete_the_cluster_2">Delete the cluster</h4>
<div class="section">
<pre
lang="bash"

>kubectl -n coherence-example delete -f src/main/yaml/example-cluster-persistence.yaml</pre>

</div>

<h4 id="_delete_the_pvcs">Delete the PVC&#8217;s</h4>
<div class="section">
<p>Ensure all the pods have all terminated before you delete the PVC&#8217;s.</p>

<pre
lang="bash"

>kubectl get pvc -n coherence-example | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n coherence-example</pre>

</div>

<h4 id="_remove_the_coherence_operator">Remove the Coherence Operator</h4>
<div class="section">
<p>Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it.</p>

</div>

<h4 id="_delete_prometheus_operator">Delete Prometheus Operator</h4>
<div class="section">
<pre
lang="bash"

> helm delete prometheus --namespace coherence-example

 kubectl -n coherence-example delete -f src/main/yaml/grafana-datasource-config.yaml

 kubectl -n coherence-example delete configmap coherence-grafana-dashboards

 kubectl delete -f src/main/yaml/prometheus-rbac.yaml</pre>

<div class="admonition tip">
<p class="admonition-textlabel">Tip</p>
<p ><p>For Helm version 2 use the following:</p>

<pre
lang="bash"

>helm delete prometheus --purge</pre>
</p>
</div>
<div class="admonition note">
<p class="admonition-inline">You can optionally delete the Prometheus Operator Custom Resource Definitions (CRD&#8217;s) if you are not going to install Prometheus Operator again.</p>
</div>
<pre
lang="bash"

>kubectl delete crd alertmanagers.monitoring.coreos.com
kubectl delete crd podmonitors.monitoring.coreos.com
kubectl delete crd prometheuses.monitoring.coreos.com
kubectl delete crd prometheusrules.monitoring.coreos.com
kubectl delete crd prometheusrules.monitoring.coreos.com
kubectl delete crd servicemonitors.monitoring.coreos.com
kubectl delete crd thanosrulers.monitoring.coreos.com</pre>

<p>A shorthand way of doing this if you are running Linux/Mac is:</p>

<pre
lang="bash"

>kubectl get crds -n coherence-example | grep monitoring.coreos.com | awk '{print $1}' | xargs kubectl delete crd</pre>

</div>
</div>
</div>
</doc-view>
<!-- pages/examples/030_federation.js -->
<doc-view>

<h1 id="_federation_example">Federation Example</h1>
<div class="section">

<h2 id="_federation_example_2">Federation Example</h2>
<div class="section">
<p>This simple example demonstrates the Coherence federation feature.  It shows how to deploy two Coherence clusters that federating data between them using the Coherence Operator. The Coherence federation feature requires Coherence Grid Edition. See <a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/docs/latest/#/installation/04_obtain_coherence_images">Obtain Coherence Images</a> on how to get a commercial Coherence image.</p>

<p>You can find the source code in the <a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/tree/master/examples/federation">Operator GitHub Repo</a>.</p>


<h3 id="_what_the_example_will_cover">What the Example will Cover</h3>
<div class="section">
<ul class="ulist">
<li>
<p><router-link to="#install-operator" @click.native="this.scrollFix('#install-operator')">Install the Coherence Operator</router-link></p>

</li>
<li>
<p><router-link to="#create-the-example-namespace" @click.native="this.scrollFix('#create-the-example-namespace')">Create the example namespace</router-link></p>

</li>
<li>
<p><router-link to="#create-secret" @click.native="this.scrollFix('#create-secret')">Create image pull and config store secrets</router-link></p>

</li>
<li>
<p><router-link to="#example" @click.native="this.scrollFix('#example')">Run the Example</router-link></p>

</li>
<li>
<p><router-link to="#cleanup" @click.native="this.scrollFix('#cleanup')">Cleaning Up</router-link></p>

</li>
</ul>
</div>

<h3 id="install-operator">Install the Coherence Operator</h3>
<div class="section">
<p>To run the examples below, you will need to have installed the Coherence Operator, do this using whatever method you prefer from the <a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/docs/latest/#/installation/01_installation">Installation Guide</a>.</p>

<p>Once you complete, confirm the operator is running, for example:</p>

<pre
lang="bash"

>kubectl get pods -n coherence

NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-controller-manager-74d49cd9f9-sgzjr   1/1     Running   1          27s</pre>

</div>
</div>

<h2 id="create-the-example-namespace">Create the example namespace</h2>
<div class="section">
<p>First, run the following command to create the namespace, coherence-example, for the example:</p>

<pre
lang="bash"

>kubectl create namespace coherence-example

namespace/coherence-example created</pre>


<h3 id="create-secret">Create image pull and configure store secrets</h3>
<div class="section">
<p>This example reqires two secrets:</p>

<ul class="ulist">
<li>
<p>An image pull secret named ocr-pull-secret containing your OCR credentials to be used by the example.</p>

</li>
</ul>
<p>Use a command similar to the following to create the image pull secret:</p>

<pre
lang="bash"

>kubectl create secret docker-registry ocr-pull-secret \
    --docker-server=container-registry.oracle.com \
    --docker-username="&lt;username&gt;" --docker-password="&lt;password&gt;" \
    --docker-email="&lt;email&gt;" -n coherence-example</pre>

<ul class="ulist">
<li>
<p>A configure store secret named storage-config to store the Coherence configuration files.</p>

</li>
</ul>
<p>Run the following command to create the configure store secret:</p>

<pre
lang="bash"

>kubectl create secret generic storage-config -n coherence-example \
    --from-file=src/main/resources/tangosol-coherence-override.xml \
    --from-file=src/main/resources/storage-cache-config.xml</pre>

</div>

<h3 id="example">Run the Example</h3>
<div class="section">
<p>Ensure you are in the <code>examples/federation</code> directory to run the example. This example uses the yaml files <code>src/main/yaml/primary-cluster.yaml</code> and <code>src/main/yaml/secondary-cluster.yaml</code>, which
define a primary cluster and a secondary cluster.</p>


<h4 id="_1_install_the_coherence_clusters">1. Install the Coherence clusters</h4>
<div class="section">
<p>Run the following commands to create the primary and secondary clusters:</p>

<pre
lang="bash"

>kubectl -n coherence-example create -f src/main/yaml/primary-cluster.yaml

coherence.coherence.oracle.com/primary-cluster created</pre>

<pre
lang="bash"

>kubectl -n coherence-example create -f src/main/yaml/secondary-cluster.yaml

coherence.coherence.oracle.com/secondary-cluster created</pre>

</div>

<h4 id="_2_list_the_created_coherence_clusters">2. List the created Coherence clusters</h4>
<div class="section">
<p>Run the following command to list the clusters:</p>

<pre
lang="bash"

>kubectl -n coherence-example get coherence

NAME                CLUSTER             ROLE                REPLICAS   READY   PHASE
primary-cluster     primary-cluster     primary-cluster     2          2       Ready
secondary-cluster   secondary-cluster   secondary-cluster   2          2       Ready</pre>

<p>To see the Coherence cache configuration file loaded from the secret volumn we defined, run the following command:</p>

<pre
lang="bash"

>kubectl logs -n coherence-example primary-cluster-0 | grep "Loaded cache"

... Oracle Coherence GE 14.1.1.0.0 &lt;Info&gt; (thread=main, member=n/a): Loaded cache configuration from "file:/config/storage-cache-config.xml"</pre>

</div>

<h4 id="_3_view_the_running_pods">3. View the running pods</h4>
<div class="section">
<p>Run the following command to view the Pods:</p>

<pre
lang="bash"

>kubectl -n coherence-example get pods</pre>

<pre
lang="bash"

>NAME                  READY   STATUS    RESTARTS   AGE
primary-cluster-0     1/1     Running   0          83s
primary-cluster-1     1/1     Running   0          83s
secondary-cluster-0   1/1     Running   0          74s
secondary-cluster-1   1/1     Running   0          73s</pre>

</div>

<h4 id="_4_connect_to_the_coherence_console_inside_the_primary_cluster_to_add_data">4. Connect to the Coherence Console inside the primary cluster to add data</h4>
<div class="section">
<p>We will connect via Coherence console to add some data using the following commands:</p>

<pre
lang="bash"

>kubectl exec -it -n coherence-example primary-cluster-0 /coherence-operator/utils/runner console</pre>

<p>At the prompt type the following to create a cache called <code>test</code>:</p>

<pre
lang="bash"

>cache test</pre>

<p>Use the following to add an entry with "primarykey" and "primaryvalue":</p>

<pre
lang="bash"

>put "primarykey" "primaryvalue"</pre>

<p>Use the following to create 10,000 entries of 100 bytes:</p>

<pre
lang="bash"

>bulkput 10000 100 0 100</pre>

<p>Lastly issue the command <code>size</code> to verify the cache entry count. It should be 10001.</p>

<p>Type <code>bye</code> to exit the console.</p>

</div>

<h4 id="_6_connect_to_the_coherence_console_inside_the_secondary_cluster_to_verify_that_data_is_federated_from_primary_cluster">6. Connect to the Coherence Console inside the secondary cluster to verify that data is federated from primary cluster</h4>
<div class="section">
<p>We will connect via Coherence console to confirm that the data we added to the primary cluster is federated to the secondary cluster.</p>

<pre
lang="bash"

>kubectl exec -it -n coherence-example secondary-cluster-0 /coherence-operator/utils/runner console</pre>

<p>At the prompt type the following to set the cache to <code>test</code>:</p>

<pre
lang="bash"

>cache test</pre>

<p>Use the following to get entry with "primarykey":</p>

<pre
lang="bash"

>get "primarykey"
primaryvalue</pre>

<p>Issue the command <code>size</code> to verify the cache entry count. It should be 10001.</p>

<p>Our federation has Active/Active topology. So, the data changes in both primary and secondary clusters are federated between the clusters. Use the following to add an entry with "secondarykey" and "secondaryvalue":</p>

<pre
lang="bash"

>put "secondarykey" "secondaryvalue"</pre>

</div>

<h4 id="_7_confirm_the_primary_cluster_also_received_secondarykey_secondaryvalue_entry">7. Confirm the primary cluster also received "secondarykey", "secondaryvalue" entry</h4>
<div class="section">
<p>Follow the command in the previous section to connect to the Coherence Console inside the primary cluster.</p>

<p>Use the following command to confirm that entry with "secondarykey" is federated to primary cluster:</p>

<pre
lang="bash"

>get "secondarykey"
secondaryvalue</pre>

</div>
</div>

<h3 id="cleanup">Cleaning up</h3>
<div class="section">
<p>Use the following commands to delete the primary and secondary clusters:</p>

<pre
lang="bash"

>kubectl -n coherence-example delete -f src/main/yaml/primary-cluster.yaml

kubectl -n coherence-example delete -f src/main/yaml/secondary-cluster.yaml</pre>

<p>Uninstall the Coherence operator using the undeploy commands for whichever method you chose to install it.</p>

</div>
</div>
</div>
</doc-view>
<!-- pages/examples/100_tls.js -->
<doc-view>

<h2 id="_secure_coherence_with_tls">Secure Coherence with TLS</h2>
<div class="section">
<p>This example is going to show how to use TLS (or SSL) to secure communication between different parts of a Coherence cluster and applications. This is quite a long guide as there are a number of things that can be secured wth TLS.</p>

<p>This example shows how to secure various parts of Coherence clusters using TLS.
You can find the source code in the <a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/tree/master/examples/tls">Operator GitHub Repo</a></p>

<p>In this example we are going to use <a id="" title="" target="_blank" href="https://cert-manager.io">Cert Manager</a> to manage the keys and certs for our Coherence server and clients. Cert Manage makes managing certificates in Kubernetes very simple, but it isn&#8217;t the only solution.</p>

<p>Although securing clusters with TLS is a common request, if running in a secure isolated Kubernetes cluster, you need to weigh up the pros and cons regarding the performance impact TLS will give over the additional security.</p>

<p>Using Cert Manager we will ultimately end up with four k8s <code>Secrets</code>:</p>

<ul class="ulist">
<li>
<p>A <code>Secret</code> containing the server keys, certs, keystore and truststore</p>

</li>
<li>
<p>A <code>Secret</code> containing a single file containing the server keystore, truststore and key password</p>

</li>
<li>
<p>A <code>Secret</code> containing the client keys, certs, keystore and truststore</p>

</li>
<li>
<p>A <code>Secret</code> containing a single file containing the client keystore, truststore and key password</p>

</li>
</ul>
<p>If you do not want to use Cert Manager to try this example then a long as you have a way to create the required <code>Secrets</code> containing the keys and passwords above then you can skip to the section on <router-link to="#coherence" @click.native="this.scrollFix('#coherence')">Securing Coherence</router-link>.</p>


<h4 id="_what_the_example_will_cover">What the Example will Cover</h4>
<div class="section">
<ul class="ulist">
<li>
<p><router-link to="#install_operator" @click.native="this.scrollFix('#install_operator')">Install the Operator</router-link></p>

</li>
<li>
<p><router-link to="#setup_cert_manager" @click.native="this.scrollFix('#setup_cert_manager')">Setting Up Cert-Manager</router-link></p>
<ul class="ulist">
<li>
<p><router-link to="#create_self_signed_issuer" @click.native="this.scrollFix('#create_self_signed_issuer')">Create the SelfSigned Issuer</router-link></p>

</li>
<li>
<p><router-link to="#create_ce_cert" @click.native="this.scrollFix('#create_ce_cert')">Create the CA Certificate</router-link></p>

</li>
<li>
<p><router-link to="#create_ca_issuer" @click.native="this.scrollFix('#create_ca_issuer')">Create the CA issuer</router-link></p>

</li>
<li>
<p><router-link to="#create_coherence_keystores" @click.native="this.scrollFix('#create_coherence_keystores')">Create the Coherence Keys, Certs and KeyStores</router-link></p>
<ul class="ulist">
<li>
<p><router-link to="#server_password_secret" @click.native="this.scrollFix('#server_password_secret')">Create the Server Keystore Password Secret</router-link></p>

</li>
<li>
<p><router-link to="#server_cert" @click.native="this.scrollFix('#server_cert')">Create the Server Certificate</router-link></p>

</li>
<li>
<p><router-link to="#client_certs" @click.native="this.scrollFix('#client_certs')">Create the Client Certificate</router-link></p>

</li>
</ul>
</li>
</ul>
</li>
<li>
<p><router-link to="#coherence" @click.native="this.scrollFix('#coherence')">Securing Coherence Clusters</router-link></p>
<ul class="ulist">
<li>
<p><router-link to="#images" @click.native="this.scrollFix('#images')">Build the Example Images</router-link></p>

</li>
<li>
<p><router-link to="#socket_provider" @click.native="this.scrollFix('#socket_provider')">Configure a Socket Provider</router-link></p>

</li>
</ul>
</li>
<li>
<p><router-link to="#tcmp" @click.native="this.scrollFix('#tcmp')">Secure Cluster Membership</router-link></p>

</li>
<li>
<p><router-link to="#extend" @click.native="this.scrollFix('#extend')">Secure Extend</router-link></p>

</li>
<li>
<p><router-link to="#grpc" @click.native="this.scrollFix('#grpc')">Secure gRPC</router-link></p>

</li>
</ul>
</div>
</div>

<h2 id="install_operator">Install the Operator</h2>
<div class="section">
<p>To run the examples below, you will need to have installed the Coherence Operator, do this using whatever method you prefer from the <a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/docs/latest/#/installation/01_installation">Installation Guide</a></p>

</div>

<h2 id="setup_cert_manager">Setting Up Cert-Manager</h2>
<div class="section">
<p>In this example we will use self-signed certs as this makes everything easy to get going.
Cert Manager has a number of ways to configure real certificates for production use.
Assuming that you&#8217;ve installed Cert Manager using one of the methods in its <a id="" title="" target="_blank" href="https://cert-manager.io/docs/installation/">Install Guide</a> we can proceed to created all of the required resources.</p>


<h3 id="create_self_signed_issuer">Create the SelfSigned Issuer</h3>
<div class="section">
<p>This is used to generate a root CA for use with the CA Issuer.
Here we are using a <code>ClusterIssuer</code> so that we can use a single self-signed issuer across all namespaces.
We could have instead created an <code>Issuer</code> in a single namespace.</p>

<pre
lang="yaml"
title="manifests/selfsigned-issuer.yaml"
>apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}</pre>

<p>Create the <code>ClusterIssuer</code> with the following command. As this is a <code>ClusterIssuer</code>, is does not require a namespace.</p>

<pre
lang="bash"

>kubectl apply -f manifests/selfsigned-issuer.yaml</pre>

<p>We can list the <code>ClusterIssuers</code> in the cluster:</p>

<pre
lang="bash"

>kubectl get clusterissuer</pre>

<p>We should see that the <code>selfsigned-issuer</code> is present and is ready.</p>

<pre
lang="bash"

>NAME                READY   AGE
selfsigned-issuer   True    14m</pre>

</div>

<h3 id="create_ce_cert">Create the CA Certificate</h3>
<div class="section">
<p>Were going to create an internal CA that will be used to sign our certificate requests for the Coherence server and clients that we will run later. Both the server and client will use the CA to validate a connection.</p>

<p>To create the CA issuer, first create a self-signed CA certificate.</p>

<pre
lang="yaml"
title="manifests/ca-cert.yaml"
>apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ca-certificate
spec:
  issuerRef:
    name: selfsigned-issuer   <span class="conum" data-value="1" />
    kind: ClusterIssuer
    group: cert-manager.io
  secretName: ca-cert        <span class="conum" data-value="2" />
  duration: 2880h # 120d
  renewBefore: 360h # 15d
  commonName: Cert Admin
  isCA: true                 <span class="conum" data-value="3" />
  privateKey:
    size: 2048
  usages:
    - digital signature
    - key encipherment</pre>

<ul class="colist">
<li data-value="1">The certificate will use the <code>selfsigned-issuer</code> cluster issuer we created above.</li>
<li data-value="2">There will be a secret named <code>ca-cert</code> created containing the key and certificate</li>
<li data-value="3">Note that the <code>isCA</code> field is set to <code>true</code> in the body of the spec.</li>
</ul>
<p>The CA issuer that we will create later will also be a <code>ClusterIssuer</code>, so in order for the issuer to find the <code>Certificate</code> above we will create the certificate in the <code>cert-manager</code> namespace, which is where Cert Manager is running.</p>

<pre
lang="bash"

>kubectl -n cert-manager apply -f manifests/ca-cert.yaml</pre>

<p>We can see that the certificate was created and should be ready:</p>

<pre
lang="bash"

>kubectl -n cert-manager get certificate</pre>

<pre
lang="bash"

>NAME             READY   SECRET    AGE
ca-certificate   True    ca-cert   12m</pre>

<p>There will also be a secret named <code>ca-secret</code> created in the <code>cert-manager</code> namespace.
The Secret will contain the certificate and signing key, this will be created when the CA certificate is deployed, and the CA issuer will reference that secret.</p>

</div>

<h3 id="create_ca_issuer">Create the CA issuer.</h3>
<div class="section">
<p>As with the self-signed issuer above, we will create a <code>ClusterIssuer</code> for the CA issuer.</p>

<pre
lang="bash"
title="manifests/ca-cert.yaml"
>apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: ca-issuer
spec:
  ca:
    secretName: ca-cert  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>ca-issuer</code> will use the <code>ca-cert</code> secret created by the <code>ca-certificate</code> <code>Certificate</code> we created above.</li>
</ul>
<p>Create the CA issuer with the following command. As this is a <code>ClusterIssuer</code>, is does not require a namespace.</p>

<pre
lang="bash"

>kubectl apply -f manifests/ca-issuer.yaml</pre>

<p>You can then check that the issuer have been successfully configured by checking the status.</p>

<pre
lang="bash"

>kubectl get clusterissuer</pre>

<p>We should see that both <code>ClusterIssuers</code> we created are present and is ready.</p>

<pre
lang="bash"

>NAME                READY   AGE
ca-issuer           True    22m
selfsigned-issuer   True    31m</pre>

</div>

<h3 id="create_coherence_keystores">Create the Coherence Keys, Certs and KeyStores</h3>
<div class="section">
<p>As the Coherence server, and client in this example, are Java applications they will require Java keystores to hold the certificates. We can use Cert-Manager to create these for us.</p>


<h4 id="_create_a_namespace">Create a Namespace</h4>
<div class="section">
<p>We will run the Coherence cluster in a namespace called <code>coherence-test</code>, so we will first create this:</p>

<pre
lang="bash"

>kubectl create ns coherence-test</pre>

</div>

<h4 id="server_password_secret">Create the Server Keystore Password Secret</h4>
<div class="section">
<p>The keystore will be secured with a password. We will create this password in a <code>Secret</code> so that Cert-Manager can find and use it.
The simplest way to create this secret is with kubectl:</p>

<pre
lang="bash"

>kubectl -n coherence-test create secret generic \
    server-keystore-secret --from-literal=password-key=[your-password]</pre>

<p>&#8230;&#8203;replacing <code>[your-password]</code> with the actual password you want to use.
Resulting in a <code>Secret</code> similar to this:</p>

<pre
lang="bash"
title="manifests/ca-cert.yaml"
>apiVersion: v1
kind: Secret
metadata:
  name: server-keystore-secret
data:
  password-key: "cGFzc3dvcmQ=" <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">In this example the password used is <code>password</code></li>
</ul>
</div>

<h4 id="server_cert">Create the Server Certificate</h4>
<div class="section">
<p>We can now create the server certificate and keystore.</p>

<pre
lang="yaml"
title="manifests/server-keystore.yaml"
>apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: server-keystore
spec:
  issuerRef:
    name: ca-issuer                   <span class="conum" data-value="1" />
    kind: ClusterIssuer
    group: cert-manager.io
  secretName: coherence-server-certs  <span class="conum" data-value="2" />
  keystores:
    jks:
      create: true
      passwordSecretRef:
        key: password-key
        name: server-keystore-secret  <span class="conum" data-value="3" />
  duration: 2160h # 90d
  renewBefore: 360h # 15d
  privateKey:
    size: 2048
    algorithm: RSA
    encoding: PKCS1
  usages:
    - digital signature
    - key encipherment
    - client auth
    - server auth
  commonName: Coherence Certs</pre>

<ul class="colist">
<li data-value="1">The issuer will the <code>ClusterIssuer</code> named <code>ca-issuer</code> that we created above.</li>
<li data-value="2">The keys, certs and keystores will be created in a secret named <code>coherence-server-certs</code></li>
<li data-value="3">The keystore password secret is the <code>Secret</code> named <code>server-keystore-secret</code> we created above</li>
</ul>
<p>We can create the certificate in the <code>coherence-test</code> namespace with the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/server-keystore.yaml</pre>

<p>If we list the certificate in the <code>coherence-test</code> namespace we should see the new certificate and that it is ready.</p>

<pre
lang="bash"

>kubectl -n coherence-test get certificate</pre>

<pre
lang="bash"

>NAME              READY   SECRET                   AGE
server-keystore   True    coherence-server-certs   4s</pre>

<p>If we list the secrets in the <code>coherence-test</code> namespace we should see both the password secret and the keystore secret:</p>

<pre
lang="bash"

>kubectl -n coherence-test get secret</pre>

<pre
lang="bash"

>NAME                     TYPE                 DATA   AGE
coherence-server-certs   kubernetes.io/tls    5      117s
server-keystore-secret   Opaque               1      2m9s</pre>

</div>

<h4 id="client_certs">Create the Client Certificate</h4>
<div class="section">
<p>We can create the certificates and keystores for the client in exactly the same way we did for the server.</p>

<p>Create a password secret for the client keystore:</p>

<pre
lang="bash"

>kubectl -n coherence-test create secret generic \
    client-keystore-secret --from-literal=password-key=[your-password]</pre>

<p>Create the client certificate and keystore.</p>

<pre
lang="yaml"
title="manifests/client-keystore.yaml"
>apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: client-keystore
spec:
  issuerRef:
    name: ca-issuer                   <span class="conum" data-value="1" />
    kind: ClusterIssuer
    group: cert-manager.io
  secretName: coherence-client-certs  <span class="conum" data-value="2" />
  keystores:
    jks:
      create: true
      passwordSecretRef:
        key: password-key
        name: client-keystore-secret  <span class="conum" data-value="3" />
  duration: 2160h # 90d
  renewBefore: 360h # 15d
  privateKey:
    size: 2048
    algorithm: RSA
    encoding: PKCS1
  usages:
    - digital signature
    - key encipherment
    - client auth
  commonName: Coherence Certs</pre>

<ul class="colist">
<li data-value="1">The issuer is the same cluster-wide <code>ca-issuer</code> that we used for the server.</li>
<li data-value="2">The keys, certs and keystores will be created in a secret named <code>coherence-client-certs</code></li>
<li data-value="3">The keystore password secret is the <code>Secret</code> named <code>client-keystore-secret</code> we created above</li>
</ul>
<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/client-keystore.yaml</pre>

<p>If we list the certificate in the <code>coherence-test</code> namespace we should see the new client certificate and that it is ready.</p>

<pre
lang="bash"

>kubectl -n coherence-test get certificate</pre>

<pre


>NAME              READY   SECRET                   AGE
client-keystore   True    coherence-client-certs   12s
server-keystore   True    coherence-server-certs   2m13s</pre>

</div>
</div>
</div>

<h2 id="coherence">Securing Coherence</h2>
<div class="section">
<p>By this point, you should have installed the Operator and have the four <code>Secrets</code> required, either created by Cert Manager, or manually. Now we can secure Coherence clusters.</p>


<h3 id="images">Build the Test Images</h3>
<div class="section">
<p>This example includes a Maven project that will build a Coherence server and client images with configuration files that allow us to easily demonstrate TLS. To build the images run the following command:</p>

<pre
lang="bash"

>./mvnw clean package jib:dockerBuild</pre>

<p>This will produce two images:</p>

<ul class="ulist">
<li>
<p><code>tls-example-server:1.0.0</code></p>

</li>
<li>
<p><code>tls-example-client:1.0.0</code></p>

</li>
</ul>
<p>These images can run secure or insecure depending on various system properties passed in at runtime.</p>

</div>

<h3 id="socket_provider">Configure a Socket Provider</h3>
<div class="section">
<p>When configuring Coherence to use TLS, we need to configure a socket provider that Coherence can use to create secure socket. We then tell Coherence to use this provider in various places, such as Extend connections, cluster member TCMP connections etc.
This configuration is typically done by adding the provider configuration to the Coherence operational configuration override file.</p>

<p>The Coherence documentation has a lot of details on configuring socket providers in the section on <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/secure/using-ssl-secure-communication.html#GUID-21CBAF48-BA78-4373-AC90-BF668CF31776">Using SSL Secure Communication</a></p>

<p>Below is an example that we will use on the server cluster members</p>

<pre
lang="xml"
title="src/main/resources/tls-coherence-override.xml"
>&lt;coherence xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns="http://xmlns.oracle.com/coherence/coherence-operational-config"
    xsi:schemaLocation="http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd"&gt;
  &lt;cluster-config&gt;
    &lt;socket-providers&gt;
      &lt;socket-provider id="tls"&gt;
        &lt;ssl&gt;
          &lt;protocol&gt;TLS&lt;/protocol&gt;
          &lt;identity-manager&gt;
            &lt;key-store&gt;
              &lt;url system-property="coherence.tls.keystore"/&gt;
              &lt;password-provider&gt;
                &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt;
                  &lt;init-params&gt;
                    &lt;init-param&gt;
                      &lt;param-type&gt;String&lt;/param-type&gt;
                      &lt;param-value system-property="coherence.tls.keystore.password"&gt;/empty.txt&lt;/param-value&gt;
                    &lt;/init-param&gt;
                &lt;/init-params&gt;
              &lt;/password-provider&gt;
            &lt;/key-store&gt;
            &lt;password-provider&gt;
              &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt;
              &lt;init-params&gt;
                &lt;init-param&gt;
                  &lt;param-type&gt;String&lt;/param-type&gt;
                  &lt;param-value system-property="coherence.tls.key.password"&gt;/empty.txt&lt;/param-value&gt;
              &lt;/init-param&gt;
            &lt;/init-params&gt;
          &lt;/password-provider&gt;
          &lt;/identity-manager&gt;
          &lt;trust-manager&gt;
            &lt;key-store&gt;
              &lt;url system-property="coherence.tls.truststore"/&gt;
              &lt;password-provider&gt;
                &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt;
                &lt;init-params&gt;
                  &lt;init-param&gt;
                    &lt;param-type&gt;String&lt;/param-type&gt;
                    &lt;param-value system-property="coherence.tls.truststore.password"&gt;/empty.txt&lt;/param-value&gt;
                  &lt;/init-param&gt;
                &lt;/init-params&gt;
              &lt;/password-provider&gt;
            &lt;/key-store&gt;
          &lt;/trust-manager&gt;
        &lt;/ssl&gt;
      &lt;/socket-provider&gt;
    &lt;/socket-providers&gt;
  &lt;/cluster-config&gt;
&lt;/coherence&gt;</pre>

<p>The file above has a number of key parts.</p>

<p>We must give the provider a name so that we can refer to it in other configuration.
This is done by setting the <code>id</code> attribute of the <code>&lt;socket-provider&gt;</code> element. In this case we name the provider "tls" in <code>&lt;socket-provider id="tls"&gt;</code>.</p>

<p>We set the <code>&lt;protocol&gt;</code> element to TLS to tell Coherence that this is a TLS socket.</p>

<p>We need to set the keystore URL. If we always used a common location, we could hard code it in the configuration. In this case we will configure the <code>&lt;keystore&gt;&lt;url&gt;</code> element to be injected from a system property which we will configure at runtime <code>&lt;url system-property="coherence.tls.keystore"/&gt;</code>.</p>

<p>We obviously do not want hard-coded passwords in our configuration.
In this example we will use a password provider, which is a class implementing the <code>com.tangosol.net.PasswordProvider</code> interface, that can provide the password by reading file.
In this case the file will be the one from the password secret created above that we will mount into the container.</p>

<pre
lang="xml"
title="src/main/resources/server-cache-config.xml"
>&lt;password-provider&gt;
  &lt;class-name&gt;com.oracle.coherence.k8s.FileBasedPasswordProvider&lt;/class-name&gt;
    &lt;init-params&gt;
      &lt;init-param&gt;
        &lt;param-type&gt;String&lt;/param-type&gt;
        &lt;param-value system-property="coherence.tls.keystore.password"/&gt;
      &lt;/init-param&gt;
  &lt;/init-params&gt;
&lt;/password-provider&gt;</pre>

<p>In the snippet above the password file location will be passed in using the
<code>coherence.tls.keystore.password</code> system property.</p>

<p>We declare another password provider for the private key password.</p>

<p>We then declare the configuration for the truststore, which follows the same pattern as the keystore.</p>

<p>The configuration above is included in both of the example images that we built above.</p>

</div>
</div>

<h2 id="tcmp">Secure Cluster Membership</h2>
<div class="section">
<p>Now we have a "tls" socket provider we can use it to secure Coherence. The Coherence documentation has a section on <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/secure/using-ssl-secure-communication.html#GUID-21CBAF48-BA78-4373-AC90-BF668CF31776">Securing Coherence TCMP with TLS</a>.
Securing communication between cluster members is very simple, we just set the <code>coherence.socketprovider</code> system property to the name of the socket provider we want to use. In our case this will be the "tls" provider we configured above, so we would use <code>-Dcoherence.socketprovider=tls</code></p>

<p>The yaml below is a <code>Coherence</code> resource that will cause the Operator to create a three member Coherence cluster.</p>

<pre
lang="yaml"
title="manifests/coherence-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: tls-cluster
spec:
  replicas: 3
  image: tls-example-server:1.0.0    <span class="conum" data-value="1" />
  cluster: test-cluster              <span class="conum" data-value="2" />
  coherence:
    overrideConfig: tls-coherence-override.xml  <span class="conum" data-value="3" />
    cacheConfig: server-cache-config.xml        <span class="conum" data-value="4" />
  jvm:
    args:
      - -Dcoherence.socketprovider=tls  <span class="conum" data-value="5" />
      - -Dcoherence.tls.keystore=file:/coherence/certs/keystore.jks
      - -Dcoherence.tls.keystore.password=file:/coherence/certs/credentials/password-key
      - -Dcoherence.tls.key.password=file:/coherence/certs/credentials/password-key
      - -Dcoherence.tls.truststore=file:/coherence/certs/truststore.jks
      - -Dcoherence.tls.truststore.password=file:/coherence/certs/credentials/password-key
  secretVolumes:
    - mountPath: coherence/certs             <span class="conum" data-value="6" />
      name: coherence-server-certs
    - mountPath: coherence/certs/credentials
      name: server-keystore-secret
  ports:
    - name: extend  <span class="conum" data-value="7" />
      port: 20000
    - name: grpc
      port: 1408
    - name: management
      port: 30000
    - name: metrics
      port: 9612</pre>

<ul class="colist">
<li data-value="1">The image name is the server image built from this example project</li>
<li data-value="2">We specify a cluster name because we want to be able to demonstrate other Coherence deployments can or cannot join this cluster, so their yaml files will use this same cluster name.</li>
<li data-value="3">We set the Coherence override file to the file containing the "tls" socket provider configuration.</li>
<li data-value="4">We use a custom cache configuration file that has an Extend proxy that we can secure later.</li>
<li data-value="5">We set the <code>coherence.socketprovider</code> system property to use the "tls" provider, we also set a number of other properties that will set the locations of the keystores and password files to map to the secret volume mounts.</li>
<li data-value="6">We mount the certificate and password secrets to volumes</li>
<li data-value="7">We expose some ports for clients which we will use later, and for management, so we can enquire on the cluster state using REST.</li>
</ul>
<p>Install the yaml above into the <code>coherence-test</code> namespace:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/coherence-cluster.yaml</pre>

<p>If we list the Pods in the <code>coherence-test</code> namespace then after a minute or so there should be three ready Pods.</p>

<pre
lang="bash"

>kubectl -n coherence-test get pods</pre>

<pre
lang="bash"

>NAME             READY   STATUS    RESTARTS   AGE
tls-cluster-0    1/1     Running   0          88s
tls-cluster-1    1/1     Running   0          88s
tls-cluster-2    1/1     Running   0          88s</pre>


<h3 id="_port_forward_to_the_rest_management_port">Port Forward to the REST Management Port</h3>
<div class="section">
<p>Remember that we exposed a number of ports in our Coherence cluster, one of these was REST management on port <code>30000</code>.
We can use this along with <code>curl</code> to enquire about the cluster state.
We need to use <code>kubectl</code> to forward a local port to one of the Coherence Pods.</p>

<p>Open another terminal session and run the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-test port-forward tls-cluster-0 30000:30000</pre>

<p>This will forward port <code>30000</code> on the local machine (e.g. your dev laptop) to the <code>tls-cluster-0</code> Pod.</p>

<p>We can now obtain the cluster state from the REST endpoint with the following command:</p>

<pre
lang="bash"

>curl -X GET http://127.0.0.1:30000/management/coherence/cluster</pre>

<p>or if you have the <a id="" title="" target="_blank" href="https://stedolan.github.io/jq/">jq</a> utility we can pretty print the json output:</p>

<pre
lang="bash"

>curl -X GET http://127.0.0.1:30000/management/coherence/cluster | jq</pre>

<p>We will see json something like this:</p>

<pre
lang="json"

>{
  "links": [
  ],
  "clusterSize": 3,      <span class="conum" data-value="1" />
  "membersDeparted": [],
  "memberIds": [
    1,
    2,
    3
  ],
  "oldestMemberId": 1,
  "refreshTime": "2021-03-07T12:27:20.193Z",
  "licenseMode": "Development",
  "localMemberId": 1,
  "version": "21.06",
  "running": true,
  "clusterName": "test-cluster",
  "membersDepartureCount": 0,
  "members": [                     <span class="conum" data-value="2" />
    "Member(Id=1, Timestamp=2021-03-07 12:24:32.982, Address=10.244.1.6:38271, MachineId=17483, Location=site:zone-two,rack:two,machine:operator-worker2,process:33,member:tls-cluster-1, Role=tls-cluster)",
    "Member(Id=2, Timestamp=2021-03-07 12:24:36.572, Address=10.244.2.5:36139, MachineId=21703, Location=site:zone-one,rack:one,machine:operator-worker,process:35,member:tls-cluster-0, Role=tls-cluster)",
    "Member(Id=3, Timestamp=2021-03-07 12:24:36.822, Address=10.244.1.7:40357, MachineId=17483, Location=site:zone-two,rack:two,machine:operator-worker2,process:34,member:tls-cluster-2, Role=tls-cluster)"
  ],
  "type": "Cluster"
}</pre>

<ul class="colist">
<li data-value="1">We can see that the cluster size is three.</li>
<li data-value="2">The member list shows details of the three Pods in the cluster</li>
</ul>
</div>

<h3 id="_start_non_tls_cluster_members">Start Non-TLS Cluster Members</h3>
<div class="section">
<p>To demonstrate that the cluster is secure we can start another cluster with yaml that does not enable TLS.</p>

<pre
lang="yaml"
title="manifests/coherence-cluster-no-tls.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: no-tls-cluster
spec:
  replicas: 3
  image: tls-example-server:1.0.0     <span class="conum" data-value="1" />
  cluster: test-cluster               <span class="conum" data-value="2" />
  coherence:
    cacheConfig: server-cache-config.xml
  ports:
    - name: extend
      port: 20000
    - name: grpc
      port: 1408
    - name: management
      port: 30000
    - name: metrics
      port: 9612</pre>

<ul class="colist">
<li data-value="1">This <code>Coherence</code> resource uses the same server image as the secure cluster</li>
<li data-value="2">This <code>Coherence</code> resource also uses the same cluster name as the secure cluster, <code>test-cluster</code>, so it should attempt to join with the secure cluster.
If the existing cluster is not secure, we will end up with a cluster of six members.</li>
</ul>
<p>Install the yaml above into the <code>coherence-test</code> namespace:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/coherence-cluster-no-tls.yaml</pre>

<p>If we list the Pods in the <code>coherence-test</code> namespace then after a minute or so there should be three ready Pods.</p>

<pre
lang="bash"

>kubectl -n coherence-test get pods</pre>

<pre
lang="bash"

>NAME                READY   STATUS    RESTARTS   AGE
tls-cluster-0       1/1     Running   0          15m
tls-cluster-1       1/1     Running   0          15m
tls-cluster-2       1/1     Running   0          15m
no-tls-cluster-0    1/1     Running   0          78s
no-tls-cluster-1    1/1     Running   0          78s
no-tls-cluster-2    1/1     Running   0          78s</pre>

<p>There are six pods running, but they have not formed a six member cluster.
If we re-run the curl command to query the REST management endpoint of the secure cluster we will see that the cluster size is still three:</p>

<pre
lang="bash"

>curl -X GET http://127.0.0.1:30000/management/coherence/cluster -s | jq '.clusterSize'</pre>

<p>What happens is that the non-TLS members have effectively formed their own cluster of three members, but have not been able to form a cluster with the TLS enabled members.</p>

</div>

<h3 id="_cleanup">Cleanup</h3>
<div class="section">
<p>After trying the example, remove both clusters with the corresponding <code>kubectl delete</code> commands so that they do not interfere with the next example.</p>

<pre
lang="bash"

>kubectl -n coherence-test delete -f manifests/coherence-cluster-no-tls.yaml

kubectl -n coherence-test delete -f manifests/coherence-cluster.yaml</pre>

</div>

<h3 id="extend">Secure Extend Connections</h3>
<div class="section">
<p>A common connection type to secure are client connections into the cluster from Coherence Extend clients. The Coherence documentation contains details on <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/secure/using-ssl-secure-communication.html#GUID-0F636928-8731-4228-909C-8B8AB09613DB">Using SSL to Secure Extend Client Communication</a> for more in-depth details.</p>

<p>As with securing TCMP, we can specify a socket provider in the Extend proxy configuration in the server&#8217;s cache configuration file and also in the remote scheme in the client&#8217;s cache configuration. In this example we will use exactly the same TLS socket provider configuration that we created above. The only difference being the name of the <code>PasswordProvider</code> class used by the client. At the time of writing this, Coherence does not include an implementation of <code>PasswordProvider</code> that reads from a file. The Coherence Operator injects one into the classpath of the server, but our simple client is not managed by the Operator. We have added a simple <code>FileBasedPasswordProvider</code> class to the client code in this example.</p>


<h4 id="_secure_the_proxy">Secure the Proxy</h4>
<div class="section">
<p>To enable TLS for an Extend proxy, we can just specify the name of the socket provider that we want to use in the <code>&lt;proxy-scheme&gt;</code> in the server&#8217;s cache configuration file.</p>

<p>The snippet of configuration below is taken from the <code>server-cache-config.xml</code> file in the example source.</p>

<pre
lang="xml"
title="src/main/resources/server-cache-config.xml"
>&lt;proxy-scheme&gt;
    &lt;service-name&gt;Proxy&lt;/service-name&gt;
    &lt;acceptor-config&gt;
        &lt;tcp-acceptor&gt;
            &lt;socket-provider system-property="coherence.extend.socket.provider"/&gt;       <span class="conum" data-value="1" />
            &lt;local-address&gt;
                &lt;address system-property="coherence.extend.address"&gt;0.0.0.0&lt;/address&gt;   <span class="conum" data-value="2" />
                &lt;port system-property="coherence.extend.port"&gt;20000&lt;/port&gt;              <span class="conum" data-value="3" />
            &lt;/local-address&gt;
        &lt;/tcp-acceptor&gt;
    &lt;/acceptor-config&gt;
    &lt;load-balancer&gt;client&lt;/load-balancer&gt;
    &lt;autostart&gt;true&lt;/autostart&gt;
&lt;/proxy-scheme&gt;</pre>

<ul class="colist">
<li data-value="1">The <code>&lt;socket-provider&gt;</code> element is empty by default, but is configured to be set from the system property named <code>coherence.extend.socket.provider</code>. This means that by default, Extend will run without TLS. If we start the server with the system property set to "tls", the name of our socket provider, then the proxy will use TLS.</li>
<li data-value="2">The Extend proxy will bind to all local addresses.</li>
<li data-value="3">The Extend proxy service will bind to port 20000.</li>
</ul>
<p>We add the additional <code>coherence.extend.socket.provider</code> system property to the <code>spec.jvm.args</code> section of the Coherence resource yaml we will use to deploy the server. The yaml below is identical to the yaml we used above to secure TCMP, but with the addition of the <code>coherence.extend.socket.provider</code> property.</p>

<pre
lang="yaml"
title="coherence-cluster-extend.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: tls-cluster
spec:
  replicas: 3
  image: tls-example-server:1.0.0
  cluster: test-cluster
  coherence:
    cacheConfig: server-cache-config.xml
    overrideConfig: tls-coherence-override.xml
  jvm:
    args:
      - -Dcoherence.socketprovider=tls
      - -Dcoherence.extend.socket.provider=tls    <span class="conum" data-value="1" />
      - -Dcoherence.tls.keystore=file:/coherence/certs/keystore.jks
      - -Dcoherence.tls.keystore.password=file:/coherence/certs/credentials/password-key
      - -Dcoherence.tls.key.password=file:/coherence/certs/credentials/password-key
      - -Dcoherence.tls.truststore=file:/coherence/certs/truststore.jks
      - -Dcoherence.tls.truststore.password=file:/coherence/certs/credentials/password-key
  secretVolumes:
    - mountPath: coherence/certs
      name: coherence-server-certs
    - mountPath: coherence/certs/credentials
      name: server-keystore-secret
  ports:
    - name: extend
      port: 20000
    - name: grpc
      port: 1408</pre>

<ul class="colist">
<li data-value="1">The <code>-Dcoherence.extend.socket.provider=tls</code> has been added to enable TLS for the Extend proxy.</li>
</ul>
<p>Installing the yaml above will give us a Coherence cluster that uses TLS for both TCMP inter-cluster communication and for Extend connections.</p>

</div>

<h4 id="_install_the_cluster">Install the Cluster</h4>
<div class="section">
<p>We can install the Coherence cluster defined in the yaml above using <code>kubectl</code>:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/coherence-cluster-extend.yaml</pre>

<p>After a minute or two the three Pods should be ready, which can be confirmed with <code>kubectl</code>.
Because the yaml above declares a port named <code>extend</code> on port <code>20000</code>, the Coherence Operator will create a k8s <code>Service</code> to expose this port. The service name will be the Coherence resource name suffixed with the port name, so in this case <code>tls-cluster-extend</code>. As a <code>Service</code> in k8s can be looked up by DNS, we can use this service name as the host name for the client to connect to.</p>

</div>

<h4 id="_configure_the_extend_client">Configure the Extend Client</h4>
<div class="section">
<p>Just like the server, we can include a socket provider configuration in the override file and configure the name of the socket provider that the client should use in the client&#8217;s cache configuration file. The socket provider configuration is identical to that shown already above (with the different <code>FileBasedPasswordProvider</code> class name).</p>

<p>The Extend client code used in the <code>src/main/java/com/oracle/coherence/examples/k8s/client/Main.java</code> file in this example just starts a Coherence client, then obtains a <code>NamedMap</code>, and in a very long loop just puts data into the map, logging out the keys added. This is very trivial but allows us to see that the client is connected and working (or not).</p>

<p>The snippet of xml below is from the client&#8217;s cache configuration file.</p>

<pre
lang="xml"
title="src/main/resources/client-cache-config.xml"
>&lt;remote-cache-scheme&gt;
    &lt;scheme-name&gt;remote&lt;/scheme-name&gt;
    &lt;service-name&gt;Proxy&lt;/service-name&gt;
    &lt;initiator-config&gt;
        &lt;tcp-initiator&gt;
            &lt;socket-provider system-property="coherence.extend.socket.provider"/&gt;           <span class="conum" data-value="1" />
            &lt;remote-addresses&gt;
                &lt;socket-address&gt;
                    &lt;address system-property="coherence.extend.address"&gt;127.0.0.1&lt;/address&gt; <span class="conum" data-value="2" />
                    &lt;port system-property="coherence.extend.port"&gt;20000&lt;/port&gt;              <span class="conum" data-value="3" />
                &lt;/socket-address&gt;
            &lt;/remote-addresses&gt;
        &lt;/tcp-initiator&gt;
    &lt;/initiator-config&gt;
&lt;/remote-cache-scheme&gt;</pre>

<ul class="colist">
<li data-value="1">The <code>&lt;socket-provider&gt;</code> element is empty by default, but is configured to be set from the system property named <code>coherence.extend.socket.provider</code>. This means that by default, the Extend client will connect without TLS. If we start the client with the system property set to "tls", the name of our socket provider, then the client will use TLS.</li>
<li data-value="2">By default, the Extend client will connect loopback, on <code>127.0.0.1</code> but this can be overridden by setting the <code>coherence.extend.address</code> system property. We will use this when we deploy the client to specify the name of the <code>Service</code> that is used to expose the server&#8217;s Extend port.</li>
<li data-value="3">The Extend client will connect to port 20000. Although this can be overridden with a system property, port 20000 is also the default port used by the server, so there is no need to override it.</li>
</ul>
</div>

<h4 id="_start_an_insecure_client">Start an Insecure Client</h4>
<div class="section">
<p>As a demonstration we can first start a non-TLS client and see what happens. We can create a simple <code>Pod</code> that will run the client image using the yaml below.</p>

<p>One of the features of newer Coherence CE versions is that configuration set via system properties prefixed with <code>coherence.</code> can also be set with corresponding environment variable names. The convention used for the environment variable name is to convert the system property name to uppercase and convert "." characters to "_", so setting the cache configuration file with the <code>coherence.cacheconfig</code> system property can be done using the <code>COHERENCE_CACHECONFIG</code> environment variable.
This makes it simple to set Coherence configuration properties in a Pod yaml using environment variables instead of having to build a custom Java command line.</p>

<pre
lang="yaml"
title="manifests/client-no-tls.yaml"
>apiVersion: v1
kind: Pod
metadata:
  name: client
spec:
  containers:
    - name: client
      image: tls-example-client:1.0.0
      env:
        - name: COHERENCE_CACHECONFIG       <span class="conum" data-value="1" />
          value: client-cache-config.xml
        - name: COHERENCE_EXTEND_ADDRESS    <span class="conum" data-value="2" />
          value: tls-cluster-extend</pre>

<ul class="colist">
<li data-value="1">The client will use the <code>client-cache-config.xml</code> cache configuration file.</li>
<li data-value="2">The <code>COHERENCE_EXTEND_ADDRESS</code> is set to <code>tls-cluster-extend</code>, which is the name of the service exposing the server&#8217;s Extend port and which will be injected into the client&#8217;s cache configuration file, as explained above.</li>
</ul>
<p>We can run the client Pod with the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/client-no-tls.yaml</pre>

<p>If we look at the Pods now in the <code>coherence-test</code> namespace we will see the client running:</p>

<pre
lang="bash"

>$ kubectl -n coherence-test get pod</pre>

<pre
lang="bash"

>NAME            READY   STATUS    RESTARTS   AGE
client          1/1     Running   0          3s
tls-cluster-0   1/1     Running   0          2m8s
tls-cluster-1   1/1     Running   0          2m8s
tls-cluster-2   1/1     Running   0          2m8s</pre>

<p>If we look at the log of the client Pod though we will see a stack trace with the cause:</p>

<pre
lang="bash"

>kubectl -n coherence-test logs client</pre>

<pre


>2021-03-07 12:53:13.481/1.992 Oracle Coherence CE 21.06 &lt;Error&gt; (thread=main, member=n/a): Error while starting service "Proxy": com.tangosol.net.messaging.ConnectionException: could not establish a connection to one of the following addresses: []</pre>

<p>This tells us that the client failed to connect to the cluster, because the client is not using TLS.</p>

<p>We can remove the non-TLS client:</p>

<pre


>kubectl -n coherence-test delete -f manifests/client-no-tls.yaml</pre>

</div>

<h4 id="_start_a_tls_enabled_client">Start a TLS Enabled Client</h4>
<div class="section">
<p>We can now modify the client yaml to run the client with TLS enabled.
The client image already contains the <code>tls-coherence-override.xml</code> file with the configuration for the TLS socket provider.
We need to set the relevant environment variables to inject the location of the keystores and tell Coherence to use the "tls" socket provider for the Extend connection.</p>

<pre
lang="yaml"
title="manifests/client.yaml"
>apiVersion: v1
kind: Pod
metadata:
  name: client
spec:
  containers:
    - name: client
      image: tls-example-client:1.0.0
      env:
        - name: COHERENCE_CACHECONFIG
          value: client-cache-config.xml
        - name: COHERENCE_EXTEND_ADDRESS
          value: tls-cluster-extend
        - name: COHERENCE_OVERRIDE
          value: tls-coherence-override.xml                 <span class="conum" data-value="1" />
        - name: COHERENCE_EXTEND_SOCKET_PROVIDER
          value: tls
        - name: COHERENCE_TLS_KEYSTORE
          value: file:/coherence/certs/keystore.jks
        - name: COHERENCE_TLS_KEYSTORE_PASSWORD
          value: /coherence/certs/credentials/password-key
        - name: COHERENCE_TLS_KEY_PASSWORD
          value: /coherence/certs/credentials/password-key
        - name: COHERENCE_TLS_TRUSTSTORE
          value: file:/coherence/certs/truststore.jks
        - name: COHERENCE_TLS_TRUSTSTORE_PASSWORD
          value: /coherence/certs/credentials/password-key
      volumeMounts:                                         <span class="conum" data-value="2" />
        - name: coherence-client-certs
          mountPath: coherence/certs
        - name: keystore-credentials
          mountPath: coherence/certs/credentials
  volumes:                                                  <span class="conum" data-value="3" />
    - name: coherence-client-certs
      secret:
        defaultMode: 420
        secretName: coherence-client-certs
    - name: keystore-credentials
      secret:
        defaultMode: 420
        secretName: client-keystore-secret</pre>

<ul class="colist">
<li data-value="1">The yaml is identical to the non-TLS client with the addition of the environment variables to configure TLS.</li>
<li data-value="2">We create volume mount points to map the Secret volumes containing the keystores and password to directories in the container</li>
<li data-value="3">We mount the Secrets as volumes</li>
</ul>
<p>We can run the client Pod with the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f manifests/client.yaml</pre>

<p>If we now look at the client&#8217;s logs:</p>

<pre
lang="bash"

>kubectl -n coherence-test logs client</pre>

<p>The end of the log should show the messages from the client as it puts each entry into a <code>NamedMap</code>.</p>

<pre


>Put 0
Put 1
Put 2
Put 3
Put 4
Put 5</pre>

<p>So now we have a TLS secured Extend proxy and client.
We can remove the client and test cluster:</p>

<pre
lang="bash"

>kubectl -n coherence-test delete -f manifests/client.yaml

kubectl -n coherence-test delete -f manifests/coherence-cluster-extend.yaml</pre>

</div>
</div>
</div>
</doc-view>
<!-- pages/examples/500_autoscaler.js -->
<doc-view>

<h2 id="_kubernetes_horizontal_pod_autoscaler_example">Kubernetes Horizontal Pod autoscaler Example</h2>
<div class="section">
<p>This example shows how to use the Kubernetes Horizontal Pod Autoscaler to scale Coherence clusters.
You can find the source code in the <a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/tree/master/examples/autoscaler">Operator GitHub Repo</a></p>

</div>

<h2 id="_how_does_the_horizontal_pod_autoscaler_work">How Does the Horizontal Pod autoscaler Work</h2>
<div class="section">
<p>There is a lot of good documentation on the HPA, particularly the <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Kubernetes documentation</a>.</p>

<p>The HPA uses metrics, which it obtains from one of the Kubernetes metrics APIs.
Many cloud providers and custom Kubernetes installations have metrics features that may be able to expose those metrics to
the <code>custom/metrics.k8s.io</code> API.
It is possible to even do everything yourself and build a custom REST endpoint that serves custom metrics to the HPA.
Those alternatives are beyond the scope of this example though so to keep things simple we will use Prometheus.
The diagram below shows, at a high level, how this works.</p>



<v-card>
<v-card-text class="overflow-y-hidden" >
<img src="./images/images/autoscaler.png" alt="autoscaler" />
</v-card-text>
</v-card>

<p>Prometheus will obtain metrics from the Coherence Pod&#8217;s metrics endpoints.
The Prometheus Adapter exposes certain configured metrics polled from Prometheus as custom Kubernetes metrics.
The HPA is configured to poll the custom metrics and use those to scale the <code>Coherence</code> resource (which will in turn cause
the Coherence Operator to scale the <code>StatefulSet</code>).</p>

</div>

<h2 id="_autoscaling_coherence_clusters">Autoscaling Coherence Clusters</h2>
<div class="section">
<p>This example will show how to configure the HPA to scale Coherence clusters based on heap usage metrics.
As Coherence stores data in memory, monitoring heap usage and using it to scale seems a sensible approach.</p>

<p>The <code>Coherence</code> CRD supports the <code>scale</code> sub-resource, which means that the Kubernetes HPA can be
used to scale a <code>Coherence</code> deployment.
In this example we are going to use heap usage as the metric - or to be more specific the amount of heap in use <em>after</em> the
last garbage collection.
This is an important point, plain heap usage is a poor metric to use for scaling decisions because the heap may be very
full at a given point in time, but most of that memory may be garbage so scaling on the plain heap usage figure may cause the
cluster to scale up needlessly as a milli-second later a GC could run, and the heap use shrinks down to acceptable levels.</p>

<p>The problem is that there is no single metric in a JVM that gives heap usage after garbage collection.
Coherence has some metrics that report this value, but they are taken from the <code>MemoryPool</code> MBeans and this is not reliable
for scaling.
For example, if the JVM is using the G1 collector the <code>G1 Old Gen</code> memory pool value for heap use after garbage collection
will be zero unless a full GC has run.
It is quite possible to almost fill the heap without running a full GC so this figure could remain zero or be wildly inaccurate.</p>

<p>A more reliable way to work out the heap usage is to obtain the values for the different heap memory pools from the
Garbage Collector MBeans. There could be multiple of these MBeans with different names depending on which collector
has been configured for the JVM.
The Garbage Collector Mbeans have a <code>LastGCcInfo</code> attribute, which is a composite attribute containing information about the last
garbage collection that ran on this collector. One of the attributes is the <code>endTime</code>, which we can use to determine which
collector&#8217;s <code>LastGCcInfo</code> is the most recent. Once we have this we can obtain the <code>memoryUsageAfterGc</code> attribute for the last gc,
which is a map of memory pool name to heap use data after the GC.
We can use this to then sum up the usages for the different heap memory pools.</p>

<p>The Java code in this example contains a simple MBean class <code>HeapUsage</code> and corresponding MBean interface <code>HeapUsageMBean</code>
that obtain heap use metrics in the way detailed above. There is also a configuration file <code>custom-mbeans.xml</code> that
Coherence will use to automatically add the custom MBean to Coherence management and metrics.
There is Coherence documentation on
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/manage/using-coherence-metrics.html#GUID-CFC31D23-06B8-49AF-8996-ADBA806E0DD9">how to add custom metrics</a>
and
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/manage/registering-custom-mbeans.html#GUID-1EE749C5-BC0D-4353-B5FE-1C5DCDEAE48C">how to register custom MBeans</a>.</p>

<p>The custom heap use MBean will be added with an ObjectName of <code>Coherence:type=HeapUsage,nodeId=1</code> where <code>nodeId</code> will change to
match the Coherence member id for the specific JVM. There will be one heap usage MBean for each cluster member.</p>

<p>The Coherence metrics framework will expose the custom metrics with metric names made up from the MBean domain name,
type, and the attribute name. The MBean has attribute names <code>Used</code> and <code>PercentageUsed</code>, so the metric names will be:</p>

<ul class="ulist">
<li>
<p><code>Coherence.HeapUsage.Used</code></p>

</li>
<li>
<p><code>Coherence.HeapUsage.PercentageUsed</code></p>

</li>
</ul>
<p>These metrics will be scoped as application metrics, as opposed to Coherence standard metrics that are vendor scoped.
This means that in Prometheus the names will be converted to:</p>

<ul class="ulist">
<li>
<p><code>application:coherence_heap_usage_used</code></p>

</li>
<li>
<p><code>application:coherence_heap_usage_percentage_used</code></p>

</li>
</ul>
<p>The metrics will have corresponding tags to identify which cluster member (<code>Pod</code>) they relate to.</p>

</div>

<h2 id="_building_the_example">Building the Example</h2>
<div class="section">

<h3 id="_clone_the_coherence_operator_repository">Clone the Coherence Operator Repository:</h3>
<div class="section">
<p>To build the examples, you first need to clone the Operator GitHub repository to your development machine.</p>

<pre
lang="bash"

>git clone https://github.com/oracle/coherence-operator

cd coherence-operator/examples</pre>

</div>

<h3 id="_build_the_examples">Build the Examples</h3>
<div class="section">

<h4 id="_prerequisites">Prerequisites</h4>
<div class="section">
<ul class="ulist">
<li>
<p>Java 11+ JDK either [OpenJDK](<a id="" title="" target="_blank" href="https://adoptopenjdk.net/">https://adoptopenjdk.net/</a>) or [Oracle JDK](<a id="" title="" target="_blank" href="https://www.oracle.com/java/technologies/javase-downloads.html">https://www.oracle.com/java/technologies/javase-downloads.html</a>)</p>

</li>
<li>
<p>[Docker](<a id="" title="" target="_blank" href="https://docs.docker.com/install/">https://docs.docker.com/install/</a>) version 17.03+.</p>

</li>
<li>
<p>[kubectl](<a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>) version v1.13.0+ .</p>

</li>
<li>
<p>Access to a Kubernetes v1.14.0+ cluster.</p>

</li>
<li>
<p>[Helm](<a id="" title="" target="_blank" href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a>) version 3.2.4+</p>

</li>
</ul>
<p>Building the project requires [Maven](<a id="" title="" target="_blank" href="https://maven.apache.org">https://maven.apache.org</a>) version 3.6.0+.
The commands below use the Maven Wrapper to run the commands, which will install Maven if it is not
already on the development machine. If you already have a suitable version of Maven installed feel free to replace
the use of <code>./mvnw</code> in the examples with your normal Maven command (typically just <code>mvn</code>).</p>


<h5 id="_corporate_proxies">Corporate Proxies</h5>
<div class="section">
<p>If building inside a corporate proxy (or any machine that requires http and https proxies to be configured) then
the build will require the <code>MAVEN_OPTS</code> environment variable to be properly set, for example:</p>

<pre
lang="bash"

>export MAVEN_OPTS="-Dhttps.proxyHost=host -Dhttps.proxyPort=80 -Dhttp.proxyHost=host -Dhttp.proxyPort=80"</pre>

<p>replacing <code>host</code> with the required proxy hostname and <code>80</code> with the proxy&#8217;s port.</p>

</div>
</div>

<h4 id="_build_instructions">Build Instructions</h4>
<div class="section">
<p>The autoscaler example uses the <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin#build-your-image">JIB Maven plugin</a> to build the example image.
To build the image run the following command from the <code>examples/autoscaler</code> directory:</p>

<pre
lang="bash"

>./mvnw package jib:dockerBuild</pre>

<p>The build will produce various example images, for the autoscaler example we will be using the <code>autoscaler-example:latest</code> image.</p>

</div>
</div>
</div>

<h2 id="_run_the_example">Run the Example</h2>
<div class="section">
<p>Running the example requires a number of components to be installed.
The example will use Prometheus as a custom metrics source, which requires installation of Prometheus and the
Prometheus Adapter custom metrics source.</p>

<div class="admonition note">
<p class="admonition-inline">To simplify the example commands none of the examples below use a Kubernetes namespace.
If you wish to install the components below into a namespace other than <code>default</code>, then use the required
kubectl and Helm namespace options.</p>
</div>

<h3 id="_install_the_coherence_operator">Install the Coherence Operator</h3>
<div class="section">
<p>First install the Coherence Operator, TBD&#8230;&#8203;</p>

</div>

<h3 id="_install_coherence_cluster">Install Coherence cluster</h3>
<div class="section">
<p>With the Coherence Operator running we can now install a simple Coherence cluster.
An example of the yaml required is below:</p>

<pre
lang="yaml"
title="cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  image: autoscaler-example:latest  <span class="conum" data-value="1" />
  imagePullPolicy: IfNotPresent
  replicas: 2                       <span class="conum" data-value="2" />
  coherence:
    metrics:
      enabled: true                 <span class="conum" data-value="3" />
  jvm:
    memory:
      heapSize: 500m                <span class="conum" data-value="4" />
  ports:
    - name: metrics                 <span class="conum" data-value="5" />
      serviceMonitor:
        enabled: true               <span class="conum" data-value="6" />
    - name: extend                  <span class="conum" data-value="7" />
      port: 20000</pre>

<ul class="colist">
<li data-value="1">The image used for the application will be the <code>autoscaler-example:latest</code> image we built above.</li>
<li data-value="2">The deployment will initially have 2 replicas.</li>
<li data-value="3">Coherence metrics must be enabled to publish the metrics we require for scaling.</li>
<li data-value="4">In this example the JVM heap has been fixed to <code>500m</code>, which is quite small but this means we do not need to add a lot of data
to cause excessive heap usage when we run the example.</li>
<li data-value="5">The metrics port must also be exposed on a <code>Service</code>.</li>
<li data-value="6">A Prometheus <code>ServiceMonitor</code> must also be enabled for the metrics service so that Prometheus can find the Coherence <code>Pods</code>
and poll metrics from them.</li>
<li data-value="7">This example also exposes a Coherence Extend port so that test data can easily be loaded into the caches.</li>
</ul>
<p>The autoscaler example includes a suitable yaml file named <code>cluster.yaml</code> in the <code>manifests/</code> directory that can be used
to create a Coherence deployment.</p>

<pre
lang="bash"

>kubectl create -f manifests/cluster.yaml</pre>

<p>The <code>Pods</code> that are part of the Coherence cluster can be listed with <code>kubectl</code>.
All the <code>Pods</code> have a label <code>coherenceCluster</code> set by the Coherence Operator to match the name of the
<code>Coherence</code> resource that they belong to, which makes it easier to list <code>Pods</code> for a specific deployment
using <code>kubectl</code>:</p>

<pre
lang="bash"

>kubectl get pod -l coherenceCluster=test-cluster</pre>

<p>In a short time the <code>Pods</code> should both be ready.</p>

<pre
lang="bash"

>NAME             READY   STATUS    RESTARTS   AGE
test-cluster-0   1/1     Running   0          2m52s
test-cluster-1   1/1     Running   0          2m52s</pre>


<h4 id="_test_the_custom_heap_metrics">Test the Custom Heap Metrics</h4>
<div class="section">
<p>The Metrics endpoint will be exposed on port 9612 on each <code>Pod</code>, so it is possible to query the metrics endpoints
for the custom heap metrics. The simplest way to test the metrics is to use the <code>kubectl</code> <code>port-forward</code> command and <code>curl</code>.</p>

<p>In one terminal session start the port forwarder to the first <code>Pod</code>, <code>test-cluster-0</code>:</p>

<pre
lang="bash"

>kubectl port-forward pod/test-cluster-0 9612:9612</pre>

<p>metrics from <code>Pod</code>, <code>test-cluster-0</code> can be queried on <code><a id="" title="" target="_blank" href="http://127.0.0.1:9612/metrics">http://127.0.0.1:9612/metrics</a></code></p>

<p>In a second terminal we can use curl to query the metrics.
The Coherence metrics endpoint serves metrics in two formats, plain text compatible with Prometheus and JSON.
If the required content type has not been specified in the curl command it could be either that is returned.
To specify a content type set the accepted type in the header, for example <code>--header "Accept: text/plain"</code> or
<code>--header "Accept: application/json"</code>.</p>

<p>This command will retrieve metrics from <code>test-cluster-0</code> in the same format that Prometheus would.</p>

<pre
lang="bash"

>curl -s --header "Accept: text/plain" -X GET http://127.0.0.1:9612/metrics</pre>

<p>This will return quite a lot of metrics, somewhere in that output is the custom application metrics for heap usage.
The simplest way to isolate them would be to use <code>grep</code>, for example:</p>

<pre
lang="bash"

>curl -s --header "Accept: text/plain" -X GET http://127.0.0.1:9612/metrics | grep application</pre>

<p>which should show something like:</p>

<pre
lang="bash"

>application:coherence_heap_usage_percentage_used{cluster="test-cluster", machine="docker-desktop", member="test-cluster-0", node_id="2", role="test-cluster", site="test-cluster-sts.operator-test.svc.cluster.local"} 3.09
application:coherence_heap_usage_used{cluster="test-cluster", machine="docker-desktop", member="test-cluster-0", node_id="2", role="test-cluster", site="test-cluster-sts.operator-test.svc.cluster.local"} 16177976</pre>

<p>The first metric <code>application:coherence_heap_usage_percentage_used</code> shows the heap was <code>3.09%</code> full after the last gc.
The second metric <code>application:coherence_heap_usage_used</code> shows that the in-use heap after the last gc was 16177976 bytes,
or around 16 MB.</p>

<p>The port forwarder can be changed to connect to the second <code>Pod</code> <code>test-cluster-1</code>, and the same curl command
will retrieve metrics from the second <code>Pod</code>, which should show different heap use values.</p>

</div>
</div>

<h3 id="_install_prometheus">Install Prometheus</h3>
<div class="section">
<p>The simplest way to install Prometheus as part of an example or demo is to use the
<a id="" title="" target="_blank" href="https://github.com/prometheus-operator/prometheus-operator">Prometheus Operator</a>, which can be
installed using a Helm chart.</p>


<h4 id="_setup_the_helm_repo">Setup the Helm Repo</h4>
<div class="section">
<p>Make sure the <code>stable</code> helm repository has been added to Helm if it isn&#8217;t already present in your local Helm repositories.</p>

<pre
lang="bash"

>helm repo add stable https://kubernetes-charts.storage.googleapis.com/</pre>

<p>Make sure the local Helm repository is up to date.</p>

<pre
lang="bash"

>helm repo update</pre>

</div>

<h4 id="_configure_prometheus_rbac">Configure Prometheus RBAC</h4>
<div class="section">
<p>If you are using a Kubernetes cluster with RBAC enabled then the rules required by Prometheus need to be added.
The autoscale example contains a yaml file with the required RBAC rules in it in the <code>manifests/</code> directory.</p>

<p>The <code>manifests/prometheus-rbac.yaml</code> uses a namespace <code>coherence-example</code> which may need to be changed
if you are installing into a different namespace.</p>

<p>The following commands use <code>sed</code> to replace <code>coherence-example</code> with <code>default</code> and pipe the result to <code>kubectl</code>
to create the RBAC rules in the <code>default</code> Kubernetes namespace.</p>

<pre
lang="bash"

>sed "s/coherence-example/default/g"  manifests/prometheus-rbac.yaml | kubectl create -f -</pre>

</div>

<h4 id="_install_the_prometheus_operator">Install the Prometheus Operator</h4>
<div class="section">
<p>The Prometheus Operator can now be installed using Helm. The autoscaler example contains a simple values files
that can be used when installing the chart in the <code>manifests/</code> directory.</p>

<pre
lang="bash"

>helm install --atomic --version 8.13.9 --wait \
    --set prometheus.service.type=NodePort \
    --values manifests/prometheus-values.yaml prometheus stable/prometheus-operator</pre>

<p>The <code>--wait</code> parameter makes Helm block until all the installed resources are ready.</p>

<p>The command above sets the <code>prometheus.service.type</code> value to <code>NodePort</code> so that the Prometheus UI will be exposed
on a port on the Kubernetes node. This is particularly useful when testing with a local Kubernetes cluster, such as in Docker
on a laptop because the UI can be reached on localhost at that port. The default node port is <code>30090</code>, this can be
changed by setting a different port, e.g: <code>--set prometheus.service.nodePort=9090</code>.</p>

<p>Assuming the default port of <code>30090</code> is used the UI can be reached on <a id="" title="" target="_blank" href="http://127.0.0.1:30090">http://127.0.0.1:30090</a>.</p>



<v-card>
<v-card-text class="overflow-y-hidden" >
<img src="./images/images/prometheus-ui-empty.png" alt="prometheus ui empty" />
</v-card-text>
</v-card>

<p>After Prometheus has started up and is scraping metrics we should be able to see our custom metrics in the UI.
Type the metric name <code>application:coherence_heap_usage_percentage_used</code> in the expression box and click <code>Execute</code>
and Prometheus should show two values for the metric, one for each <code>Pod</code>.</p>



<v-card>
<v-card-text class="overflow-y-hidden" >
<img src="./images/images/prometheus-ui-metrics.png" alt="prometheus ui metrics" />
</v-card-text>
</v-card>

<p>Prometheus is scraping many more Coherence metrics that can also be queried in the UI.</p>

</div>
</div>

<h3 id="_install_prometheus_adapter">Install Prometheus Adapter</h3>
<div class="section">
<p>The next step in the example is to install the Prometheus Adapter. This is a custom metrics server that published metrics
using the Kubernetes <code>custom/metrics.k8s.io</code> API. This is required because the HPA cannot query metrics directly from
Prometheus, only from standard Kubernetes metrics APIs.
As with Prometheus the simplest way to install the adapter is by using the Helm chart.
Before installing though we need to create the adapter configuration so that it can publish our custom metrics.</p>

<p>The documentation for the adapter configuration is not the simplest to understand quickly.
On top of that the adapter documentation shows how to configure the adapter using a <code>ConfigMap</code> whereas the Helm chart
adds the configuration to the Helm values file.</p>

<p>The basic format for configuring a metric in the adapter is as follows:</p>

<pre
lang="yaml"

>- seriesQuery: 'application:coherence_heap_usage_percentage_used'   <span class="conum" data-value="1" />
  resources:
    overrides:   <span class="conum" data-value="2" />
      namespace: <span class="conum" data-value="3" />
        resource: "namespace"
      pod:   <span class="conum" data-value="4" />
        resource: "pod"
      role:  <span class="conum" data-value="5" />
        group: "coherence.oracle.com"
        resource: "coherence"
  name:
    matches: ""
    as: "heap_memory_usage_after_gc_pct"  <span class="conum" data-value="6" />
  metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;)  <span class="conum" data-value="7" /></pre>

<ul class="colist">
<li data-value="1">The <code>seriesQuery</code> is the name of the metric to be retrieved from Prometheus.
This is the same name used when querying in the UI.
The name can be qualified further with tags/labels but in our case just the metric name is sufficient.</li>
<li data-value="2">The <code>overrides</code> section matches metric labels to Kubernetes resources, which can be used in queries (more about this below).</li>
<li data-value="3">The metrics have a <code>namespace</code> label (as can be seen in the UI above) and this maps to a Kubernetes <code>Namespace</code> resource.</li>
<li data-value="4">The metrics have a <code>pod</code> label (as can be seen in the UI above) and this maps to a Kubernetes <code>Pod</code> resource.</li>
<li data-value="5">The metrics have a <code>role</code> label (as can be seen in the UI above) and this maps to a Kubernetes
<code>coherence.coherence.oracle.com</code> resource.</li>
<li data-value="6">The <code>name.as</code> field gives the name of the metric in the metrics API.</li>
<li data-value="7">The <code>metricsQuery</code> determines how a specific metric will be fetched, in this case we are summing the values.</li>
</ul>
<p>The configuration above will create a metric in the <code>custom/metrics.k8s.io</code> API named heap_memory_usage_after_gc_pct.
This metric can be retrieved from the API for a namespace, for a Pod or for a Coherence deployment
(the <code>coherence.coherence.oracle.com</code> resource). This is why the <code>metricsQuery</code> uses <code>sum</code>, so that when querying for
a metric at the namespace level we see the total summed up for the namespace.</p>

<p>Summing up the metric might not be the best approach. Imagine that we want to scale when the heap after gc usage exceeds 80%.
Ideally this is when any JVM heap in use after garbage collection exceeds 80%.
Whilst Coherence will distribute data evenly across the cluster so that each member holds a similar amount of data and has
similar heap usage, there could be an occasion where one member for whatever reason is processing extra load and exceeds 80%
before other members.</p>

<p>One way to approach this issue is instead of summing the metric value for a namespace or <code>coherence.coherence.oracle.com</code>
resource we can fetch the maximum value. We do this by changing the <code>metricsQuery</code> to use <code>max</code> as shown below:</p>

<pre
lang="yaml"

>- seriesQuery: 'application:coherence_heap_usage_percentage_used'
  resources:
    overrides:
      namespace:
        resource: "namespace"
      pod:
        resource: "pod"
      role:
        group: "coherence.oracle.com"
        resource: "coherence"
  name:
    matches: ""
    as: "heap_memory_usage_after_gc_max_pct"
  metricsQuery: max(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;)</pre>

<p>This is the same configuration as previously but now the <code>metricsQuery</code> uses the <code>max</code> function, and the
metric name has been changed to <code>heap_memory_usage_after_gc_max_pct</code> so that it is obvious it is a maximum value.</p>

<p>We can repeat the configuration above for the <code>application:coherence_heap_usage_used</code> metric too so that we will end up with
four metrics in the <code>custom/metrics.k8s.io</code> API:</p>

<ul class="ulist">
<li>
<p><code>heap_memory_usage_after_gc_max_pct</code></p>

</li>
<li>
<p><code>heap_memory_usage_after_gc_pct</code></p>

</li>
<li>
<p><code>heap_memory_usage_after_gc</code></p>

</li>
<li>
<p><code>heap_memory_usage_after_gc_max</code></p>

</li>
</ul>
<p>The autoscaler example has a Prometheus Adapter Helm chart values file that contains the configuration for the
four metrics. This can be used to install the adapter
<a id="" title="" target="_blank" href="https://hub.helm.sh/charts/prometheus-com/prometheus-adapter">Helm chart</a>:</p>

<div class="admonition note">
<p class="admonition-inline">In the command below the <code>--set prometheus.url=http://prometheus-prometheus-oper-prometheus.default.svc</code>
parameter tells the adapter how to connect to Prometheus.
The Prometheus Operator creates a <code>Service</code> named <code>prometheus-prometheus-oper-prometheus</code> to expose Prometheus.
In this case the command assumes Prometheus is installed in the <code>default</code> namespace.
If you installed Prometheus into a different namespace change the <code>default</code> part of
<code>prometheus-prometheus-oper-prometheus.<strong>default</strong>.svc</code> to the actual namespace name.</p>
</div>
<div class="admonition note">
<p class="admonition-inline">The <code>manifests/prometheus-adapter-values.yaml</code> contains the configurations for metrics that the adapter
will publish. These work with Coherence Operator 3.1.0 and above. If using an earlier 3.0.x version the values
file must first be edited to change all occurrences of <code>resource: "coherence"</code> to <code>resource: "coherence"</code> (to
make the resource name singular).</p>
</div>
<pre
lang="bash"

>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install --atomic --wait \
    --set prometheus.url=http://prometheus-prometheus-oper-prometheus.default.svc \
    --values manifests/prometheus-adapter-values.yaml \
    prometheus-adapter prometheus-community/prometheus-adapter</pre>


<h4 id="_query_custom_metrics">Query Custom Metrics</h4>
<div class="section">
<p>Now the Prometheus adapter is running we can query metrics from the <code>custom/metrics.k8s.io</code> API using <code>kubectl</code> raw API access.
This is the same API that the HPA will use to obtain metrics.</p>

<p>If a Coherence cluster had been installed into the <code>default</code> namespace, then metrics could be fetched for all <code>Pods</code> in
that specific namespace, for example to obtain the <code>heap_memory_usage_after_gc_pct</code> metric:</p>

<pre
lang="bash"

>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct</pre>

<p>The <code>*</code> after <code>pods/</code> tells the adapter to fetch metrics for all <code>Pods</code> in the namespace.
To fetch the metric for pods in another namespace change the <code>default</code> part of the URL to the namespace name.</p>

<p>If you have the <code>jq</code> utility installed that formats json then piping the output to <code>jq</code> will make it prettier.</p>

<pre
lang="bash"

>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/heap_memory_usage_after_gc_pct | jq</pre>

<p>We could fetch a metric for a specific <code>Pod</code> in the <code>default</code> namespace, for example a <code>Pod</code> named <code>test-cluster-1</code> as follows:</p>

<pre
lang="bash"

>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/test-cluster-1/heap_memory_usage_after_gc_pct</pre>

<p>which might display something like:</p>

<pre
lang="json"

>{
  "kind": "MetricValueList",
  "apiVersion": "custom.metrics.k8s.io/v1beta1",
  "metadata": {
    "selfLink": "/apis/custom.metrics.k8s.io/v1beta1/namespaces/coherence-test/pods/test-cluster-1/heap_memory_usage_after_gc_pct"
  },
  "items": [
    {
      "describedObject": {
        "kind": "Pod",
        "namespace": "operator-test",
        "name": "test-cluster-1",
        "apiVersion": "/v1"
      },
      "metricName": "heap_memory_usage_after_gc_pct",
      "timestamp": "2020-09-02T12:12:01Z",
      "value": "1300m",
      "selector": null
    }
  ]
}</pre>

<div class="admonition note">
<p class="admonition-inline">The format of the <code>value</code> field above might look a little strange. This is because it is a Kubernetes <code>Quantity</code>
format, in this case it is <code>1300m</code> where the <code>m</code> stand for millis. So in this case 1300 millis is 1.3% heap usage.
This is to get around the poor support in yaml and json for accurate floating-point numbers.</p>
</div>
<p>In our case for auto-scaling we are interested in the maximum heap for a specific <code>Coherence</code> resource.
Remember in the Prometheus Adapter configuration we configured the <code>role</code> metric tag to map to
<code>coherence.coherence.oracle.com</code> resources.
We also configured a query that will give back the maximum heap usage value for a query.</p>

<p>The example yaml used to deploy the <code>Coherence</code> resource above will create a resource named <code>test-cluster</code>.
If we installed this into the <code>default</code> Kubernetes namespace then we can fetch the maximum heap use after gc
for the <code>Pods</code> in that <code>Coherence</code> deployment as follows:</p>

<pre
lang="bash"

>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct</pre>

<p>which might display something like:</p>

<pre
lang="json"

>{
  "kind": "MetricValueList",
  "apiVersion": "custom.metrics.k8s.io/v1beta1",
  "metadata": {
    "selfLink": "/apis/custom.metrics.k8s.io/v1beta1/namespaces/operator-test/coherence.coherence.oracle.com/test-cluster/heap_memory_usage_after_gc_max_pct"
  },
  "items": [
    {
      "describedObject": {
        "kind": "Coherence",
        "namespace": "operator-test",
        "name": "test-cluster",
        "apiVersion": "coherence.oracle.com/v1"
      },
      "metricName": "heap_memory_usage_after_gc_max_pct",
      "timestamp": "2020-09-02T12:21:02Z",
      "value": "3300m",
      "selector": null
    }
  ]
}</pre>

</div>
</div>

<h3 id="_configure_the_horizontal_pod_autoscaler">Configure The Horizontal Pod autoscaler</h3>
<div class="section">
<p>Now that we have custom metrics in the Kubernets <code>custom.metrics.k8s.io</code> API, the final piece is to add the HPA
configuration for the Coherence deployment that we want to scale.
To configure the HPA we need to create a <code>HorizontalPodautoscaler</code> resource for each Coherence deployment in the same namespace
as we deployed the Coherence deployment to.</p>

<p>Below is an example <code>HorizontalPodautoscaler</code> resource that will scale our example Coherence deployment:</p>

<pre
lang="yaml"
title="hpa.yaml"
>apiVersion: autoscaling/v2beta2
kind: HorizontalPodautoscaler
metadata:
  name: test-cluster-hpa
spec:
  scaleTargetRef:                         <span class="conum" data-value="1" />
    apiVersion: coherence.oracle.com/v1
    kind: Coherence
    name: test-cluster
  minReplicas: 2         <span class="conum" data-value="2" />
  maxReplicas: 5
  metrics:               <span class="conum" data-value="3" />
  - type: Object
    object:
      describedObject:
        apiVersion: coherence.oracle.com/v1
        kind: Coherence
        name: test-cluster
      metric:
        name: heap_memory_usage_after_gc_max_pct  <span class="conum" data-value="4" />
      target:
        type: Value       <span class="conum" data-value="5" />
        value: 80
  behavior:                             <span class="conum" data-value="6" />
    scaleUp:
      stabilizationWindowSeconds: 120
    scaleDown:
      stabilizationWindowSeconds: 120</pre>

<ul class="colist">
<li data-value="1">The <code>scaleTargetRef</code> points to the resource that the HPA will scale. In this case it is our <code>Coherence</code> deployment
which is named <code>test-cluster</code>. The <code>apiVersion</code> and <code>kind</code> fields match those in the <code>Coherence</code> resource.</li>
<li data-value="2">For this example, the Coherence deployment will have a minimum of 2 replicas and a maximum of 5, so the HPA will not scale up too much.</li>
<li data-value="3">The <code>metrics</code> section in the yaml above tells the HPA how to query our custom metric.
In this case we want to query the single max usage value metric for the <code>Coherence</code> deployment (like we did manually when using
kubectl above). To do this we add a metric with a <code>type</code> of <code>Object</code>.
The <code>describedObject</code> section describes the resource to query, in this case kind <code>Coherence</code> in resource group <code>coherence.oracle.com</code> with the name <code>test-cluster</code>.</li>
<li data-value="4">The metric name to query is our custom max heap usage percentage metric <code>heap_memory_usage_after_gc_max_pct</code>.</li>
<li data-value="5">The <code>target</code> section describes the target value for the metric, in this case 80 thousand millis - which is 80%.</li>
<li data-value="6">The <code>behavior</code> section sets a window of 120 seconds so that the HAP will wait at least 120 seconds after scaling up
or down before re-evaluating the metric. This gives Coherence enough time to scale the deployment and for the data to redistribute
and gc to occur. In real life this value would need to be adjusted to work correctly on your actual cluster.</li>
</ul>
<p>The autoscaler example contains yaml to create the <code>HorizontalPodautoscaler</code> resource in the <code>manifests/</code> directory.</p>

<div class="admonition warning">
<p class="admonition-inline">If using a version of Kubernetes prior to 1.18 the <code>behaviour</code> secion of the yaml above is invalid and should be
removed. This could cause the HPA not to work the way we want it to as there will be no cool-down period specified
between scaling operations, and the HPA could thrash or suddenly scale up or down by a lot of Pods.
The only way to set these values prior to 1.18 was for the HPA as a whole
(see the documentation <a id="" title="" target="_blank" href="https://v1-17.docs.kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-cooldown-delay">support for cooldown delay</a>).</p>
</div>
<pre
lang="bash"

>kubectl create -f manifests/hpa.yaml</pre>

<p>The <code>hpa.yaml</code> file will create a <code>HorizontalPodautoscaler</code> resource named <code>test-cluster-hpa</code>.
After waiting a minute or two for the HPA to get around to polling our new <code>HorizontalPodautoscaler</code> resource
we can check its status.</p>

<pre
lang="bash"

>kubectl describe horizontalpodautoscaler.autoscaling/test-cluster-hpa</pre>

<p>Which should show something like:</p>

<pre
lang="bash"

>Name:                                                                             test-cluster-hpa
Namespace:                                                                        operator-test
Labels:                                                                           &lt;none&gt;
Annotations:                                                                      &lt;none&gt;
CreationTimestamp:                                                                Wed, 02 Sep 2020 15:58:26 +0300
Reference:                                                                        Coherence/test-cluster
Metrics:                                                                          ( current / target )
  "heap_memory_usage_after_gc_max_pct" on Coherence/test-cluster (target value):  3300m / 80
Min replicas:                                                                     2
Max replicas:                                                                     10
Coherence pods:                                                                   2 current / 2 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from Coherence metric heap_memory_usage_after_gc_max_pct
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:           &lt;none&gt;</pre>

<p>We can see that the HPA has successfully polled the metric and obtained a value of <code>3300m</code> (so 3.3%) and has
decided that it does not need to scale.</p>

</div>

<h3 id="_add_data_scale_up">Add Data - Scale Up!</h3>
<div class="section">
<p>The HPA is now monitoring our Coherence deployment so we can now add data to the cluster and see the HPA scale up when
heap use grows.
The autoscaler example Maven pom file has been configured to use the Maven exec plugin to execute a Coherence command line
client that will connect over Coherence Extend to the demo cluster that we have deployed.</p>

<p>First we need to create a port forwarder to expose the Coherence Extend port locally.
Extend is bound to port 20000 in the <code>Pods</code> in our example.</p>

<pre
lang="bash"

>kubectl port-forward pod/test-cluster-0 20000:20000</pre>

<p>The command above forwards port 20000 in the <code>Pod</code> <code>test-cluster-0</code> to the local port 20000.</p>

<p>To start the client, run the following command in a terminal:</p>

<pre
lang="bash"

>./mvnw exec:java -pl autoscaler/</pre>

<p>The command above will start the console client and eventually display a <code>Map (?):</code> prompt.</p>

<p>At the map prompt, first create a cache named <code>test</code> with the <code>cache</code> command, type <code>cache test</code> and hit enter:</p>

<pre
lang="bash"

>Map (?): cache test</pre>

<p>There will now be a cache created in the cluster named <code>test</code>, and the map prompt will change to <code>Map (test):</code>.
We can add random data to this with the <code>bulkput</code> command. The format of the <code>bulkput</code> command is:</p>

<pre
lang="bash"

>bulkput &lt;# of iterations&gt; &lt;block size&gt; &lt;start key&gt; [&lt;batch size&gt; | all]</pre>

<p>So to add 20,000 entries of 10k bytes each starting at key <code>1</code> adding in batches of 1000 we can run
the <code>bulkput 20000 10000 1 1000</code> command at the map prompt:</p>

<pre
lang="bash"

>Map (test): bulkput 20000 10000 1 1000</pre>

<p>We can now look at the <code>HorizontalPodautoscaler</code> resource we create earlier with the command:</p>

<pre
lang="bash"

>kubectl get horizontalpodautoscaler.autoscaling/test-cluster-hpa</pre>

<p>Which will display something like:</p>

<pre
lang="bash"

>NAME               REFERENCE                TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
test-cluster-hpa   Coherence/test-cluster   43700m/80   2         10        2          41m</pre>

<p>The HPA is now saying that the value of our heap use metric is 43.7%, so we can add a bit more data.
It may take a minute or two for the heap to increase and stabilise as different garbage collections happen across the Pods.
We should be able to safely add another 20000 entries putting the heap above 80% and hopefully scaling our deployment.</p>

<p>We need to change the third parameter to bulk put to 20000 otherwise the put will start again at key <code>1</code> and just overwrite the
previous entries, not really adding to the heap.</p>

<pre
lang="bash"

>Map (test): bulkput 20000 10000 20000 1000</pre>

<p>Now run the <code>kubectl describe</code> command on the <code>HorizontalPodautoscaler</code> resource again, and we should see that it has scaled
our cluster. If another 20,000 entries does not cause the heap to exceed 80% then you may need to run the <code>bulkput</code> command
once or twice more with a smaller number of entries to push the heap over 80%.</p>

<div class="admonition note">
<p class="admonition-inline">As previously mentioned, everything with HPA is slightly delayed due to the different components polling, and
stabilization times. It could take a few minutes for the HPA to actually scale the cluster.</p>
</div>
<pre
lang="bash"

>kubectl describe horizontalpodautoscaler.autoscaling/test-cluster-hpa</pre>

<p>The output of the <code>kubectl describe</code> command should now be something like this:</p>

<pre
lang="bash"

>Name:                                                                             test-cluster-hpa
Namespace:                                                                        operator-test
Labels:                                                                           &lt;none&gt;
Annotations:                                                                      &lt;none&gt;
CreationTimestamp:                                                                Wed, 02 Sep 2020 15:58:26 +0300
Reference:                                                                        Coherence/test-cluster
Metrics:                                                                          ( current / target )
  "heap_memory_usage_after_gc_max_pct" on Coherence/test-cluster (target value):  88300m / 80
Min replicas:                                                                     2
Max replicas:                                                                     10
Coherence pods:                                                                   2 current / 3 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 3
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from Coherence metric heap_memory_usage_after_gc_max_pct
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  1s    horizontal-pod-autoscaler  New size: 3; reason: Coherence metric heap_memory_usage_after_gc_max_pct above target</pre>

<p>We can see that the heap use value is now <code>88300m</code> or 88.3% and the events section shows that the HPA has scaled the <code>Coherence</code>
deployment to <code>3</code>. We can list the <code>Pods</code> and there should be three:</p>

<pre
lang="bash"

>kubectl get pod -l coherenceCluster=test-cluster</pre>

<pre
lang="bash"

>NAME             READY   STATUS    RESTARTS   AGE
test-cluster-0   1/1     Running   0          3h14m
test-cluster-1   1/1     Running   0          3h14m
test-cluster-2   1/1     Running   0          1m10s</pre>

<div class="admonition note">
<p class="admonition-inline">At this point Coherence will redistribute data to balance it over the three members of the cluster.
It may be that it takes considerable time for this to affect the heap usage as a lot of the cache data will be in the old generation of
the heap and not be immediately collected. This may then trigger another scale after the 120 second stabilization period that
we configured in the <code>HorizontalPodautoscaler</code>.</p>
</div>
</div>

<h3 id="_clean_up">Clean-Up</h3>
<div class="section">
<p>To clean-up after running the example just uninstall everything in the reverse order:</p>

<pre
lang="bash"

>kubectl delete -f manifests/hpa.yaml
helm delete prometheus-adapter
helm delete prometheus
kubectl delete -f manifests/cluster.yaml</pre>

<p>Remove the Prometheus RBAC rules, remembering to change the namespace name.</p>

<pre
lang="bash"

>sed "s/coherence-example/default/g"  manifests/prometheus-rbac.yaml | kubectl delete -f -</pre>

<p>Delete the Coherence deployment.</p>

<pre
lang="bash"

>kubectl delete manifests/cluster.yaml</pre>

<p>Undeploy the Operator.
TBD&#8230;&#8203;</p>

</div>
</div>

<h2 id="_conclusions">Conclusions</h2>
<div class="section">
<p>As we&#8217;ve shown, it is possible to use the HPA to scale a Coherence cluster based on metrics published by Coherence or
custom metrics, but there are some obvious caveats due to how HPA works.
There are inherent delays in the scaling process, the HPA only polls metrics periodically,
which themselves have been polled by Prometheus periodically and hence there can be some delay after
reaching a given heap size before the scale command actually reaches the Coherence Operator.
This will be obvious when running the example below.
Given a suitable configuration the HPA can be useful to scale as load increases but in no way can it
guarantee that an out of memory exception will never happen.</p>

<p>Using the HPA to scale as Coherence Pod&#8217;s heaps become filled is in no way an excuse not to do proper capacity planning
and size your Coherence clusters appropriately.</p>

</div>
</doc-view>
<!-- pages/examples/800_istio.js -->
<doc-view>

<h2 id="_istio_support">Istio Support</h2>
<div class="section">
<p>You can run the Coherence cluster and manage then using the Coherence Operator alongside Istio. Coherence clusters managed with the Coherence Operator 3.2.0 and later work with Istio 1.9.1 and later. Coherence caches can be accessed from outside the Coherence cluster via Coherence*Extend, REST, and other supported Coherence clients. Using Coherence clusters with Istio does not require the Coherence Operator to also be using Istio (and vice-versa) . The Coherence Operator can manage Coherence clusters independent of whether those clusters are using Istio or not.</p>

<div class="admonition important">
<p class="admonition-textlabel">Important</p>
<p ><p>The current support for Istio has the following limitation:</p>

<p>Ports that are exposed in the ports list of the container spec in a Pod will be intercepted by the Envoy proxy in the Istio side-car container. Coherence cluster traffic must not pass through Envoy proxies as this will break Coherence, so the Coherence cluster port must never be exposed as a container port if using Istio. There is no real reason to expose the Coherence cluster port in a container because there is no requirement to have this port externally visible.</p>
</p>
</div>

<h3 id="_prerequisites">Prerequisites</h3>
<div class="section">
<p>The instructions assume that you are using a Kubernetes cluster with Istio installed and configured already.</p>

</div>

<h3 id="_using_the_coherence_operator_with_istio">Using the Coherence operator with Istio</h3>
<div class="section">
<p>To use Coherence operator with Istio, you can deploy the operator into a namespace which has Istio automatic sidecar injection enabled.  Before installing the operator, create the namespace in which you want to run the Coherence operator and label it for automatic injection.</p>

<pre
lang="bash"

>kubectl create namespace coherence
kubectl label namespace coherence istio-injection=enabled</pre>

<p>Istio Sidecar AutoInjection is done automatically when you label the coherence namespace with istio-injection.</p>

<p>After the namespace is labeled, you can install the operator using your preferred method in the Operator <a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/docs/latest/#/installation/01_installation">Installation Guide</a>.</p>

<p>After installed operator, use the following command to confirm the operator is running:</p>

<pre
lang="bash"

>kubectl get pods -n coherence

NAME                                                     READY   STATUS    RESTARTS   AGE
coherence-operator-controller-manager-7d76f9f475-q2vwv   2/2     Running   1          17h</pre>

<p>2/2 in READY column means that there are 2 containers running in the operator Pod. One is Coherence operator and the other is Envoy Proxy.</p>

</div>

<h3 id="_creating_a_coherence_cluster_with_istio">Creating a Coherence cluster with Istio</h3>
<div class="section">
<p>You can configure your cluster to run with Istio automatic sidecar injection enabled. Before creating your cluster, create the namespace in which you want to run the cluster and label it for automatic injection.</p>

<pre
lang="bash"

>kubectl create namespace coherence-example
kubectl label namespace coherence-example istio-injection=enabled</pre>

<p>There is no other requirements to run Coherence in Istio environment.</p>

<p>The following is an example that creates a cluster named example-cluster-storage:</p>

<p>example.yaml</p>

<pre
lang="bash"

># Example
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: example-cluster-storage</pre>

<pre
lang="bash"

>$ kubectl -n coherence-example apply -f example.yaml</pre>

<p>After you installed the Coherence cluster, run the following command to view the pods:</p>

<pre
lang="bash"

>$ kubectl -n coherence-example get pods

NAME                                             READY   STATUS    RESTARTS   AGE
example-cluster-storage-0                        2/2     Running   0          45m
example-cluster-storage-1                        2/2     Running   0          45m
example-cluster-storage-2                        2/2     Running   0          45m</pre>

<p>You can see that 3 members in the cluster are running with 3 pods. 2/2 in READY column means that there are 2 containers running in each Pod. One is Coherence member and the other is Envoy Proxy.</p>

</div>

<h3 id="_tls">TLS</h3>
<div class="section">
<p>Coherence cluster works with mTLS. Coherence client can also support TLS through Istio Gateway with TLS termination to connect to Coherence cluster running inside kubernetes.  For example, you can apply the following Istio Gateway and Virtual Service in the namespace of the Coherence cluster.  Before applying the gateway, create a secret for the credential from the certiticate and key (e.g. server.crt and server.key) to be used by the Gateway:</p>

<pre
lang="bash"

>kubectl create -n istio-system secret tls extend-credential --key=server.key --cert=server.crt</pre>

<p>Then, create a keystore (server.jks) to be used by the Coherence Extend client, e.g.:</p>

<pre
lang="bash"

>openssl pkcs12 -export -in server.crt -inkey server.key -chain -CAfile ca.crt -name "server" -out server.p12

keytool -importkeystore -deststorepass password -destkeystore server.jks -srckeystore server.p12 -srcstoretype PKCS12</pre>

<p>tlsGateway.yaml</p>

<pre
lang="bash"

>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: tlsgateway
spec:
  selector:
    istio: ingressgateway # use istio default ingress gateway
  servers:
  - port:
      number: 8043
      name: tls
      protocol: TLS
    tls:
      mode: SIMPLE
      credentialName: "extend-credential" # the secret created in the previous step
      maxProtocolVersion: TLSV1_3
    hosts:
    - "*"</pre>

<p>tlsVS.yaml</p>

<pre
lang="bash"

>apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: extend
spec:
  hosts:
  - "*"
  gateways:
  - tlsgateway
  tcp:
  - match:
    route:
    - destination:
        host: example-cluster-proxy-proxy  # the service name used to expose the Extend proxy port</pre>

<p>Apply the Gateway and VirtualService:</p>

<pre
lang="bash"

>kubectl apply -f tlsGateway.yaml -n coherence-example
kubectl apply -f tlsVS.yaml -n coherence-example</pre>

<p>Then configure a Coherence*Extend client to connect to the proxy server via TLS protocol.  Below is an example of a &lt;remoce-cache-scheme&gt; configuration of an Extend client using TLS port 8043 configured in the Gateway and server.jks created earlier in the example.</p>

<p>client-cache-config.xml</p>

<div class="listing">
<pre>...
    &lt;remote-cache-scheme&gt;
        &lt;scheme-name&gt;extend-direct&lt;/scheme-name&gt;
        &lt;service-name&gt;ExtendTcpProxyService&lt;/service-name&gt;
        &lt;initiator-config&gt;
            &lt;tcp-initiator&gt;
                &lt;socket-provider&gt;
                    &lt;ssl&gt;
                        &lt;protocol&gt;TLS&lt;/protocol&gt;
                        &lt;trust-manager&gt;
                            &lt;algorithm&gt;PeerX509&lt;/algorithm&gt;
                            &lt;key-store&gt;
                                &lt;url&gt;file:server.jks&lt;/url&gt;
                                &lt;password&gt;password&lt;/password&gt;
                            &lt;/key-store&gt;
                        &lt;/trust-manager&gt;
                    &lt;/ssl&gt;
                &lt;/socket-provider&gt;
                &lt;remote-addresses&gt;
                    &lt;socket-address&gt;
                        &lt;address&gt;$INGRESS_HOST&lt;/address&gt;
                        &lt;port&gt;8043&lt;/port&gt;
                    &lt;/socket-address&gt;
                &lt;/remote-addresses&gt;
            &lt;/tcp-initiator&gt;
        &lt;/initiator-config&gt;
    &lt;/remote-cache-scheme&gt;
...</pre>
</div>

<p>If you are using Docker for Desktop, $INGRESS_HOST is 127.0.0.1 and you can use the Kubectl port-forward to allow the Extend client to access the Coherence cluster from your localhost:</p>

<pre
lang="bash"

>kubectl port-forward -n istio-system &lt;istio-ingressgateway-pod&gt; 8043:8043</pre>

</div>

<h3 id="_prometheus">Prometheus</h3>
<div class="section">
<p>The coherence metrics that record and track the health of Coherence cluster using Prometheus are also available in Istio environment and can be viewed through Granfana. However, Coherence cluster traffic is not visible by Istio.</p>

</div>

<h3 id="_traffic_visualization">Traffic Visualization</h3>
<div class="section">
<p>Istio provides traffic management capabilities, including the ability to visualize traffic in Kiali. You do not need to change your applications to use this feature. The Istio proxy (envoy) sidecar that is injected into your pods provides it. The image below shows an example with traffic flow. In this example, you can see how the traffic flows in from the Istio gateway on the left, to the cluster services, and then to the individual cluster members.  This example has storage members (example-cluster-storage), a proxy member running proxy service (example-cluster-proxy), and a REST member running http server (example-cluster-rest).  However, Coherence cluster traffic between members is not visible.</p>



<v-card>
<v-card-text class="overflow-y-hidden" >
<img src="./images/istioKiali.png" alt="istioKiali"width="1024" />
</v-card-text>
</v-card>

<p>To learn more, see <a id="" title="" target="_blank" href="https://istio.io/latest/docs/concepts/traffic-management/">Istio traffic management</a>.</p>

</div>
</div>
</doc-view>
<!-- pages/examples/900_demo.js -->
<doc-view>
<p>The Coherence Demonstration application is an application which demonstrates various Coherence
related features such include Persistence, Federation and Lambda support.  This demonstration
can run stand alone but can also be installed on the Coherence Operator.</p>

<p>When installed using the Coherence Operator, the setup includes two Coherence Clusters, in the same Kubernetes cluster,
which are configured with Active/Active Federation.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/coherence-demo.png" alt="Service Details"width="950" />
</v-card-text>
</v-card>



<v-card>
<v-card-text class="overflow-y-hidden" >
<img src="./images/GitHub-Mark-64px.png" alt="GitHub"width="32" />
</v-card-text>
</v-card>

<p>Please see <a id="" title="" target="_blank" href="https://github.com/coherence-community/coherence-demo">The Coherence Demo GitHub project</a> for full instructions.</p>

</doc-view>
<!-- pages/installation/01_installation.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Coherence Operator Installation</dt>
<dd slot="desc"><p>The Coherence Operator is available as a Docker image <code>oracle/coherence-operator:3.2.1</code> that can
easily be installed into a Kubernetes cluster.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_coherence_operator_installation">Coherence Operator Installation</h2>
<div class="section">

<h3 id="_prerequisites">Prerequisites</h3>
<div class="section">
<p>The prerequisites apply to all installation methods.</p>

<ul class="ulist">
<li>
<p>Access to Oracle Coherence Operator images.</p>

</li>
<li>
<p>Access to a Kubernetes v1.16.0+ cluster. The Operator test pipeline is run using Kubernetes versions v1.16 upto v1.21</p>

</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">ARM Support: As of version 3.2.0, the Coherence Operator is build as a multi-architecture image that supports running in Kubernetes on both Linux/amd64 and Linux/arm64. The prerequisite is that the Coherence application image used has been built to support ARM.</p>
</div>
<p>There are a few ways to install the Coherence Operator documented below:</p>

<ul class="ulist">
<li>
<p><router-link to="#manifest" @click.native="this.scrollFix('#manifest')">Simple installation using Kubectl</router-link></p>

</li>
<li>
<p><router-link to="#helm" @click.native="this.scrollFix('#helm')">Install the Helm chart</router-link></p>

</li>
<li>
<p><router-link to="#kubectl" @click.native="this.scrollFix('#kubectl')">Kubectl with Kustomize</router-link></p>

</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">Installing the Coherence Operator using the methods above will create a number of <code>ClusterRole</code> RBAC resources.
Some corporate security policies do not like to give cluster wide roles to third-party products.
To help in this situation the operator can be installed without cluster roles, but with caveats
(see the <router-link to="/installation/09_RBAC">RBAC</router-link> documentation) for more details.</p>
</div>
<div class="admonition note">
<p class="admonition-inline">OpenShift - the Coherence Operator works without modification on OpenShift, but some versions
of the Coherence images will not work out of the box.
See the <router-link to="/installation/06_openshift">OpensShift</router-link> section of the documentation that explains how to
run Coherence clusters with the Operator on OpenShift.</p>
</div>
<div class="admonition note">
<p class="admonition-inline">Whilst Coherence works out of the box on many Kubernetes installations, some Kubernetes
installations may configure iptables in a way that causes Coherence to fail to create clusters.
See the <router-link to="/installation/08_networking">O/S Network Configuration</router-link> section of the documentation
for more details if you have well-known-address issues when Pods attempt to form a cluster.</p>
</div>
<div class="admonition warning">
<p class="admonition-textlabel">Warning</p>
<p ><p><strong>Upgrading from version 3.1.0</strong></p>

<p>Due to a CRD incompatibility that was initially un-noticed in v3.1.0 we deprecated v3.1.0 and recommended that
it not be used.
If you did install v3.1.0 and are upgrading to the latest version you must manually uninstall the
<code>coherences.coherence.oracle.com</code> CRD from your Kubernetes cluster before installing the new Operator version.</p>

<pre
lang="bash"

>kubectl delete crd coherences.coherence.oracle.com</pre>

<p>Note that the CRD name in the 3.1.0 CRD being uninstalled has a plural <code>coherences</code> for the first part of the name.</p>
</p>
</div>
</div>
</div>

<h2 id="manifest">Default Install with Kubectl</h2>
<div class="section">
<p>If you want the default Coherence Operator installation then the simplest solution is use <code>kubectl</code> to apply the manifests from the Operator release.</p>

<pre
lang="bash"

>kubectl apply -f https://github.com/oracle/coherence-operator/releases/download/v3.2.2/coherence-operator.yaml</pre>

<p>This will create a namespace called <code>coherence</code> and install the Operator into it along with all the required <code>ClusterRole</code> and <code>RoleBinding</code> resources. The <code>coherence</code> namespace can be changed by downloading and editing the yaml file.</p>

<div class="admonition note">
<p class="admonition-inline">Because the <code>coherence-operator.yaml</code> manifest also creates the namespace, the corresponding <code>kubectl delete</code> command will <em>remove the namespace and everything deployed to it</em>! If you do not want this behaviour you should edit the <code>coherence-operator.yaml</code> to remove the namespace section from the start of the file.</p>
</div>
<p>Instead of using a hard coded version in the command above you can find the latest Operator version using <code>curl</code>:</p>

<pre
lang="bash"

>export VERSION=$(curl -s \
  https://api.github.com/repos/oracle/coherence-operator/releases/latest \
  | grep '"name": "v' \
  | cut -d '"' -f 4 \
  | cut -b 2-10)</pre>

<p>Then download with:</p>

<pre
lang="bash"

>kubectl apply -f https://github.com/oracle/coherence-operator/releases/download/${VERSION}/coherence-operator.yaml</pre>

</div>

<h2 id="_installing_with_helm">Installing With Helm</h2>
<div class="section">
<p>For more flexibility but the simplest way to install the Coherence Operator is to use the Helm chart.
This ensures that all the correct resources will be created in Kubernetes.</p>


<h3 id="helm">Add the Coherence Helm Repository</h3>
<div class="section">
<p>Add the <code>coherence</code> helm repository using the following commands:</p>

<pre
lang="bash"

>helm repo add coherence https://oracle.github.io/coherence-operator/charts

helm repo update</pre>

<div class="admonition note">
<p class="admonition-inline">To avoid confusion, the URL <code><a id="" title="" target="_blank" href="https://oracle.github.io/coherence-operator/charts">https://oracle.github.io/coherence-operator/charts</a></code> is a Helm repo, it is not a web site you open in a browser. You may think we shouldn&#8217;t have to say this, but you&#8217;d be surprised.</p>
</div>
</div>

<h3 id="_install_the_coherence_operator_helm_chart">Install the Coherence Operator Helm chart</h3>
<div class="section">
<p>Once the Coherence Helm repo has been configured the Coherence Operator can be installed using a normal Helm 3
install command:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \      <span class="conum" data-value="1" />
    coherence \                    <span class="conum" data-value="2" />
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">where <code>&lt;namespace&gt;</code> is the namespace that the Coherence Operator will be installed into.</li>
<li data-value="2"><code>coherence</code> is the name of this Helm installation.</li>
</ul>

<h4 id="_uninstall_the_coherence_operator_helm_chart">Uninstall the Coherence Operator Helm chart</h4>
<div class="section">
<p>To uninstall the operator:</p>

<pre
lang="bash"

>helm delete coherence-operator --namespace &lt;namespace&gt;</pre>

</div>
</div>
</div>

<h2 id="_operator_scope">Operator Scope</h2>
<div class="section">
<p>The recommended way to install the Coherence Operator is to install a single instance of the operator into a namespace
and where it will then control <code>Coherence</code> resources in all namespaces across the Kubernetes cluster.
Alternatively it may be configured to watch a sub-set of namespaces by setting the <code>WATCH_NAMESPACE</code> environment variable.
The watch namespace(s) does not have to include the installation namespace.</p>

<p>In theory, it is possible to install multiple instances of the Coherence Operator into different namespaces, where
each instances monitors a different set of namespaces. There are a number of potential issues with this approach, so
it is not recommended.</p>

<ul class="ulist">
<li>
<p>Only one CRD can be installed - Different releases of the Operator may use slightly different CRD versions, for example
a new version may introduce extra fields not in the previous version. As the CRD version is <code>v1</code> there is no guarantee
which CRD version has actually installed, which could lead to subtle issues.</p>

</li>
<li>
<p>The operator creates and installs defaulting and validating web-hooks. A web-hook is associated to a CRD resource so
installing multiple web-hooks for the same resource may lead to issues. If an operator is uninstalled, but the web-hook
configuration remains, then Kubernetes will not accept modifications to resources of that type as it will be
unable to contact the web-hook.</p>

</li>
</ul>
<p>To set the watch namespaces when installing with helm set the <code>watchNamespaces</code> value, for example:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    --set watchNamespaces=payments,catalog,customers <span class="conum" data-value="1" />
    coherence-operator \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">The <code>payments</code>, <code>catalog</code> and <code>customers</code> namespaces will be watched by the Operator.</li>
</ul>
</div>

<h2 id="_operator_image">Operator Image</h2>
<div class="section">
<p>The Helm chart uses a default registry to pull the Operator image from.
If the image needs to be pulled from a different location (for example an internal registry) then the <code>image</code> field
in the values file can be set, for example:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    --set image=images.com/coherence-operator:0.1.2 <span class="conum" data-value="1" />
    coherence-operator \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">The image used to run the Operator will be <code>images.com/coherence-operator:0.1.2</code>.</li>
</ul>

<h3 id="_image_pull_secrets">Image Pull Secrets</h3>
<div class="section">
<p>If the image is to be pulled from a secure repository that requires credentials then the image pull secrets
can be specified.
See the Kubernetes documentation on <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">Pulling from a Private Registry</a>.</p>


<h4 id="_add_pull_secrets_using_a_values_file">Add Pull Secrets Using a Values File</h4>
<div class="section">
<p>Create a values file that specifies the secrets, for example the <code>private-repo-values.yaml</code> file below:</p>

<pre
lang="yaml"
title="private-repo-values.yaml"
>imagePullSecrets:
- name: registry-secrets</pre>

<p>Now use that file in the Helm install command:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    -f private-repo-values.yaml <span class="conum" data-value="1" />
    coherence-operator \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">the <code>private-repo-values.yaml</code> values fle will be used by Helm to inject the settings into the Operator deployment</li>
</ul>
</div>

<h4 id="_add_pull_secrets_using_set">Add Pull Secrets Using --Set</h4>
<div class="section">
<p>Although the <code>imagePullSecrets</code> field in the values file is an array of <code>name</code> to value pairs it is possible to set
these values with the normal Helm <code>--set</code> parameter.</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    --set imagePullSecrets[0].name=registry-secrets <span class="conum" data-value="1" />
    coherence-operator \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">this creates the same imagePullSecrets as the values file above.</li>
</ul>
</div>
</div>
</div>

<h2 id="kubectl">Install with Kubectl and Kustomize</h2>
<div class="section">
<p>If you want to use yaml directly to install the operator, with something like <code>kubectl</code>, you can use the manifest files
published with the GitHub release at this link:
<a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/releases/download/v3.2.2/coherence-operator-manifests.tar.gz">3.2.2 Manifests</a></p>

<p>These manifest files are for use with a tool called Kustomize, which is built into <code>kubectl</code>
see the documentation here: <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/</a></p>

<p>Download the
<a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/releases/download/v3.2.2/coherence-operator-manifests.tar.gz">3.2.2 Manifests</a>
from the release page and unpack the file, which should produce a directory called <code>manifests</code> with a structure like this:</p>

<pre


>manifests
    default
        config.yaml
        kustomization.yaml
    manager
        kustomization.yaml
        manager.yaml
        service.yaml
    rbac
        coherence_editor_role.yaml
        coherence_viewer_role.yaml
        kustomization.yaml
        leader_election_role.yaml
        leader_election_role_binding.yaml
        role.yaml
        role_binding.yaml</pre>

<p>There are two ways to use these manifest files, either install using <code>kustomize</code> or generate the yaml and manually
install with <code>kubectl</code>.</p>

<div class="admonition note">
<p class="admonition-inline">All the commands below are run from a console in the <code>manifests/</code> directory from the extracted file above.</p>
</div>

<h3 id="_install_with_kustomize">Install with Kustomize</h3>
<div class="section">
<p>If you have Kustomize installed (or can install it from <a id="" title="" target="_blank" href="https://github.com/kubernetes-sigs/kustomize">https://github.com/kubernetes-sigs/kustomize</a>) you can use
Kustomize to configure the yaml and install.</p>


<h4 id="_set_image_names">Set Image Names</h4>
<div class="section">
<p>If you need to use different iamge names from the defaults <code>kustomize</code> can be used to specify different names:</p>

<p>Change the name of the Operator image by running the command below, changing the image name to the registry and image name
that you are using for the Operator</p>

<pre
lang="bash"

>cd ./manager &amp;&amp; kustomize edit set image controller=container-registry.oracle.com/middleware/coherence-operator:3.2.2</pre>

<p>Change the name of the Operator utilities image by running the command below, changing the image name to the registry and image name
that you are using for the Operator utilities image</p>

<pre
lang="bash"

>cd ./manager &amp;&amp; kustomize edit add configmap env-vars --from-literal UTILS_IMAGE=container-registry.oracle.com/middleware/coherence-operator:3.2.2-utils</pre>

<p>Change the name of the default Coherence image. If you are always going to be deploying your own application images then this
does not need to change.</p>

<pre
lang="bash"

>cd ./manager &amp;&amp; $(GOBIN)/kustomize edit add configmap env-vars --from-literal COHERENCE_IMAGE=$(COHERENCE_IMAGE)</pre>

<p>Set the namespace to install into, the example below sets the namespace to <code>coherence-test</code>:</p>

<pre
lang="bash"

>cd ./default &amp;&amp; /kustomize edit set namespace coherence-test</pre>

</div>

<h4 id="_install">Install</h4>
<div class="section">
<p>The Operator requires a <code>Secret</code> for its web-hook certificates. This <code>Secret</code> needs to exist but can be empty.
The <code>Secret</code> must be in the same namespace that the Operator will be deployed to.
For example, if the Operator namespace is <code>coherence-test</code>, then the <code>Secret</code> can be created with this command:</p>

<pre
lang="bash"

>kubectl -n coherence-test create secret generic coherence-webhook-server-cert</pre>

<p>The Operator can now be installed by running the following command from the <code>manifests</code> directory:</p>

<pre
lang="bash"

>kustomize build ./default | kubectl apply -f -</pre>

</div>
</div>

<h3 id="_generate_yaml_install_with_kubectl">Generate Yaml - Install with Kubectl</h3>
<div class="section">
<p>Instead of using Kustomize to modify and install the Operator we can use <code>kubectl</code> to generate the yaml from the manifests.
You can then edit this yaml and manually deploy it with <code>kubectl</code>.</p>

<p>Run the following command from the <code>manifests</code> directory:</p>

<pre
lang="bash"

>kubectl create --dry-run -k default/ -o yaml &gt; operator.yaml</pre>

<p>This will create a file in the <code>manifests</code> directory called <code>operator.yaml</code> that contains all the yaml required
to install the Operator. You can then edit this yaml to change image names or add other settings.</p>

<p>The Operator can be installed using the generated yaml.</p>

<p>For example if the Operator is to be deployed to the <code>coherence-test</code> namespace:</p>

<pre
lang="bash"

>kubectl -n coherence-test create secret generic coherence-webhook-server-cert
kubectl -n coherence-test create -f operator.yaml</pre>

</div>
</div>
</doc-view>
<!-- pages/installation/02_pre_release_versions.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Accessing Pre-Release Versions</dt>
<dd slot="desc"><p>Pre-release version of the Coherence Operator are made available from time to time.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_accessing_pre_release_versions">Accessing Pre-Release Versions</h2>
<div class="section">
<div class="admonition warning">
<p class="admonition-inline">We cannot guarantee that pre-release versions of the Coherence Operator are bug free and hence they should
not be used in production.
We reserve the right to remove pre-release versions of the Helm chart and Docker images ant any time and without notice.
We cannot guarantee that APIs and CRD specifications will remain stable or backwards compatible between pre-release versions.</p>
</div>
<p>To access pre-release versions of the Helm chart add the unstable chart repository.</p>

<pre
lang="bash"

>helm repo add coherence-unstable https://oracle.github.io/coherence-operator/charts-unstable

helm repo update</pre>

<p>To list all the available Coherence Operator chart versions:</p>

<pre
lang="bash"

>helm search coherence-operator -l</pre>

<p>The <code>-l</code> parameter shows all versions as opposed to just the latest versions if it was omitted.</p>

<p>A specific pre-release version of the Helm chart can be installed using the <code>--version</code> argument,
for example to use version <code>3.0.0-2005140315</code>:</p>

<pre
lang="bash"

>helm install coherence-unstable/coherence-operator \
    --version 3.0.0-2005140315 \   <span class="conum" data-value="1" />
    --namespace &lt;namespace&gt; \      <span class="conum" data-value="2" />
    --name coherence-operator</pre>

<ul class="colist">
<li data-value="1">The <code>--version</code> argument is used to specify the exact version of the chart</li>
<li data-value="2">The optional <code>--namespace</code> parameter to specify which namespace to install the operator into, if omitted then
Helm will install into whichever is currently the default namespace for your Kubernetes configuration.</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">When using pre-release versions of the Helm chart it is always advisable to install a specific version otherwise
Helm will try to work out the latest version in the pre-release repo and as pre-release version numbers are not strictly
sem-ver compliant this may be unreliable.</p>
</div>
</div>
</doc-view>
<!-- pages/installation/04_obtain_coherence_images.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Obtain Coherence Images</dt>
<dd slot="desc"><p>For most use-cases we expect the developer to provide a suitable Coherence application image to be
run by the operator. For POCs, demos and experimentation the Coherence Operator uses the OSS Coherence CE image as
the image to run when no image has been specified for a <code>Coherence</code> resource.
Commercial Coherence images are not available from public image registries and must be pulled from the
middleware section of <a id="" title="" target="_blank" href="https://container-registry.oracle.com">Oracle Container Registry.</a></p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_coherence_images_from_oracle_container_registry">Coherence Images from Oracle Container Registry</h2>
<div class="section">
<p>Get the Coherence Docker image from the Oracle Container Registry:</p>

<ul class="ulist">
<li>
<p>In a web browser, navigate to <a id="" title="" target="_blank" href="https://container-registry.oracle.com/">Oracle Container Registry</a> and click Sign In.</p>

</li>
<li>
<p>Enter your Oracle credentials or create an account if you don&#8217;t have one.</p>

</li>
<li>
<p>Search for coherence in the Search Oracle Container Registry field.</p>

</li>
<li>
<p>Click coherence in the search result list.</p>

</li>
<li>
<p>On the Oracle Coherence page, select the language from the drop-down list and click Continue.</p>

</li>
<li>
<p>Click Accept on the Oracle Standard Terms and Conditions page.</p>

</li>
</ul>
<p>Once this is done the Oracle Container Registry credentials can be used to create Kubernetes secret to pull the
Coherence image.</p>

</div>

<h2 id="_use_imagepullsecrets">Use ImagePullSecrets</h2>
<div class="section">
<p>Kubernetes supports configuring pods to use <code>imagePullSecrets</code> for pulling images. If possible, this is the preferable
and most portable route.
See the <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">kubernetes docs</a>
for this.</p>

<p>Once secrets have been created in the namespace that the <code>Coherence</code> resource is to be installed in then the secret name
can be specified in the <code>Coherence</code> CRD <code>spec</code>. It is possible to specify multiple secrets in the case where the different
images being used are pulled from different registries.</p>

<p>For example to use the commercial Coherence 14.1.1.0.0 image from OCR specify the image and image pull secrets in
the <code>Coherence</code> resource yaml</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0
  imagePullSecrets:
    - name: coherence-secret  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>coherence-secret</code> will be used for pulling images from the registry associated to the secret</li>
</ul>
<p>Also see <router-link to="/installation/05_private_repos">Using Private Image Registries</router-link></p>

</div>
</doc-view>
<!-- pages/installation/05_private_repos.js -->
<doc-view>

<h2 id="_using_private_image_registries">Using Private Image Registries</h2>
<div class="section">
<p>Sometimes the images used by a Coherence cluster need to be pulled from a private image registry that requires credentials.
The Coherence Operator supports supplying credentials in the <code>Coherence</code> CRD configuration.
The Kubernetes documentation on <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry">using a private registries</a>
gives a number of options for supplying credentials.</p>

</div>

<h2 id="_use_imagepullsecrets">Use ImagePullSecrets</h2>
<div class="section">
<p>Kubernetes supports configuring pods to use <code>imagePullSecrets</code> for pulling images.
If possible, this is the preferable, and most portable route.
See the <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">kubernetes docs</a>
for this.
Once secrets have been created in the namespace that the <code>Coherence</code> resource is to be installed in then the secret name
can be specified in the <code>Coherence</code> <code>spec</code>.
It is possible to specify multiple secrets in the case where the different images being used will be pulled from different registries.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  imagePullSecrets:
    - name: coherence-secret  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>coherence-secret</code> will be used for pulling images from the registry associated to the secret</li>
</ul>
<p>The <code>imagePullSecrets</code> field is a list of values in the same format that they would be specified in Kubernetes <code>Pod</code>
specs, so multiple secrets can be specified for different authenticated registries in the case where the Coherence
cluster will use images from different authenticated registries..</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  imagePullSecrets:           <span class="conum" data-value="1" />
    - name: coherence-secret
    - name: ocr-secret</pre>

<ul class="colist">
<li data-value="1">The <code>imagePullSecrets</code> list specifies two secrets to use <code>coherence-secret</code> and <code>ocr-secret</code></li>
</ul>
</div>
</doc-view>
<!-- pages/installation/06_openshift.js -->
<doc-view>

<h2 id="_coherence_clusters_on_openshift">Coherence Clusters on OpenShift</h2>
<div class="section">
<p>Whilst the Coherence Operator will run out of the box on OpenShift some earlier versions of the Coherence Docker
image will not work without configuration changes.</p>

<p>These earlier versions of the Coherence Docker images that Oracle publishes default the container user
as <code>oracle</code>. When running the Oracle images or layered images that retain the default user as <code>oracle</code>
with OpenShift, the <code>anyuid</code> security context constraint is required to ensure proper access to the file
system within the Docker image. Later versions of the Coherence images have been modified to work without
needing <code>anyuid</code>.</p>

<p>To work with older image versions , the administrator must:</p>

<ul class="ulist">
<li>
<p>Ensure the <code>anyuid</code> security content is granted</p>

</li>
<li>
<p>Ensure that Coherence containers are annotated with <code>openshift.io/scc: anyuid</code></p>

</li>
</ul>
<p>For example, to update the OpenShift policy, use:</p>

<pre
lang="bash"

>oc adm policy add-scc-to-user anyuid -z default</pre>

<p>and to annotate the Coherence containers, update the <code>Coherence</code> resource to include annotations</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  annotations:
    openshift.io/scc: anyuid  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>openshift.io/scc: anyuid</code> annotation will be applied to all of the Coherence Pods.</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">For additional information about OpenShift requirements see the
<a id="" title="" target="_blank" href="https://docs.openshift.com/container-platform/3.3/creating_images/guidelines.html">OpenShift documentation</a></p>
</div>
</div>
</doc-view>
<!-- pages/installation/07_webhooks.js -->
<doc-view>

<h2 id="_operator_web_hooks">Operator Web-Hooks</h2>
<div class="section">
<p>The Coherence Operator uses Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">dynamic admission control</a>
commonly known as defaulting and validating web-hooks. As the name implies, these are used to provide default values
for some fields in a <code>Coherence</code> resource and to also validate <code>Coherence</code> resources on creation and update.
The operator creates and configures the two web-hooks when it starts.</p>


<h3 id="_webhook_scope">Webhook Scope</h3>
<div class="section">
<p>Webhooks in Kubernetes are a cluster resource, not a namespaced scoped resource, so consequently there is typically only
a single webhook installed for a given resource type. If the Coherence Operator has been installed as a cluster scoped
operator then this is not a problem but if multiple Coherence Operators have been deployed then they could all attempt
to install the webhooks and update or overwrite a previous configuration.
This might not be an issue if all the operators deployed in a Kubernetes cluster are the same version but different
versions could cause issues.
This is one of the reasons that it is recommended to install a single cluster scoped Coherence Operator.</p>

</div>
</div>

<h2 id="_manage_web_hook_certificates">Manage Web-Hook Certificates</h2>
<div class="section">
<p>A web-hook requires certificates to be able to work in Kubernetes.
By default, the operator will create and manage self-signed certificates for this purpose.
It is possible to use other certificates, either managed by the
<a id="" title="" target="_blank" href="https://cert-manager.io/docs/installation/kubernetes/">Kubernetes cert-manager</a> or managed manually.</p>

<p>The certificates should be stored in a <code>Secret</code> named <code>coherence-webhook-server-cert</code> in the same namespace that
the operator has installed in. (although this name can be changed if required). This <code>Secret</code> must exist, or the operator
wil fail to start. The Operator Helm chart will create this <code>Secret</code> when the Operator is managing its own self-signed
certs, otherwise the <code>Secret</code> must be created manually or by an external certificate manager.</p>


<h3 id="_self_signed_certificates">Self-Signed Certificates</h3>
<div class="section">
<p>This is the default option, the operator will create and manage a set of self-signed certificates.
The Operator will update the <code>Secret</code> with its certificates and create the <code>MutatingWebhookConfiguration</code> and
<code>ValidatingWebhookConfiguration</code> resources configured to use those certificates.</p>

</div>

<h3 id="_cert_manager_self_signed">Cert Manager (Self-Signed)</h3>
<div class="section">
<p>Assuming Cert Manager has been installed in the Kubernetes cluster then to use it for managing the web-hook certificates,
the Operator needs to be installed with the <code>CERT_TYPE</code> environment variable set to <code>cert-manager</code>.</p>

<p>The Operator will then detect the version of Cert Manager and automatically create the required self-signed <code>Issuer</code>
and <code>Certificate</code> resources. Cert Manager will detect these and create the <code>Secret</code>. This may cause the operator Pod to
re-start until the <code>Secret</code> has been created.</p>

<p>To set the certificate manager to use when installing the Helm chart, set the <code>webhookCertType</code> value:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    --set webhookCertType=cert-manager <span class="conum" data-value="1" />
    coherence-operator \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">The certificate manager will be set to <code>cert-manager</code></li>
</ul>
</div>

<h3 id="_manual_certificates">Manual Certificates</h3>
<div class="section">
<p>If certificates will managed some other way (for example by Cert Manager managing real certificates) then the
<code>CERT_TYPE</code> environment variable should be set to <code>manual</code>.</p>

<p>Before the Operator starts the <code>Secret</code> must exist containing the valid certificates.
The Operator will use the certificates that it finds in the <code>Secret</code> to create the web-hook resources.</p>

<p>To set the certificate manager to use when installing the Helm chart, set the <code>webhookCertType</code> value:</p>

<pre
lang="bash"

>helm install  \
    --namespace &lt;namespace&gt; \
    --set webhookCertType=manual <span class="conum" data-value="1" />
    coherence-operator \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">The certificate manager will be set to <code>manual</code></li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/installation/08_networking.js -->
<doc-view>

<h2 id="_os_networking_configuration">O/S Networking Configuration</h2>
<div class="section">

<h3 id="_operating_system_library_requirements">Operating System Library Requirements</h3>
<div class="section">
<p>In order for Coherence clusters to form correctly, the <code>conntrack</code> library
must be installed.  Most Kubernetes distributions will do this for you.
If you have issues with clusters not forming, then you should check that
<code>conntrack</code> is installed using this command (or equivalent):</p>

<pre
lang="bash"

>rpm -qa | grep conntrack</pre>

<p>You should see output similar to that shown below.  If you do not, then you
should install <code>conntrack</code> using your operating system tools.</p>

<pre
lang="bash"

>libnetfilter_conntrack-1.0.6-1.el7_3.x86_64
conntrack-tools-1.4.4-4.el7.x86_64</pre>

</div>

<h3 id="_firewall_iptables_requirements">Firewall (iptables) Requirements</h3>
<div class="section">
<p>Some Kubernetes distributions create <code>iptables</code> rules that block some
types of traffic that Coherence requires to form clusters.  If you are
not able to form clusters, then you can check for this issue using the
following command:</p>

<pre
lang="bash"

>iptables -t nat -v  -L POST_public_allow -n</pre>

<p>If you see output similar to the example below:</p>

<pre
lang="bash"

>Chain POST_public_allow (1 references)
pkts bytes target     prot opt in     out     source               destination
164K   11M MASQUERADE  all  --  *      !lo     0.0.0.0/0            0.0.0.0/0
   0     0 MASQUERADE  all  --  *      !lo     0.0.0.0/0            0.0.0.0/0</pre>

<p>For example, if you see any entries in this chain, then you need to remove them.
You can remove the entries using this command:</p>

<pre
lang="bash"

>iptables -t nat -v -D POST_public_allow 1</pre>

<p>Note that you will need to run that command for each line. So in the example
above, you would need to run it twice.</p>

<p>After you are done, you can run the previous command again and verify that
the output is now an empty list.</p>

<p>After making this change, restart your domains and the Coherence cluster
should now form correctly.</p>


<h4 id="_make_iptables_updates_permanent_across_reboots">Make iptables Updates Permanent Across Reboots</h4>
<div class="section">
<p>The recommended way to make <code>iptables</code> updates permanent across reboots is
to create a <code>systemd</code> service that applies the necessary updates during
the startup process.</p>

<p>Here is an example; you may need to adjust this to suit your own
environment:</p>

<ul class="ulist">
<li>
<p>Create a <code>systemd</code> service:</p>

</li>
</ul>
<pre
lang="bash"

>echo 'Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...'
mkdir -p /etc/systemd/system/
cat &gt; /etc/systemd/system/fix-iptables.service &lt;&lt; EOF
[Unit]
Description=Fix iptables
After=firewalld.service
After=docker.service

[Service]
ExecStart=/sbin/fix-iptables.sh

[Install]
WantedBy=multi-user.target
EOF</pre>

<ul class="ulist">
<li>
<p>Create the script to update <code>iptables</code>:</p>

</li>
</ul>
<pre
lang="bash"

>cat &gt; /sbin/fix-iptables.sh &lt;&lt; EOF
#!/bin/bash
echo 'Fixing iptables rules for Coherence issue...'
TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2))
COUNTER=1
while [ $COUNTER -le $TIMES ]; do
  iptables -t nat -v -D POST_public_allow 1
  ((COUNTER++))
done
EOF</pre>

<ul class="ulist">
<li>
<p>Start the service (or just reboot):</p>

</li>
</ul>
<pre
lang="bash"

>echo 'Start the systemd service to fix iptables nat chain...'
systemctl enable --now fix-iptables</pre>

</div>
</div>
</div>
</doc-view>
<!-- pages/installation/09_RBAC.js -->
<doc-view>

<h2 id="_rbac_roles">RBAC Roles</h2>
<div class="section">
<p>When installing the Coherence Operator into Kubernetes clusters with RBAC enabled, the Operator will require certain roles to work properly. Both the Operator Helm chart, and the Operator k8s manifest files will install all the required roles, role bindings and create a service account.</p>

</div>

<h2 id="_cluster_roles">Cluster Roles</h2>
<div class="section">
<p>By default, both install methods will create ClusterRole resources and ClusterRoleBinding resources to bind those roles to the Operator ServiceAccount. Some Kubernetes administrators are wary of letting arbitrary installations have ClusterRole permissions and try to discourage it. The Coherence Operator can run without ClusterRole permissions, but it is important to understand what this means from an operational point of view.</p>

<p>Cluster roles are used for a number of operator features:</p>

<ul class="ulist">
<li>
<p>Installing the CRDs - the Operator automatically ensures that the CRDs it requires are installed when it starts.</p>

</li>
<li>
<p>Installing the Web-Hook - the Operator automatically installs the defaulting and validating web-hooks for the <code>Coherence</code> resource when it starts. Without the validating web-hook a lot more care must be taken to ensure that only valid <code>Coherence</code> resource yaml is added to k8s. In the worst case, invalid yaml may ultimately cause the Operator to panic where invalid yaml would normally have been disallowed by the web-hook.</p>

</li>
<li>
<p>Coherence CLuster site and rack information - the Operator is used to supply site and rack values for the Coherence clusters that it manages. These values come from <code>Node</code> labels that the Operator must be able to look up. Without this information a Coherence cluster will have empty values for the <code>coherence.site</code> and <code>coherence.rack</code> system properties, meaning that Coherence will be unable to make data site-safe in k8s clusters that have multiple availability zones.</p>

</li>
<li>
<p>Monitoring multiple namespaces - if the Operator is to monitor multiple namespaces it must have cluster wide roles to do this</p>

</li>
</ul>
<p>Assuming that all the above reductions in features are acceptable then the Operator can be installed without creating cluster roles.</p>

</div>

<h2 id="_install_the_operator_without_clusterroles">Install the Operator Without ClusterRoles</h2>
<div class="section">
<p>The two methods of installing the Operator discussed in the <router-link to="/installation/01_installation">Install Guide</router-link> can be used to install the Operator without ClusterRoles.</p>


<h3 id="_manually_install_crds">Manually Install CRDs</h3>
<div class="section">
<div class="admonition important">
<p class="admonition-textlabel">Important</p>
<p ><p>Before installing the Operator, with either method described below, the CRDs MUST be manually installed from the Operator manifest files.</p>

<p>The manifest files are published with the GitHub release at this link:
<a id="" title="" target="_blank" href="https://github.com/oracle/coherence-operator/releases/download/v3.2.2/coherence-operator-manifests.tar.gz">3.2.2 Manifests</a></p>

<p>You MUST ensure that the CRD manifests match the version of the Operator being installed.</p>

<ul class="ulist">
<li>
<p>Download the manifests and unpack them.</p>

</li>
<li>
<p>In the directory that the .tar.gz file was unpacked to will be two versions of the CRDs.
The directory <code>crd/</code> contains the <code>apiextensions.k8s.io/v1</code> version, which must be installed into Kubernetes cluster from k8s v1.16.x and above. The <code>crd-v1beta1/</code> directory contains the <code>apiextensions.k8s.io/v1beta1</code> version, which must be installed into Kubernetes cluster of k8s v1.15.x and below.</p>

</li>
</ul>
<p>The required CRD can be installed with kubectl</p>

<pre
lang="bash"

>kubectl create -f crd/coherence.oracle.com_coherence.yaml</pre>
</p>
</div>
</div>

<h3 id="_install_using_helm">Install Using Helm</h3>
<div class="section">
<p>The Operator can be installed from the Helm chart, as described in the <router-link to="/installation/01_installation">Install Guide</router-link>.
The Helm chart contains values that control whether cluster roles are created when installing the chart. To install the chart without any cluster roles set the <code>clusterRoles</code> value to <code>false</code>.</p>

<pre
lang="bash"

>helm install  \
    --set clusterRoles=false       <span class="conum" data-value="1" />
    --namespace &lt;namespace&gt; \      <span class="conum" data-value="2" />
    coherence \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">The <code>clusterRoles</code> value is set to false.</li>
<li data-value="2">The <code>&lt;namespace&gt;</code> value is the namespace that the Coherence Operator will be installed into
and without cluster roles will be the <em>only</em> namespace that the Operator monitors.</li>
</ul>

<h4 id="_allow_node_lookup">Allow Node Lookup</h4>
<div class="section">
<p>The Helm chart allows the Operator to be installed with a single <code>ClusterRole</code> allowing it to read k8s <code>Node</code> information. This is used to provide site, and rack labels, for Coherence cluster members. In environments where Kubernetes administrators are happy to allow the Operator read-only access to <code>Node</code> information the <code>nodeRoles</code> value can be set to <code>true</code>.</p>

<pre
lang="bash"

>helm install  \
    --set clusterRoles=false       <span class="conum" data-value="1" />
    --set nodeRoles=true           <span class="conum" data-value="2" />
    --namespace &lt;namespace&gt; \
    coherence \
    coherence/coherence-operator</pre>

<ul class="colist">
<li data-value="1">The <code>clusterRoles</code> value is set to <code>false</code>.</li>
<li data-value="2">The <code>nodeRoles</code> value is set to <code>true</code>, so a single ClusterRole will be applied to the Operator&#8217;s service account</li>
</ul>
</div>
</div>

<h3 id="_install_using_kustomize">Install Using Kustomize</h3>
<div class="section">
<p>The Operator can be installed using Kustomize with the manifest files, as described in the <router-link to="/installation/01_installation">Install Guide</router-link>.</p>


<h4 id="_exclude_the_clusterrole_manifests">Exclude the ClusterRole Manifests</h4>
<div class="section">
<p>To install without cluster roles, after unpacking the manifests <code>.tar.gz</code> edit the <code>config/kustomization.yaml</code> file to comment out the inclusion of the cluster role bindings.</p>

<p>For example:</p>

<pre
lang="yaml"
title="kustomization.yaml"
>resources:
- service_account.yaml
- role.yaml
- role_binding.yaml
#- node_viewer_role.yaml
#- node_viewer_role_binding.yaml
#- cluster_role.yaml
#- cluster_role_binding.yaml</pre>

</div>

<h4 id="_disable_web_hooks_and_crd_installation">Disable Web-Hooks and CRD Installation</h4>
<div class="section">
<p>The Operator would normally install validating and defaulting web-hooks as well as ensuring that the Coherence CRDs are installed. Without cluster roles this must be disabled by editing the <code>manager/manager.yaml</code> file in the manifests.</p>

<p>Edit the Operator container <code>args</code> section of the deployment yaml to add command line arguments to <code>--enable-webhook=false</code> to disable web-hook creation and <code>--install-crd=false</code> to disable CRD installation.</p>

<p>For example, change the section of the <code>manager/manager.yaml</code> file that looks like this:</p>

<pre
lang="yaml"
title="manager/manager.yaml"
>        command:
          - /manager
        args:
          - --enable-leader-election
        envFrom:</pre>

<p>to be:</p>

<pre
lang="yaml"
title="manager/manager.yaml"
>        command:
          - /manager
        args:
          - --enable-leader-election
          - --enable-webhook=false
          - --install-crd=false
        envFrom:</pre>

</div>

<h4 id="_edit_the_operator_clusterrole_clusterrolebinding">Edit the Operator ClusterRole &amp; ClusterRoleBinding</h4>
<div class="section">
<p>The Operator will require a role and role binding to work in a single namespace.
Edit the <code>config/role.yaml</code> to change its type from <code>ClusterRole</code> to <code>Role</code>.</p>

<p>For example, change:</p>

<pre
lang="yaml"
title="role.yaml"
>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: manager-role</pre>

<p>to be:</p>

<pre
lang="yaml"
title="role.yaml"
>apiVersion: rbac.authorization.k8s.io/v1
kind: Role  <span class="conum" data-value="1" />
metadata:
  creationTimestamp: null
  name: manager-role</pre>

<ul class="colist">
<li data-value="1"><code>ClusterRole</code> has been changed to <code>Role</code></li>
</ul>
<p>Edit the <code>config/role_binding.yaml</code> to change its type from <code>ClusterRoleBinding</code> to <code>RoleBinding</code>.</p>

<p>For example change:</p>

<pre
lang="yaml"
title="role_binding.yaml"
>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: manager-rolebinding
  labels:
    control-plane: coherence
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: manager-role
subjects:
- kind: ServiceAccount
  name: coherence-operator
  namespace: default</pre>

<p>to be:</p>

<pre
lang="yaml"
title="role_binding.yaml"
>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding  <span class="conum" data-value="1" />
metadata:
  name: manager-rolebinding
  labels:
    control-plane: coherence
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role <span class="conum" data-value="2" />
  name: manager-role
subjects:
- kind: ServiceAccount
  name: coherence-operator
  namespace: default</pre>

<ul class="colist">
<li data-value="1">The type has been changed from <code>ClusterRoleBinding</code> to <code>RoleBinding</code></li>
<li data-value="2">The role being bound has been changed from <code>ClusterRole</code> to <code>Role</code>.</li>
</ul>
</div>

<h4 id="_allow_node_lookup_2">Allow Node Lookup</h4>
<div class="section">
<p>In environments where Kubernetes administrators are happy to allow the Operator read-only access to <code>Node</code> information, the required <code>ClusterRole</code> can be created by leaving the relevant lines uncommented in the <code>config/kustomization.yaml</code> file.</p>

<p>For example:</p>

<pre
lang="yaml"
title="kustomization.yaml"
>resources:
- service_account.yaml
- role.yaml
- role_binding.yaml
- node_viewer_role.yaml         <span class="conum" data-value="1" />
- node_viewer_role_binding.yaml
#- cluster_role.yaml
#- cluster_role_binding.yaml</pre>

<ul class="colist">
<li data-value="1">The <code>node_viewer_role.yaml</code> and <code>node_viewer_role_binding.yaml</code> will now be left in the installation.</li>
</ul>
</div>
</div>
</div>
</doc-view>
<!-- pages/jvm/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>The Coherence Operator allows full control over the configuration of the JVM used to run the Coherence application.
The <code>jvm</code> section of the <code>Coherence</code> CRD spec has a number of fields to easily configure specific aspects of the
JVM as well as a catch-all <code>jvm.args</code> list that allows any arbitrary argument to be passed to the JVM.</p>

<p>Whilst every configuration setting could, in theory, be set only by specifying JVM arguments in the <code>jvm.args</code>
field of the <code>Coherence</code> CRD, the other configuration fields provide simpler means to set configuration
without having to remember specific JVM argument names or system property names to set.
You are, of course, free to use whichever approach best suits your requirements;
but obviously it is better to choose one approach and be consistent.</p>


<h3 id="_guides_to_jvm_settings">Guides to JVM Settings</h3>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/020_classpath"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Classpath</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Default classpath settings and options for setting a custom classpath.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/030_jvm_args"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">JVM Arguments</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding arbitrary JVM arguments and system properties.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/040_gc"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Garbage Collection</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configuring the garbage collector.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/050_memory"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Heap & Memory Settings</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configuring the heap size and other memory settings.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/070_debugger"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Debugger</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Using debugger settings.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/080_jmx"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">JMX</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Using JMX.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/jvm/090_container_limits"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Use Container Limits</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configuring the JVM to respect container resource limits.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</div>
</doc-view>
<!-- pages/jvm/020_classpath.js -->
<doc-view>

<h2 id="_set_the_classpath">Set the Classpath</h2>
<div class="section">
<p>The Coherence container in the <code>Pods</code> in a <code>Coherence</code> resource deployment runs a Java application and as such requires a classpath
with at a minimum <code>coherence.jar</code>. There are certain defaults that the Operator will use to work out the classpath to use
but additional classpath elements can be provided to the configuration.</p>


<h3 id="_the_classpath_environment_variable">The <code>CLASSPATH</code> Environment Variable</h3>
<div class="section">
<p>If the image to be run has the <code>CLASSPATH</code> environment variable set this will be used as part of the classpath.</p>

</div>

<h3 id="_the_coherence_home_environment_variable">The <code>COHERENCE_HOME</code> Environment Variable</h3>
<div class="section">
<p>If the image to be run has the <code>COHERENCE_HOME</code> environment variable set this will be used to add the following elements
to the classpath:</p>

<ul class="ulist">
<li>
<p><code>$COHERENCE_HOME/lib/coherence.jar</code></p>

</li>
<li>
<p><code>$COHERENCE_HOME/conf</code></p>

</li>
</ul>
<p>These will be added to the end of the classpath. For example in an image that has <code>CLASSPATH=/home/root/lib/*</code>
and <code>COHERENCE_HOME</code> set to <code>/oracle/coherence</code> the effective classpath used will be:</p>

<pre>/home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf</pre>
</div>

<h3 id="_jib_image_classpath">JIB Image Classpath</h3>
<div class="section">
<p>A simple way to build Java images is using <a id="" title="" target="_blank" href="https://github.com/GoogleContainerTools/jib/blob/master/README.md">JIB</a>.
When JIB was with its Maven or Gradle plugin to produce an image it packages the application&#8217;s dependencies, classes
and resources into a set of well-known locations:</p>

<ul class="ulist">
<li>
<p><code>/app/libs/</code> - the jar files that the application depends on</p>

</li>
<li>
<p><code>/app/classes</code> - the application&#8217;s class files</p>

</li>
<li>
<p><code>/app/resources</code> - the application&#8217;s other resources</p>

</li>
</ul>
<p>By default, the Operator will add these locations to the classpath. These classpath elements will be added before any
value set by the <code>CLASSPATH</code> or <code>COHERENCE_HOME</code> environment variables.</p>

<p>For example in an image that has <code>CLASSPATH=/home/root/lib/\*</code>
and <code>COHERENCE_HOME</code> set to <code>/oracle/coherence</code> the effective classpath used will be:</p>

<pre>/app/libs/*:/app/classes:/app/resources:/home/root/lib/*:/oracle/coherence/lib/coherence.jar:/oracle/coherence/conf</pre>

<h4 id="_exclude_the_jib_classpath">Exclude the JIB Classpath</h4>
<div class="section">
<p>If the image is not a JIB image there could be occasions when automatically adding <code>/app/libs/*:/app/classes:/app/resources</code>
to the classpath causes issues, for example one or more of those locations exists with files in that should not be on the
classpath. In this case the <code>Coherence</code> CRD spec has a field to specify that the JIB classpath should not be used.</p>

<p>The <code>spec.jvm.useJibClasspath</code> field can be set to <code>false</code> to exclude the JIB directories from the classpath
(the default value is <code>true</code>).</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    useJibClasspath: false  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>useJibClasspath</code> is set to <code>false</code>. Even if any of the <code>/app/resources</code>, <code>/app/classes</code> or <code>/app/libs/</code>
directories exist in the image they will not be added to the classpath.</li>
</ul>
</div>
</div>

<h3 id="_additional_classpath_elements">Additional Classpath Elements</h3>
<div class="section">
<p>If an image will be used that has artifacts in locations other than the defaults discussed above then it is possible
to specify additional elements to be added to the classpath. The <code>jvm.classpath</code> field in the <code>Coherence</code> CRD spec
allows a list of extra classpath values to be provided. These elements will be added <em>after</em> the JIB classpath
described above.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    classpath:          <span class="conum" data-value="1" />
      - "/data/lib/*"
      - "/data/config"</pre>

<ul class="colist">
<li data-value="1">The <code>classpath</code> field adds <code>/data/lib/*</code> and <code>/data/config</code> to the classpath.
In an image without the <code>CLASSPATH</code> or <code>COHERENCE_HOME</code> environment variables the effective classpath would be:</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">There is no validation of the elements of the classpath. The elements will not be verified to ensure that the locations
exist. As long as they are valid values to be used in a JVM classpath they will be accepted.</p>
</div>
</div>
</div>

<h2 id="_environment_variable_expansion">Environment Variable Expansion</h2>
<div class="section">
<p>The Operator supports environment variable expansion in classpath entries.
The runner in the Coherence container will replace <code>${var}</code> or <code>$var</code> in classpath entries with the corresponding environment variable name.</p>

<p>For example if a container has an environment variable of <code>APP_HOME</code> set to <code>/myapp</code> then it could be used in the classpath like this:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    classpath:
      - "${APP_HOME}/lib/*"  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The actual classpath entry at runtime will resolve to <code>/myapp/lib/*</code></li>
</ul>
<p>Any environment variable that is present when the Coherence container starts can be used, this would include variables created as part of the image and variables specified in the Coherence yaml.</p>

</div>
</doc-view>
<!-- pages/jvm/030_jvm_args.js -->
<doc-view>

<h2 id="_adding_arbitrary_jvm_arguments">Adding Arbitrary JVM Arguments</h2>
<div class="section">
<p>The <code>Coherence</code> CRD allows any arbitrary JVM arguments to be passed to the JVM in the <code>coherence</code> container
by using the <code>jvm.args</code> field of the CRD spec.
Any valid system property or JVM argument can be added to the <code>jvm.args</code> list.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    args:
      - "-Dcoherence.pof.config=storage-pof-config.xml"
      - "-Dcoherence.tracing.ratio=0.1"
      - "-agentpath:/yourkit/bin/linux-x86-64/libyjpagent.so"</pre>

<p>In this example the <code>args</code> list adds two System properties <code>coherence.pof.config=storage-pof-config.xml</code>
and <code>coherence.tracing.ratio=0.1</code> and also adds the YourKit profiling agent.</p>

<div class="admonition note">
<p class="admonition-inline">When the Operator builds the command line to use when starting Coherence Pods, any arguments added to
the <code>jvm.args</code> field will be added after all the arguments added by the Operator from other configuration fields.
This means that arguments such as system properties added to <code>jvm.args</code> will override any added by the Operator.</p>
</div>
<p>For example</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  coherence:
    cacheConfig: storage-config.xml                   <span class="conum" data-value="1" />
  jvm:
    args:
      - "-Dcoherence.cache.config=test-config.xml"    <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">Setting the <code>coherence.cacheConfig</code> field will make the operator add
<code>-Dcoherence.cache.config=storage-config.xml</code> to the command line.</li>
<li data-value="2">Adding <code>-Dcoherence.cache.config=test-config.xml</code> to the <code>jvm.args</code> field will make the Operator add
<code>-Dcoherence.cache.config=test-config.xml</code> to the end of the JVM arguments in the command line.</li>
</ul>
<p>When duplicate system properties are present on a command line, the last one wins so in the example above the cache
configuration used would be  <code>test-config.xml</code>.</p>


<h3 id="_default_arguments">Default Arguments</h3>
<div class="section">
<p>The Coherence Operator will add the following JVM arguments by default:</p>

<pre


>-Dcoherence.cluster=&lt;cluster-name&gt;
-Dcoherence.role=&lt;role&gt;
-Dcoherence.wka=&lt;deployment-name&gt;-wka.svc.cluster.local
-Dcoherence.cacheconfig=coherence-cache-config.xml
-Dcoherence.k8s.operator.health.port=6676
-Dcoherence.management.http.port=30000
-Dcoherence.metrics.http.port=9612
-Dcoherence.distributed.persistence-mode=on-demand
-Dcoherence.override=k8s-coherence-override.xml
-Dcoherence.ttl=0

-XX:+UseG1GC
-XX:+PrintCommandLineFlags
-XX:+PrintFlagsFinal
-XshowSettings:all
-XX:+UseContainerSupport
-XX:+HeapDumpOnOutOfMemoryError
-XX:+ExitOnOutOfMemoryError
-XX:HeapDumpPath=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/heap-dumps/&lt;member&gt;-&lt;pod-uid&gt;.hprof
-XX:ErrorFile=/jvm/&lt;member&gt;/&lt;pod-uid&gt;/hs-err-&lt;member&gt;-&lt;pod-uid&gt;.log
-XX:+UnlockDiagnosticVMOptions
-XX:NativeMemoryTracking=summary
-XX:+PrintNMTStatistics</pre>

<p>Some arguments and system properties above can be overridden or changed by setting values in the <code>Coherence</code> CDR spec.</p>

</div>
</div>

<h2 id="_environment_variable_expansion">Environment Variable Expansion</h2>
<div class="section">
<p>The Operator supports environment variable expansion in JVM arguments.
The runner in the Coherence container will replace <code>${var}</code> or <code>$var</code> in the JVM arguments with the corresponding environment variable name.</p>

<p>For example a JVM argument of <code>"-Dmy.host.name=${HOSTNAME}"</code> when run on a Pod with a host name of <code>COH-1</code> will resolve to <code>"-Dmy.host.name=COH-1"</code>.</p>

<p>Any environment variable that is present when the Coherence container starts can be used, this would include variables created as part of the image and variables specified in the Coherence yaml.</p>

</div>
</doc-view>
<!-- pages/jvm/040_gc.js -->
<doc-view>

<h2 id="_garbage_collector_settings">Garbage Collector Settings</h2>
<div class="section">
<p>The <code>Coherence</code> CRD has fields in the <code>jvm.gc</code> section to allow certain garbage collection parameters to be set.
These include GC logging, setting the collector to use and arbitrary GC arguments.</p>


<h3 id="_enable_gc_logging">Enable GC Logging</h3>
<div class="section">
<p>To enable GC logging set the <code>jvm.gc.logging</code> field to <code>true</code>.
For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    gc:
      logging: true</pre>

<p>Setting the field to true adds the following JVM arguments to the JVM in the <code>coherence</code> container:</p>

<div class="listing">
<pre>-verbose:gc
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+PrintHeapAtGC
-XX:+PrintTenuringDistribution
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime</pre>
</div>

<p>If different GC logging arguments are required then the relevant JVM arguments can be added to either the
<code>jvm.args</code> field or the <code>jvm.gc.args</code> field.</p>

</div>

<h3 id="_set_the_garbage_collector">Set the Garbage Collector</h3>
<div class="section">
<p>The garbage collector to use can be set using the <code>jvm.gc.collector</code> field.
This field can be set to either <code>G1</code>, <code>CMS</code> or <code>Parallel</code>
(the field is case-insensitive, invalid values will be silently ignored).
The default collector set, if none has been specified, will be <code>G1</code>.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
</thead>
<tbody>
<tr>
<td class="">Parameter</td>
<td class="">JVM Argument Set</td>
</tr>
<tr>
<td class=""><code>G1</code></td>
<td class=""><code>-XX:+UseG1GC</code></td>
</tr>
<tr>
<td class=""><code>CMS</code></td>
<td class=""><code>-XX:+UseConcMarkSweepGC</code></td>
</tr>
<tr>
<td class=""><code>Parallel</code></td>
<td class=""><code>-XX:+UseParallelGC</code></td>
</tr>
</tbody>
</table>
</div>
<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    gc:
      collector: "G1"</pre>

<p>The example above will add <code>-XX:+UseG1GC</code> to the command line.</p>

</div>

<h3 id="_adding_arbitrary_gc_args">Adding Arbitrary GC Args</h3>
<div class="section">
<p>Any arbitrary GC argument can be added to the <code>jvm.gc.args</code> field.
These arguments will be passed verbatim to the JVM command line.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    gc:
      args:
        - "-XX:MaxGCPauseMillis=200"</pre>

<p>In the example above the <code>-XX:MaxGCPauseMillis=200</code> JVM argument will be added to the command line.</p>

<div class="admonition note">
<p class="admonition-inline">The <code>jvm.gc.args</code> field will add the provided arguments to the end of the command line exactly as they
are in the args list. This field provides the same functionality as <router-link to="/jvm/030_jvm_args">JVM Args</router-link>
but sometimes it might be useful to be able to separate the two gorups of arguments in the CRD spec.</p>
</div>
</div>
</div>
</doc-view>
<!-- pages/jvm/050_memory.js -->
<doc-view>

<h2 id="_heap_memory_settings">Heap &amp; Memory Settings</h2>
<div class="section">
<p>The JVM has a number of arguments that set the sizes of different memory regions; the most commonly set is the heap
size but there are a number of others. The <code>Coherence</code> CRD spec has fields that allow some of these to sizes to be
set.</p>

<p>The <code>Coherence</code> CRD also has settings to control the behaviour of the JVM if an out of memory error occurs.
For example, killing the container, creating a heap dump etc.</p>


<h3 id="_max_ram">Max RAM</h3>
<div class="section">
<p>The JVM has an option <code>-XX:MaxRAM=N</code> the maximum amount of memory used by the JVM to <code>n</code>, where <code>n</code> is expressed in
terms of megabytes (for example, <code>100m</code>) or gigabytes (for example <code>2g</code>).</p>

<p>When using resource limited containers it is useful to set the max RAM option to avoid the JVM exceeding the
container limits.</p>

<p>The <code>Coherence</code> CRD allows the max RAM option to be set using the <code>jvm.memory.maxRAM</code> field, for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      maxRAM: 10g</pre>

</div>

<h3 id="_heap_size_as_a_percentage_of_container_memory">Heap Size as a Percentage of Container Memory</h3>
<div class="section">
<p>There are three JVM options that can be used to control the JVM heap as a percentage of the available memory.
These options can be useful when controlling memory as a percentage of container memory in combination
with resource limits on containers.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th>JVM Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>-XX:InitialRAMPercentage=N</code></td>
<td class="">Sets the initial amount of memory that the JVM will use for the Java heap before applying ergonomics heuristics as a
percentage of the maximum amount determined as described in the -XX:MaxRAM option. The default value is 1.5625 percent.</td>
</tr>
<tr>
<td class="">'-XX:MaxRAMPercentage=N'</td>
<td class="">Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a
percentage of the maximum amount determined as described in the <code>-XX:MaxRAM</code> option.
The default value is 25 percent.</td><td class="">Specifying this option disables automatic use of compressed oops if the combined result of this and other options
influencing the maximum amount of memory is larger than the range of memory addressable by compressed oops.</td>
</tr>
<tr>
<td class="">'-XX:MinRAMPercentage=N'</td>
<td class="">Sets the maximum amount of memory that the JVM may use for the Java heap before applying ergonomics heuristics as a
percentage of the maximum amount determined as described in the -XX:MaxRAM option for small heaps.
A small heap is a heap of approximately 125 MB.
The default value is 50 percent.</td>
</tr>
</tbody>
</table>
</div>
<p>Where <code>N</code> is a decimal value between 0 and 100. For example, 12.3456.</p>

<p>When running in a container, and the <code>-XX:+UseContainerSupport</code> is set (which it is by default for the Coherence
container), both the default heap size for containers, the <code>-XX:InitialRAMPercentage</code> option, the <code>-XX:MaxRAMPercentage</code>
option, and the <code>-XX:MaxRAMPercentage</code> option, will be based on the available container memory.</p>

<div class="admonition note">
<p class="admonition-inline">Some JVMs may not support these options.</p>
</div>
<p>The <code>Coherence</code> CRD allows these options to be set with the <code>jvm.memory.initialRAMPercentage</code>, <code>jvm.memory.minRAMPercentage</code>,
and <code>jvm.memory.maxRAMPercentage</code> fields.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      initialRAMPercentage: 10
      minRAMPercentage: 5.75
      maxRAMPercentage: 75</pre>


<h4 id="_set_heap_percentages_from_a_single_value">Set Heap Percentages From a Single Value</h4>
<div class="section">
<p>Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the
JVM runs with a fixed size heap. The <code>Coherence</code> CRD provides the <code>jvm.memory.percentage</code> field for this use-case.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      percentage: 10  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">In this case the <code>percentage</code> field has been set to <code>10</code>, so the options passed to the JVM will be
<code>-XX:InitialRAMPercentage=10 -XX:MinRAMPercentage=10 -XX:MaxRAMPercentage=10</code> meaning the heap size
will be fixed at 10% of max RAM.</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">Setting the <code>jvm.memory.percentage</code> field will cause individual RAM percentage fields to be ignored.</p>
</div>
<div class="admonition note">
<p class="admonition-inline">The JVM documentation states that <em>"If you set a value for <code>-Xms</code>, the <code>-XX:InitialRAMPercentage</code>,
<code>-XX:MinRAMPercentage</code> and <code>-XX:MaxRAMPercentage</code> options will be ignored"</em>. So if the <code>Coherence</code> CRD fields
detailed below for explictly setting the heap size as a bytes value are used then we can assume that the RAM
percentage fields detailed here will be ignored by the JVM. The Coherence Operator will pass both the percentage
and explicit values to the JVM.</p>
</div>
<div class="admonition note">
<p class="admonition-inline">Due to CRDs not supporting decimal fields the RAM percentage fields are of type resource.Quantity,
see the Kubernetes <a id="" title="" target="_blank" href="https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity">Quantity</a> API docs for details
of the different number formats allowed.</p>
</div>
</div>
</div>

<h3 id="_set_heap_size_explicitly">Set Heap Size Explicitly</h3>
<div class="section">
<p>There are two JVM options that can be used to control the JVM heap as an explicit size in bytes value.
These options can be useful when controlling memory of container memory in combination with resource limits on containers.</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th>JVM Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>-XX:InitialHeapSize=&lt;size&gt;</code></td>
<td class="">Set initial heap size</td>
</tr>
<tr>
<td class=""><code>-XX:MaxHeapSize=&lt;size&gt;</code></td>
<td class="">Set maximum heap size</td>
</tr>
</tbody>
</table>
</div>
<p>The <code>&lt;size&gt;</code> parameter is a numeric integer followed by a suffix to the size value: "k" or "K" to indicate kilobytes,
"m" or "M" to indicate megabytes, "g" or "G" to indicate gigabytes, or, "t" or "T" to indicate terabytes.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      initialHeapSize: 5g  <span class="conum" data-value="1" />
      maxHeapSize: 10g     <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The initial heap size to <code>5g</code>, passing the <code>-XX:InitialHeapSize=5g</code> option to the JVM.</li>
<li data-value="2">The max heap size to <code>10g</code>, passing the <code>-XX:MaxHeapSize=10g</code> option to the JVM.</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">Setting the <code>jvm.memory.heapSize</code> field will cause individual <code>jvm.memory.initialHeapSize</code> and
<code>jvm.memory.maxHeapSize</code> fields to be ignored.</p>
</div>

<h4 id="_set_initial_and_max_heap_size_with_a_single_value">Set Initial and Max Heap Size With a Single Value</h4>
<div class="section">
<p>Typically, with Coherence storage members the initial and maximum heap values will be set to the same value so that the
JVM runs with a fixed size heap. The <code>Coherence</code> CRD provides the <code>jvm.memory.heapSize</code> field for this use-case.</p>

<p>To set the JVM both the initial amd max heap sizes to the same value, set the <code>jvm.memory.heapSize</code> field.
The value of the field can be any value that can be used with the JVM <code>-XX:InitialHeapSize</code> and <code>-XX:MaxHeapSize</code>
(or <code>-Xmx</code> and <code>-Xms</code>) arguments.
The value of the <code>jvm.memory.heapSize</code> field will be used to set both the <code>-XX:InitialHeapSize</code>, and the
<code>-XX:MaxHeapSize</code> arguments to the same value, so the heap will be a fixed size.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      heapSize: 10g  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">Setting <code>jvm.memory.heapSize</code> to <code>10g</code> will effectively pass <code>-XX:InitialHeapSize=10g -XX:MaxHeapSize=10g</code> to the JVM.</li>
</ul>
</div>
</div>

<h3 id="_direct_memory_size_nio_memory">Direct Memory Size (NIO Memory)</h3>
<div class="section">
<p>Direct memory size is used to limit on memory that can be reserved for all Direct Byte Buffers.
If a value is set for this option, the sum of all Direct Byte Buffer sizes cannot exceed the limit.
After the limit is reached, a new Direct Byte Buffer can be allocated only when enough old buffers are freed to provide
enough space to allocate the new buffer.</p>

<p>By default, the VM limits the amount of heap memory used for Direct Byte Buffers to approximately 85% of the maximum heap size.</p>

<p>To set a value for the direct memory size use the <code>jvm.memory.directMemorySize</code> field. This wil set the value of the
<code>-XX:MaxDirectMemorySize</code> JVM option.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      directMemorySize: 10g  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The direct memory size is set to <code>10g</code> which will pass <code>-XX:MaxDirectMemorySize=10g</code> to the JVM.</li>
</ul>
</div>

<h3 id="_metaspace_size">Metaspace Size</h3>
<div class="section">
<p>Metaspace is memory the VM uses to store class metadata.
Class metadata are the runtime representation of java classes within a JVM process - basically any information the JVM
needs to work with a Java class. That includes, but is not limited to, runtime representation of data from the JVM
class file format.</p>

<p>To set the size of the metaspace use the <code>jvm.memory.metaspaceSize</code> field in the <code>Coherence</code> CRD.
Setting this field sets both the <code>-XX:MetaspaceSize</code> and <code>-XX:MaxMetaspaceSize</code> JVM options to this value giving a
fixed size metaspace.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      metaspaceSize: 100m  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">Set the metaspace size to <code>100m</code> which will pass <code>-XX:MetaspaceSize=100m -XX:MaxMetaspaceSize=100m</code>
to the JVM.</li>
</ul>
</div>

<h3 id="_stack_size">Stack Size</h3>
<div class="section">
<p>Thread stacks are memory areas allocated for each Java thread for their internal use.
This is where the thread stores its local execution state.
The current default size for a linux JVM is 1MB.</p>

<p>To set the stack size use the <code>jvm.memory.stackSize</code> field in the <code>Coherence</code> CRD.
Setting this value sets the <code>-Xss</code> JVM option.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      stackSize: 500k  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The stack size will be set to <code>500k</code>, passing <code>-Xss500k</code> to the JVM.</li>
</ul>
</div>

<h3 id="_out_of_memory_behaviour">Out Of Memory Behaviour</h3>
<div class="section">
<p>The <code>Coherence</code> CRD allows two optional behaviours to be specified if the JVM throws an out of memory error.</p>

<p>The <code>jvm.memory.onOutOfMemory.heapDump</code> is a bool field that when set to true will pass the
<code>-XX:+HeapDumpOnOutOfMemoryError</code> option to the JVM. The default value of the field when not specified is <code>true</code>,
hence to turn off heap dumps on OOM set the specifically field to be <code>false</code>.</p>

<p>The <code>jvm.memory.onOutOfMemory.exit</code> is a bool field that when set to true will pass the
<code>-XX:+ExitOnOutOfMemoryError</code> option to the JVM. The default value of the field when not specified is <code>true</code>,
hence to turn off killing the JVM on OOM set the specifically field to be <code>false</code>.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      onOutOfMemory:
        heapDump: true   <span class="conum" data-value="1" />
        exit: true       <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The JVM will create a heap dump on OOM</li>
<li data-value="2">The JVM will exit on OOM</li>
</ul>
</div>

<h3 id="_native_memory_tracking">Native Memory Tracking</h3>
<div class="section">
<p>The Native Memory Tracking (NMT) is a Java VM feature that tracks internal memory usage for a JVM.
The <code>Coherence</code> CRD allows native memory tracking to be configured using the <code>jvm.memory.nativeMemoryTracking</code> field.
Setting this field sets the <code>-XX:NativeMemoryTracking</code> JVM option. There are three valid values, <code>off</code>, <code>summary</code> or <code>detail</code>.
If not specified the default value used by the operator is <code>summary</code></p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    memory:
      nativeMemoryTracking: detail <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">Native memory tracking is set to <code>detail</code> which will pass the <code>-XX:NativeMemoryTracking=detail</code> option to the JVM.</li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/jvm/070_debugger.js -->
<doc-view>

<h2 id="_debugger_configuration">Debugger Configuration</h2>
<div class="section">
<p>Occasionally it is useful to be able to connect a debugger to a JVM, and the <code>Coherence</code> CRD spec has fields to
configure the Coherence container&#8217;s JVM to work with a debugger. The fields in the CRD will ultimately result in
arguments being passed to the JVM and could have been added as plain JVM arguments, but having specific fields in the
CRD makes it simpler to configure and the intention more obvious.</p>

<p>The fields to control debug settings of the JVM are in the <code>jvm.debug</code> section of the CRD spec.</p>


<h3 id="_listening_for_a_debugger_connection">Listening for a Debugger Connection</h3>
<div class="section">
<p>One scenario for debugging is for the Coherence JVM to open a port and listen for a debugger connection request.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    debug:
      enabled: true   <span class="conum" data-value="1" />
      port: 5005      <span class="conum" data-value="2" />
      suspend: false  <span class="conum" data-value="3" /></pre>

<ul class="colist">
<li data-value="1">The <code>jvm.debug.enabled</code> flag is set to <code>true</code> to enable debug mode.</li>
<li data-value="2">The <code>jvm.debug.port</code> field specifies the port the JVM will listen on for a debugger connection.</li>
<li data-value="3">The <code>jvm.debug.suspend</code> flag is set to <code>false</code> so that the JVM will start without waiting for a debugger to connect.</li>
</ul>
<p>The example above results in the following arguments being passed to the JVM:</p>

<pre


>-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005</pre>

<ul class="ulist">
<li>
<p>The <code>address=*:5005</code> value comes from the <code>jvm.debug.port</code> field</p>

</li>
<li>
<p>The <code>suspend=n</code> value comes from the <code>jvm.debug.suspend</code> field</p>

</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">If the <code>jvm.debug.port</code> is not specified the default value used by the Operator will be <code>5005</code>.</p>
</div>
</div>

<h3 id="_attaching_to_a_debugger_connection">Attaching to a Debugger Connection</h3>
<div class="section">
<p>Another scenario for debugging is for the Coherence JVM to connect out to a listening debugger.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    debug:
      enabled: true               <span class="conum" data-value="1" />
      attach:  "10.10.100.2:5000" <span class="conum" data-value="2" />
      suspend: false              <span class="conum" data-value="3" /></pre>

<ul class="colist">
<li data-value="1">The <code>jvm.debug.enabled</code> flag is set to <code>true</code> to enable debug mode.</li>
<li data-value="2">The <code>jvm.debug.attach</code> field specifies the address of the debugger that the JVM will connect to.</li>
<li data-value="3">The <code>jvm.debug.suspend</code> flag is set to <code>false</code> so that the JVM will start without waiting for a debugger to connect.</li>
</ul>
<p>The example above results in the following arguments being passed to the JVM:</p>

<pre


>-agentlib:jdwp=transport=dt_socket,server=n,address=10.10.100.2:5000,suspend=n,timeout=10000</pre>

</div>
</div>
</doc-view>
<!-- pages/jvm/080_jmx.js -->
<doc-view>

<h2 id="_using_jmx">Using JMX</h2>
<div class="section">
<p>The Java Management Extensions (JMX) are a common way to connect to a JVM and retrieve information from MBeans
attributes or trigger operations by calling MBean methods. By default, the JVM uses RMI as the transport layer for
JMX but RMI can be notoriously tricky to make work in a container environment due to the address and port NAT&#8217;ing
that is typical with containers or clouds. For this reason the Operator supports an alternative transport called JMXMP.
The difference is that JMXMP only requires a single port for communications and this port is simple to configure.</p>

<p>JMXMP is configured using the fields in the <code>jvm.jmxmp</code> section of the <code>Coherence</code> CRD spec.
Enabling JMXMP support adds the <code>opendmk_jmxremote_optional_jar.jar</code> JMXMP library to the classpath and sets the
the Coherence MBean server factory to produce a JMXMP MBean server. By default, the JMXMP server will bind
to port 9099 in the container but this can be configured to bind to a different port.</p>

<div class="admonition note">
<p class="admonition-inline">Using a custom transport for JMX, such as JMXMP, requires any JMX client that will connect to the JMX server to
also have a JMXMP library on its classpath.</p>
</div>
<p>See the <router-link to="/management/030_visualvm">VisualVM Example</router-link> for a detailed example of how to configure
JMX and connect to a server in a <code>Coherence</code> resource.</p>

<p>Example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    jmxmp:
      enabled: true  <span class="conum" data-value="1" />
      port: 9099     <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">JMXMP is enabled so that a JMXMP server will be started in the Coherence container&#8217;s JVM</li>
<li data-value="2">The port that the JMX server will bind to in the container is 9099</li>
</ul>
</div>
</doc-view>
<!-- pages/jvm/090_container_limits.js -->
<doc-view>

<h2 id="_respect_container_resource_limits">Respect Container Resource Limits</h2>
<div class="section">
<p>The JVM can be configured to respect container limits set, for example cpu and memory limits.
This can be important if container limits have been set for the container in the <code>resources</code> section as a JVM that
does not respect these limits can cause the <code>Pod</code> to be killed.
This is done by adding the <code>-XX:+UseContainerSupport</code> JVM option.
It is possible to control this using the <code>jvm.useContainerLimits</code> field in the <code>Coherence</code> CRD spec.
If the field is not set, the operator adds the <code>-XX:+UseContainerSupport</code> option by default.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  jvm:
    useContainerLimits: false   <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>useContainerLimits</code> field is set to false, so the <code>-XX:+UseContainerSupport</code> will not be passed to the JVM.</li>
</ul>
<p>See the <router-link to="/other/100_resources">Resource Limits</router-link> documentation on how to specify resource limits
for the Coherence container.</p>

</div>
</doc-view>
<!-- pages/logging/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>In a container environment like Kubernetes, or any cloud, it is often a requirement to centralize log files
to allow easier analysis and debugging. There are many ways to do this, including collecting container logs,
parsing and shipping log files with something like Fluentd, or using a specialized log appender specific to
your logging framework.</p>

<p>The Coherence Operator does not proscribe any particular method of log capture. The <code>Coherence</code> CRD is flexible
enough to allow any method of log capture that an application or specific cloud environment requires.
This could be as simple as adding JVM arguments to configure the Java logger, or it could be injecting a whole
side-car container to run something like Fluentd. Different approaches have their own pros and cons that need
to be weighed up on a case by case basis.</p>


<h3 id="_logging_guides">Logging Guides</h3>
<div class="section">
<p>The use of Elasticsearch, Fluentd and Kibana is a common approach. For this reason the Coherence Operator
has a set of Kibana dashboards that support the common Coherence logging format.
The logging guides below show one approach to shipping Coherence logs to Elasticsearch and importing the Coherence
dashboards into Kibana.
If this approach does not meet your needs you are obviously free to configure an alternative.</p>

<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/logging/020_logging"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Enabling Log Capture</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Capturing and viewing Coherence cluster Logs in Elasticsearch using a Fluentd side-car.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/logging/030_kibana"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Kibana Dashboards</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Importing and using the Kibana Dashboards available.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</div>
</doc-view>
<!-- pages/logging/020_logging.js -->
<doc-view>

<h2 id="_log_capture_with_fluentd">Log Capture with Fluentd</h2>
<div class="section">
<p>There are many ways to capture container logs in Kubernetes, one possibility that this guide will cover is using
a Fluentd side-car container to ship log files to Elasticsearch.
This is a common pattern and one the the <code>Coherence</code> CRD makes simple by allowing easy injection of additional containers.</p>

<div class="admonition note">
<p class="admonition-inline">This guide is going to assume that the default logging related configurations provided by the operator will
be used. For example, Coherence will be configured to use Java util logging for logs, and the default logging configuration
file will be used. Whilst these things are not pre-requisites for shipping logs to Elasticsearch they are required
to make the examples below work.</p>
</div>
<p>To be able to send Coherence logs to Elasticsearch there are some steps that must be completed:</p>

<ul class="ulist">
<li>
<p>Configure Coherence to log to files</p>

</li>
<li>
<p>Add a <code>Volume</code> and <code>VolumeMount</code> to be used for log files</p>

</li>
<li>
<p>Add the Fluentd side-car container</p>

</li>
</ul>

<h3 id="_configure_coherence_to_log_to_files">Configure Coherence to Log to Files</h3>
<div class="section">
<p>Coherence will log to the console by default so to be able to ship logs to Elasticsearch it needs to be configured
to write to log files. One way to do this is to add a Java Util Logging configuration file and then to configure
Coherence to use the JDK logger.</p>

<p>In the <code>jvm.args</code> section of the <code>Coherence</code> CRD the system properties should be added to set the configuration file used by Java util logging and to configure Coherence logging.
See the Coherence <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/develop-applications/operational-configuration-elements.html">Logging Config</a>
documentation for more details.</p>

<p>There are alternative ways to configure the Java util logger besides using a configuration file, just as there are
alternative logging frameworks that Coherence can be configured to use to produce log files.
This example is going to use Java util logging as that is the simplest to demonstrate without requiring any additional
logging libraries.</p>


<h4 id="_operator_provided_logging_configuration_file">Operator Provided Logging Configuration File</h4>
<div class="section">
<p>Whilst any valid Java util logging configuration file may be used, the Coherence Operator injects a default logging
configuration file into the <code>coherence</code> container that can be used to configure the logger to write
logs to files under the <code>/logs</code> directory. The log files will have the name <code>coherence-%g.log</code>, where <code>%g</code> is the
log file generation created as logs get rotated.</p>

<p>This file will be injected into the container at <code>/coherence-operator/utils/logging/logging.properties</code>
and will look something like this:</p>

<pre


>com.oracle.coherence.handlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler

com.oracle.coherence.level=FINEST

java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter
java.util.logging.ConsoleHandler.level=FINEST

java.util.logging.FileHandler.pattern=/logs/coherence-%g.log
java.util.logging.FileHandler.limit=10485760
java.util.logging.FileHandler.count=50
java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter

java.util.logging.SimpleFormatter.format=%5$s%6$s%n</pre>

<p>To configure Cohrence and the logger some system properties need to be added to the <code>jvm.args</code> field
of the <code>Coherence</code> CRD spec:</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: cluster-one
spec:
  jvm:
    args:
      - "-Dcoherence.log=jdk"                                                                   <span class="conum" data-value="1" />
      - "-Dcoherence.log.logger=com.oracle.coherence"                                           <span class="conum" data-value="2" />
      - "-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties"  <span class="conum" data-value="3" /></pre>

<ul class="colist">
<li data-value="1">Coherence has been configured to use the Java util logging.</li>
<li data-value="2">The Coherence logger name has been set to <code>com.oracle.coherence</code>, which matches the logging configuration file.</li>
<li data-value="3">The Java util logging configuration file is set to the file injected by the Operator.</li>
</ul>
</div>

<h4 id="_log_files_volume">Log Files Volume</h4>
<div class="section">
<p>The logging configuration above configures Coherence to write logs to the <code>/logs</code> directory.
For this location to be accessible to both the <code>coherence</code> container and to the <code>fluentd</code> container it needs to be
created as a <code>Volume</code> in the <code>Pod</code> and mounted to both containers.
As this <code>Volume</code> can be ephemeral and is typically not required to live longer than the <code>Pod</code> the simplest type of
<code>Volume</code> to use is an <code>emptyDir</code> volume source.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: cluster-one
spec:
  jvm:
    args:
      - "-Dcoherence.log=jdk"
      - "-Dcoherence.log.logger=com.oracle.coherence"
      - "-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties"
  volumes:
    - name: logs           <span class="conum" data-value="1" />
      emptyDir: {}
  volumeMounts:
    - name: logs           <span class="conum" data-value="2" />
      mountPath: /logs</pre>

<ul class="colist">
<li data-value="1">An additional empty-dir <code>Volume</code> named <code>logs</code> has been added to the <code>Coherence</code> spec.</li>
<li data-value="2">The <code>logs</code> volume will be mounted at <code>/logs</code> in all containers in the <code>Pod</code>.</li>
</ul>
</div>
</div>

<h3 id="_add_the_fluentd_side_car">Add the Fluentd Side-Car</h3>
<div class="section">
<p>With Coherence configured to write to log files, and those log files visible to other containers in the <code>Pod</code> the
Fluentd side-car container can be added.</p>

<p>The example yaml below shows a <code>Coherence</code> resource with the additional Fluentd container added.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: cluster-one
spec:
  jvm:
    args:
      - "-Dcoherence.log=jdk"
      - "-Dcoherence.log.logger=com.oracle.coherence"
      - "-Djava.util.logging.config.file=/coherence-operator/utils/logging/logging.properties"
  volumes:
    - name: logs           <span class="conum" data-value="1" />
      emptyDir: {}
  volumeMounts:
    - name: logs           <span class="conum" data-value="2" />
      mountPath: /logs
  sideCars:
    - name: fluentd                                                                    <span class="conum" data-value="1" />
      image: "fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3"
      args:
        - "-c"
        - "/etc/fluent.conf"
      env:
        - name: "FLUENTD_CONF"                          <span class="conum" data-value="2" />
          value: "fluentd-coherence.conf"
        - name: "FLUENT_ELASTICSEARCH_SED_DISABLE"      <span class="conum" data-value="3" />
          value: "true"
  configMapVolumes:
    - name: "efk-config"                                <span class="conum" data-value="4" />
      mountPath: "/fluentd/etc/fluentd-coherence.conf"
      subPath: "fluentd-coherence.conf"</pre>

<ul class="colist">
<li data-value="1">The <code>fluentd</code> container has been added to the <code>sideCars</code> list. This will create another container
in the <code>Pod</code> exactly as configured.</li>
<li data-value="2">The <code>FLUENTD_CONF</code> environment variable has been set to the name of the configuration file that Fluentd should use.
The standard Fluentd behaviour is to locate this file in the <code>/fluentd/etc/</code> directory.</li>
<li data-value="3">The <code>FLUENT_ELASTICSEARCH_SED_DISABLE</code> environment variable has been set to work around a known issue <a id="" title="" target="_blank" href="https://github.com/fluent/fluentd-kubernetes-daemonset#disable-sed-execution-on-elasticsearch-image">here</a>.</li>
<li data-value="4">An additional volume has been added from a <code>ConfigMap</code> named <code>efk-config</code>, that contains the Fluentd configuration to use.
This will be mounted to the <code>fluentd</code> container at <code>/fluentd/etc/fluentd-coherence.conf</code>, which corresponds to the
name of the file set in the <code>FLUENTD_CONF</code> environment variable.</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">There is no need to add a <code>/logs</code> volume mount to the <code>fluentd</code> container. The operator will mount the <code>logs</code>
<code>Volume</code> to <strong>all</strong> containers in the <code>Pod</code>.</p>
</div>
<p>In the example above the Fluentd configuration has been provided from a <code>ConfigMap</code>. It could just as easily have come from a
<code>Secret</code> or some other external <code>Volume</code> mount, or it could have been baked into the Fluentd image to be used.</p>


<h4 id="_the_fluentd_configuration_file">The Fluentd Configuration File</h4>
<div class="section">
<p>The <code>ConfigMap</code> used to provide the Fluentd configuration might look something like this:</p>

<pre
lang="yaml"

>apiVersion: v1
kind: ConfigMap
metadata:
  name: efk-config                              <span class="conum" data-value="1" />
  labels:
    component: coherence-efk-config
data:
  fluentd-coherence.conf: |
    # Ignore fluentd messages
    &lt;match fluent.**&gt;
      @type null
    &lt;/match&gt;

    # Coherence Logs
    &lt;source&gt;                                    <span class="conum" data-value="2" />
      @type tail
      path /logs/coherence-*.log
      pos_file /tmp/cohrence.log.pos
      read_from_head true
      tag coherence-cluster
      multiline_flush_interval 20s
      &lt;parse&gt;
       @type multiline
       format_firstline /^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3}/
       format1 /^(?&lt;time&gt;\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3})\/(?&lt;uptime&gt;[0-9\.]+) (?&lt;product&gt;.+) &lt;(?&lt;level&gt;[^\s]+)&gt; \(thread=(?&lt;thread&gt;.+), member=(?&lt;member&gt;.+)\):[\S\s](?&lt;log&gt;.*)/
      &lt;/parse&gt;
    &lt;/source&gt;

    &lt;filter coherence-cluster&gt;                  <span class="conum" data-value="3" />
     @type record_transformer
     &lt;record&gt;
       cluster "#{ENV['COH_CLUSTER_NAME']}"
       role "#{ENV['COH_ROLE']}"
       host "#{ENV['HOSTNAME']}"
       pod-uid "#{ENV['COH_POD_UID']}"
     &lt;/record&gt;
    &lt;/filter&gt;

    &lt;match coherence-cluster&gt;                   <span class="conum" data-value="4" />
      @type elasticsearch
      hosts "http://elasticsearch-master:9200"
      logstash_format true
      logstash_prefix coherence-cluster
    &lt;/match&gt;</pre>

<ul class="colist">
<li data-value="1">The name of the <code>ConfigMap</code> is <code>efk-config</code> to match the name specified in the <code>Coherence</code> CRD spec.</li>
<li data-value="2">The <code>source</code> section is configured to match log files with the name <code>/logs/coherence-*.log</code>, which is the name that
Coherence logging has been configured to use. The pattern in the <code>source</code> section is a Fluentd pattern that matches the
standard Coherence log message format.</li>
<li data-value="3">A <code>filter</code> section will add additional fields to the log message. These come from the environment variables that
the Operator will inject into all containers in the Pod. In this case the Coherence cluster name, the Coherence role name,
the Pod host name and Pod UID.</li>
<li data-value="4">The final section tells Fluentd how to ship the logs to Elasticsearch, in this case to the endpoint <code><a id="" title="" target="_blank" href="http://elasticsearch-master:9200">http://elasticsearch-master:9200</a></code></li>
</ul>
<p>There are many ways to configure Fluentd, the example above is just one way and is in fact taken from one of the Operator&#8217;s functional tests.</p>

<p>With the <code>efk-config</code> <code>ConfigMap</code> created in the same namespace as the <code>Coherence</code> resource the Coherence logs from the
containers will now be shipped to Elasticsearch.</p>

</div>
</div>
</div>
</doc-view>
<!-- pages/logging/030_kibana.js -->
<doc-view>

<h2 id="_using_kibana_dashboards">Using Kibana Dashboards</h2>
<div class="section">
<p>Kibana is often used to anyalze log files that have been collected into Elasticsearch.
The Coherence Operator provides a number of Kibana dashboards and queries
to allow you to view and analyze logs from your Coherence clusters.</p>


<h3 id="_importing_kibana_dashboards">Importing Kibana Dashboards</h3>
<div class="section">
<p>The Kibana dashboard files are located in the Coherence operator source in the <code>dashboards/kibana</code> directory.</p>

<p>The method of importing the dashboards into Kibana will depend on how Kibana is being run.
The simplest method is just to import the json file using the Kibana web UI.
An alternative approach is to load the dashboard into a <code>ConfigMap</code> in Kubernetes that is mounted into the Kibana Pod
and then trigger an import when Kibana starts.
As there are many ways to do this depending on the specifics of the version of Kibana being used,
exact instructions are beyond the scope fo this guide.</p>

</div>

<h3 id="_kibana_dashboards_searches">Kibana Dashboards &amp; Searches</h3>
<div class="section">

</div>

<h3 id="_table_of_contents">Table of Contents</h3>
<div class="section">
<ol style="margin-left: 15px;">
<li>
<router-link to="#dashboards" @click.native="this.scrollFix('#dashboards')">Dashboards</router-link>
<ol style="margin-left: 15px;">
<li>
<router-link to="#all" @click.native="this.scrollFix('#all')">Coherence Cluster - All Messages</router-link>

</li>
<li>
<router-link to="#errors" @click.native="this.scrollFix('#errors')">Coherence Cluster - Errors and Warnings</router-link>

</li>
<li>
<router-link to="#persistence" @click.native="this.scrollFix('#persistence')">Coherence Cluster - Persistence</router-link>

</li>
<li>
<router-link to="#config" @click.native="this.scrollFix('#config')">Coherence Cluster - Configuration Messages</router-link>

</li>
<li>
<router-link to="#network" @click.native="this.scrollFix('#network')">Coherence Cluster - Network</router-link>

</li>
<li>
<router-link to="#partitions" @click.native="this.scrollFix('#partitions')">Coherence Cluster - Partitions</router-link>

</li>
<li>
<router-link to="#sources" @click.native="this.scrollFix('#sources')">Coherence Cluster - Message Sources</router-link>

</li>
</ol>
</li>
<li>
<router-link to="#searches" @click.native="this.scrollFix('#searches')">Searches</router-link>

</li>
</ol>
</div>

<h3 id="dashboards">Dashboards</h3>
<div class="section">
<p>Information from all dashboards (and queries) can be filtered using the standard Kibana date/time
filtering in the top right of the UI, as well as the <code>Add a filter</code> button.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-filters.png" alt="Filters"width="600" />
</v-card-text>
</v-card>


<h4 id="all">1. Coherence Cluster - All Messages</h4>
<div class="section">
<p>This dashboard shows all messages captured for the given time period for all clusters.</p>

<p>Users can drill-down by cluster, host, message level and thread.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-all-messages.png" alt="All messages"width="900" />
</v-card-text>
</v-card>

</div>

<h4 id="errors">2. Coherence Cluster - Errors and Warnings</h4>
<div class="section">
<p>This dashboard shows errors and warning messages only.</p>

<p>Users can drill-down by cluster, host, message level and thread.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-errors-warnings.png" alt="Errors and Warnings"width="900" />
</v-card-text>
</v-card>

</div>

<h4 id="persistence">3. Coherence Cluster - Persistence</h4>
<div class="section">
<p>This dashboard shows Persistence related messages including failed and successful operations.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-persistence.png" alt="Persistence"width="900" />
</v-card-text>
</v-card>

</div>

<h4 id="config">4. Coherence Cluster - Configuration Messages</h4>
<div class="section">
<p>This dashboard shows configuration related messages such as loading of operational, cache configuration
and POF configuration files.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-configuration.png" alt="Configuration"width="900" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="network">5. Coherence Cluster - Network</h3>
<div class="section">
<p>This dashboard hows network related messages, such as communication delays and TCP ring disconnects.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-network.png" alt="Network"width="900" />
</v-card-text>
</v-card>


<h4 id="partitions">6. Coherence Cluster - Partitions</h4>
<div class="section">
<p>Shows partition transfer and partition loss messages.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-partitions.png" alt="Partitions"width="900" />
</v-card-text>
</v-card>

</div>

<h4 id="sources">7. Coherence Cluster - Message Sources</h4>
<div class="section">
<p>Shows the source (thread) for messages</p>

<p>Users can drill-down by cluster, host and message level.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-message-sources.png" alt="Sources"width="900" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="searches">Searches</h3>
<div class="section">
<p>A number of searches are automatically includes which can help assist in
diagnosis and troubleshooting a Coherence cluster. They can be accessed via the <code>Discover</code> <code>side-bar
and selecting `Open</code>.</p>



<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/kibana-search.png" alt="Search"width="700" />
</v-card-text>
</v-card>

<p>These are grouped into the following general categories:</p>

<ul class="ulist">
<li>
<p>Cluster - Cluster join, discovery, heartbeat, member joining and stopping messages</p>

</li>
<li>
<p>Cache - Cache restarting, exceptions and index exception messages</p>

</li>
<li>
<p>Configuration - Configuration loading and not loading messages</p>

</li>
<li>
<p>Persistence - Persistence success and failure messages</p>

</li>
<li>
<p>Network - Network communications delays, disconnects, timeouts and terminations</p>

</li>
<li>
<p>Partition - Partition loss, ownership and transfer related messages</p>

</li>
<li>
<p>Member - Member thread dump, join and leave messages</p>

</li>
<li>
<p>Errors - All Error messages only</p>

</li>
<li>
<p>Federation - Federation participant, disconnection, connection, errors and other messages</p>

</li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/management/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>There are a number of management and diagnostic features available with Oracle Coherence that can be used
in a cluster deployed in Kubernetes.</p>

<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/management/020_management_over_rest"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Management Over REST</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Coherence Management over REST feature</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/management/030_visualvm"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">VisualVM</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Coherence VisualVM plugin.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/management/040_ssl"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">SSL</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Enable SSL on the Management over REST endpoint.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</doc-view>
<!-- pages/management/020_management_over_rest.js -->
<doc-view>

<h2 id="_management_over_rest">Management over REST</h2>
<div class="section">
<p>Since version 12.2.1.4 Coherence has had functionality to expose a management API over REST.</p>

<div class="admonition note">
<p class="admonition-inline">The Management over REST  API is <strong>disabled</strong> by default in Coherence clusters but can be enabled and configured by
setting the relevant fields in the <code>Coherence</code> CRD.</p>
</div>
<p>The example below shows how to enable and access Coherence MBeans using Management over REST.</p>

<p>Once the Management port has been exposed, for example via a load balancer or port-forward command, the REST
endpoint is available at <code><a id="" title="" target="_blank" href="http://host:port/management/coherence/cluster">http://host:port/management/coherence/cluster</a></code>.
The Swagger JSON document for the API is available at <code><a id="" title="" target="_blank" href="http://host:port/management/coherence/cluster/metadata-catalog">http://host:port/management/coherence/cluster/metadata-catalog</a></code>.</p>

<p>See the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/rest-reference/">REST API for Managing Oracle Coherence</a>
documentation for full details on each of the endpoints.</p>

<div class="admonition note">
<p class="admonition-inline">Note: Use of Management over REST is available only when using the operator with clusters running
Coherence 12.2.1.4 or later version.</p>
</div>

<h3 id="_deploy_coherence_with_management_over_rest_enabled">Deploy Coherence with Management over REST Enabled</h3>
<div class="section">
<p>To deploy a <code>Coherence</code> resource with management over REST enabled and exposed on a port, the simplest yaml
would look like this:</p>

<pre
lang="yaml"
title="management-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: management-cluster
spec:
  coherence:
    management:
      enabled: true     <span class="conum" data-value="1" />
  ports:
    - name: management  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">Setting the <code>coherence.management.enabled</code> field to <code>true</code> will enable Management over REST</li>
<li data-value="2">To expose Management over REST via a <code>Service</code> it is added to the <code>ports</code> list.
The <code>management</code> port is a special case where the <code>port</code> number is optional so in this case Management over REST
will bind to the default port <code>30000</code>.
(see <router-link to="/ports/020_container_ports">Exposing Ports</router-link> for details)</li>
</ul>
<p>To expose Management over REST on a different port the alternative port value can be set in the <code>coherence.management</code>
section, for example:</p>

<pre
lang="yaml"
title="management-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: management-cluster
spec:
  coherence:
    management:
      enabled: true
      port: 8080      <span class="conum" data-value="1" />
  ports:
    - name: management</pre>

<ul class="colist">
<li data-value="1">Management over REST will now be exposed on port <code>8080</code></li>
</ul>
</div>

<h3 id="_port_forward_the_management_over_rest_port">Port-forward the Management over REST Port</h3>
<div class="section">
<p>After installing the basic <code>management-cluster.yaml</code> from the first example above there would be a three member
Coherence cluster installed into Kubernetes.</p>

<p>For example, the cluster can be installed with <code>kubectl</code></p>

<pre
lang="bash"

>kubectl -n coherence-test create -f management-cluster.yaml

coherence.coherence.oracle.com/management-cluster created</pre>

<p>The <code>kubectl</code> CLI can be used to list <code>Pods</code> for the cluster:</p>

<pre
lang="bash"

>kubectl -n coherence-test get pod -l coherenceCluster=management-cluster

NAME                   READY   STATUS    RESTARTS   AGE
management-cluster-0   1/1     Running   0          36s
management-cluster-1   1/1     Running   0          36s
management-cluster-2   1/1     Running   0          36s</pre>

<p>In a test or development environment the simplest way to reach an exposed port is to use the <code>kubectl port-forward</code> command.
For example to connect to the first <code>Pod</code> in the deployment:</p>

<pre
lang="bash"

>kubectl -n coherence-test port-forward management-cluster-0 30000:30000

Forwarding from [::1]:30000 -&gt; 30000
Forwarding from 127.0.0.1:30000 -&gt; 30000</pre>

</div>

<h3 id="_access_the_rest_endpoint">Access the REST Endpoint</h3>
<div class="section">
<p>Now that a port is being forwarded from localhost to a <code>Pod</code> in the cluster the Management over REST endpoints can be accessed.</p>

<p>Issue the following <code>curl</code> command to access the REST endpoint:</p>

<pre
lang="bash"

>curl http://127.0.0.1:30000/management/coherence/cluster/</pre>

<p>Which should result in a response similar to the following:</p>

<pre
lang="json"

>{
  "links": [
    {
      "rel": "parent",
      "href": "http://127.0.0.1:30000/management/coherence"
    },
    {
      "rel": "self",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/"
    },
    {
      "rel": "canonical",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/"
    },
    {
      "rel": "services",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/services"
    },
    {
      "rel": "caches",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/caches"
    },
    {
      "rel": "members",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/members"
    },
    {
      "rel": "management",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/management"
    },
    {
      "rel": "journal",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/journal"
    },
    {
      "rel": "hotcache",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/hotcache"
    },
    {
      "rel": "reporters",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/reporters"
    },
    {
      "rel": "webApplications",
      "href": "http://127.0.0.1:30000/management/coherence/cluster/webApplications"
    }
  ],
  "clusterSize": 3,
  "membersDeparted": [],
  "memberIds": [
    1,
    2,
    3
  ],
  "oldestMemberId": 1,
  "refreshTime": "2019-10-15T03:55:46.461Z",
  "licenseMode": "Development",
  "localMemberId": 1,
  "version": "14.1.1.0.0",
  "running": true,
  "clusterName": "management-cluster",
  "membersDepartureCount": 0,
  "members": [
    "Member(Id=1, Timestamp=2019-10-15 03:46:15.848, Address=10.1.2.184:36531, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-1, Role=storage)",
    "Member(Id=2, Timestamp=2019-10-15 03:46:19.405, Address=10.1.2.183:40341, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-2, Role=storage)",
    "Member(Id=3, Timestamp=2019-10-15 03:46:19.455, Address=10.1.2.185:38719, MachineId=49519, Location=site:coherence.coherence-test.svc.cluster.local,machine:docker-desktop,process:1,member:management-cluster-0, Role=storage)"
  ],
  "type": "Cluster"
}</pre>

</div>

<h3 id="_access_the_swagger_endpoint">Access the Swagger Endpoint</h3>
<div class="section">
<p>Issue the following <code>curl</code> command to access the Sagger endpoint, which documents all the REST API&#8217;s available.</p>

<pre
lang="bash"

>curl http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog</pre>

<p>Which should result in a response like the following:</p>

<pre
lang="json"

>{
  "swagger": "2.0",
  "info": {
    "title": "RESTful Management Interface for Oracle Coherence MBeans",
    "description": "RESTful Management Interface for Oracle Coherence MBeans",
    "version": "14.1.1.0.0"
  },
  "schemes": [
    "http",
    "https"
  ],
...</pre>

<div class="admonition note">
<p class="admonition-inline">The above output has been truncated due to the large size.</p>
</div>
</div>

<h3 id="_other_rest_resources">Other REST Resources</h3>
<div class="section">
<p>Management over REST can be used for all Coherence management functions, the same as would be available when using
standard MBean access over JMX.</p>

<p>Please see the
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/rest-reference/">Coherence REST API</a> for more information on these features.</p>

<ul class="ulist">
<li>
<p><a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/manage/using-jmx-manage-oracle-coherence.html#GUID-D160B16B-7C1B-4641-AE94-3310DF8082EC">Connecting JVisualVM to Management over REST</a></p>

</li>
<li>
<p><router-link to="#clusters/058_coherence_management.adoc" @click.native="this.scrollFix('#clusters/058_coherence_management.adoc')">Enabling SSL</router-link></p>

</li>
<li>
<p><a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/rest-reference/op-management-coherence-cluster-members-memberidentifier-diagnostic-cmd-jfrcmd-post.html">Produce and extract a Java Flight Recorder (JFR) file</a></p>

</li>
<li>
<p><a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/rest-reference/api-reporter.html">Access the Reporter</a></p>

</li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/management/030_visualvm.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Using VisualVM</dt>
<dd slot="desc"><p><a id="" title="" target="_blank" href="https://visualvm.github.io/">VisualVM</a> is a visual tool integrating commandline JDK tools and lightweight profiling
capabilities, designed for both development and production time use.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_access_a_coherence_cluster_via_visualvm">Access A Coherence Cluster via VisualVM</h2>
<div class="section">
<p>Coherence management is implemented using Java Management Extensions (JMX). JMX is a Java standard
for managing and monitoring Java applications and services. VisualVM and other JMX tools can be used to
manage and monitor Coherence Clusters via JMX.</p>

<p>The default transport used by JMX is RMI but RMI can be difficult to set-up reliably in containers and Kubernetes so
that it can be accessed externally due to its use of multiple TCP ports that are difficult to configure and it does
not work well with the NAT&#8217;ed type of networking typically found in these environments. JMXMP on the other hand is an
alternative to RMI that does work well in containers and only requires a single TCP port.</p>

<p>This example shows how to connect to a cluster via JMX over JMXMP.</p>

<p>As an alternative to JMX see <router-link to="/management/020_management_over_rest">Management over REST</router-link>
for how to connect to a cluster via the VisualVM plugin using REST.</p>

<div class="admonition note">
<p class="admonition-inline">See the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/manage/introduction-oracle-coherence-management.html">Coherence Management Documentation</a>
for more information on JMX and Management.</p>
</div>

<h3 id="_prerequisites">Prerequisites</h3>
<div class="section">
<ol style="margin-left: 15px;">
<li>
Install the Coherence Operator
<p>Ensure you have installed the Coherence Operator using the <router-link to="/installation/01_installation">Install Guide</router-link>.</p>

</li>
<li>
Download the JMXMP connector JAR
<p>The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath
of the Java JMX client (VisualVM and JConsole). You can use curl to download the required JAR.</p>

<pre
lang="bash"

>curl http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar \
    -o opendmk_jmxremote_optional_jar-1.0-b01-ea.jar</pre>

<p>This jar can also be downloaded as a Maven dependency if you are connecting through a Maven project.</p>

<pre
lang="xml"

>&lt;dependency&gt;
  &lt;groupId&gt;org.glassfish.external&lt;/groupId&gt;
  &lt;artifactId&gt;opendmk_jmxremote_optional_jar&lt;/artifactId&gt;
  &lt;version&gt;1.0-b01-ea&lt;/version&gt;
&lt;/dependency&gt;</pre>

</li>
</ol>
</div>

<h3 id="_install_a_jmx_enabled_coherence_cluster">Install a JMX Enabled Coherence Cluster</h3>
<div class="section">
<p>In this example a simple minimal cluster will be created running the MBean server.</p>

<pre
lang="yaml"
title="cluster-with-jmx.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  jvm:
    args:                                                  <span class="conum" data-value="1" />
      - -Dcoherence.management=all
      - -Dcoherence.management.remote=true
      - -Dcom.sun.management.jmxremote.ssl=false
      - -Dcom.sun.management.jmxremote.authenticate=false
    jmxmp:
      enabled: true                                        <span class="conum" data-value="2" />
      port: 9099
  ports:
    - name: jmx                                            <span class="conum" data-value="3" />
      port: 9099</pre>

<ul class="colist">
<li data-value="1">Additional system properties are added to enable Coherence management
See the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/fusion-middleware/coherence/12.2.1.4/manage/introduction-oracle-coherence-management.html">Coherence Management Documentation</a></li>
<li data-value="2">JMXMP is enabled on port <code>9099</code> so that a reliable JMX connection can be made to the MBean server from outside the <code>Pods</code></li>
<li data-value="3">The JMXMP port is exposed as an additional port</li>
</ul>
<p>The example <code>cluster-with-jmx.yaml</code> can be installed into Kubernetes with the following command:</p>

<pre
lang="bash"

>kubectl -n coherence-test apply -f cluster-with-jmx.yaml</pre>

<p>This should install the cluster into the namespace <code>coherence-test</code> with a default replica count of three, resulting in
a <code>StatefulSet</code> with three <code>Pods</code>.</p>

</div>

<h3 id="_port_forward_the_mbean_server_pod">Port Forward the MBean Server Pod:</h3>
<div class="section">
<p>After installing the basic <code>cluster-with-jmx.yaml</code> from the example above there would be a three member
Coherence cluster installed into Kubernetes.</p>

<p>The <code>kubectl</code> CLI can be used to list <code>Pods</code> for the cluster:</p>

<pre
lang="bash"

>kubectl -n coherence-test get pod -l coherenceCluster=test-cluster

NAME             READY   STATUS    RESTARTS   AGE
test-cluster-0   1/1     Running   0          36s
test-cluster-1   1/1     Running   0          36s
test-cluster-2   1/1     Running   0          36s</pre>

<p>In a test or development environment the simplest way to reach an exposed port is to use the <code>kubectl port-forward</code> command.
For example to connect to the first <code>Pod</code> in the deployment:</p>

<pre
lang="bash"

>kubectl -n coherence-test port-forward test-cluster-0 9099:9099

Forwarding from [::1]:9099 -&gt; 9099
Forwarding from 127.0.0.1:9099 -&gt; 9099</pre>

<p>JMX can now be access using the URL <code>service:jmx:jmxmp://127.0.0.1:9099</code></p>

</div>

<h3 id="_access_mbeans_through_jconsole">Access MBeans Through JConsole</h3>
<div class="section">
<ol style="margin-left: 15px;">
<li>
Run JConsole with the JMXMP connector on the classpath:
<pre
lang="bash"

>jconsole -J-Djava.class.path="$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar" service:jmx:jmxmp://127.0.0.1:9099</pre>

</li>
<li>
In the console UI, select the <code>MBeans</code> tab and then <code>Coherence Cluster</code> attributes.
You should see the Coherence MBeans as shown below:
<p><img src="./images/jconsole.png" alt="VisualVM"width="513" />
</p>

</li>
</ol>
</div>

<h3 id="_access_mbeans_through_visualvm">Access MBeans Through VisualVM</h3>
<div class="section">
<ol style="margin-left: 15px;">
<li>
Ensure you run VisualVM with the JMXMP connector on the classpath:
<pre
lang="bash"

>jvisualvm -cp "$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar"</pre>

<div class="admonition note">
<p class="admonition-inline">If you have downloaded VisualVM separately (as VisualVM has not been part of the JDK from Java 9 onwards),
then the executable is <code>visualvm</code> (or on MacOS it is <code>/Applications/VisualVM.app/Contents/MacOS/visualvm</code>).</p>
</div>
</li>
<li>
From the VisualVM menu select <code>File</code> / <code>Add JMX Connection</code>

</li>
<li>
Enter <code>service:jmx:jmxmp://127.0.0.1:9099</code> for the <code>Connection</code> value and click <code>OK</code>.
<p>A JMX connection should be added under the <code>Local</code> section of the left hand panel.</p>

</li>
<li>
Double-click the new local connection to connect to the management <code>Pod</code>.
You can see the <code>Coherence</code> MBeans under the <code>MBeans</code> tab.
If you have installed the Coherence VisualVM plugin, you can also see a <code>Coherence</code> tab.
<p><img src="./images/jvisualvm.png" alt="VisualVM"width="735" />
</p>

</li>
</ol>
<p>Refer to the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/manage/oracle-coherence-mbeans-reference.html#GUID-5E57FA4D-9CF8-4069-A8FD-B50E4FAB2687">Coherence MBean Reference</a>
for detailed information about Coherence MBeans.</p>

</div>
</div>
</doc-view>
<!-- pages/management/040_ssl.js -->
<doc-view>

<h2 id="_ssl_with_management_over_rest">SSL with Management over REST</h2>
<div class="section">
<p>It is possible to configure Management over REST endpoint to use SSL to secure the communication between server and
client. The SSL configuration is in the <code>coherence.metrics.ssl</code> section of the CRD spec.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  coherence:
    management:
      enabled: true
      ssl:
        enabled: true                            <span class="conum" data-value="1" />
        keyStore: metrics-keys.jks               <span class="conum" data-value="2" />
        keyStoreType: JKS                        <span class="conum" data-value="3" />
        keyStorePasswordFile: store-pass.txt     <span class="conum" data-value="4" />
        keyPasswordFile: key-pass.txt            <span class="conum" data-value="5" />
        keyStoreProvider:                        <span class="conum" data-value="6" />
        keyStoreAlgorithm: SunX509               <span class="conum" data-value="7" />
        trustStore: metrics-trust.jks            <span class="conum" data-value="8" />
        trustStoreType: JKS                      <span class="conum" data-value="9" />
        trustStorePasswordFile: trust-pass.txt   <span class="conum" data-value="10" />
        trustStoreProvider:                      <span class="conum" data-value="11" />
        trustStoreAlgorithm: SunX509             <span class="conum" data-value="12" />
        requireClientCert: true                  <span class="conum" data-value="13" />
        secrets: metrics-secret                  <span class="conum" data-value="14" /></pre>

<ul class="colist">
<li data-value="1">The <code>enabled</code> field when set to <code>true</code> enables SSL for metrics or when set to <code>false</code> disables SSL</li>
<li data-value="2">The <code>keyStore</code> field sets the name of the Java key store file that should be used to obtain the server&#8217;s key</li>
<li data-value="3">The optional <code>keyStoreType</code> field sets the type of the key store file, the default value is <code>JKS</code></li>
<li data-value="4">The optional <code>keyStorePasswordFile</code> sets the name of the text file containing the key store password</li>
<li data-value="5">The optional <code>keyPasswordFile</code> sets the name of the text file containing the password of the key in the key store</li>
<li data-value="6">The optional <code>keyStoreProvider</code> sets the provider name for the key store</li>
<li data-value="7">The optional <code>keyStoreAlgorithm</code> sets the algorithm name for the key store, the default value is <code>SunX509</code></li>
<li data-value="8">The <code>trustStore</code> field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key</li>
<li data-value="9">The optional <code>trustStoreType</code> field sets the type of the trust store file, the default value is <code>JKS</code></li>
<li data-value="10">The optional <code>trustStorePasswordFile</code> sets the name of the text file containing the trust store password</li>
<li data-value="11">The optional <code>trustStoreProvider</code> sets the provider name for the trust store</li>
<li data-value="12">The optional <code>trustStoreAlgorithm</code> sets the algorithm name for the trust store, the default value is <code>SunX509</code></li>
<li data-value="13">The optional <code>requireClientCert</code> field if set to <code>true</code> enables two-way SSL where the client must also provide
a valid certificate</li>
<li data-value="14">The optional <code>secrets</code> field sets the name of the Kubernetes <code>Secret</code> to use to obtain the key store, truct store
and password files from.</li>
</ul>
<p>The various files and keystores referred to in the configuration above can be any location accessible in the image
used by the <code>coherence</code> container in the deployment&#8217;s <code>Pods</code>. Typically, for things such as SSL keys and certs,
these would be provided by obtained from <code>Secrets</code> loaded as additional <code>Pod</code> <code>Volumes</code>.
See <router-link to="/other/060_secret_volumes">Add Secrets Volumes</router-link> for the documentation on how to specify
secrets as additional volumes.</p>

</div>
</doc-view>
<!-- pages/management/100_tmb_test.js -->
<doc-view>

<h2 id="_coherence_network_testing">Coherence Network Testing</h2>
<div class="section">
<p>Coherence provides utilities that can be used to test network performance, which obviously has a big impact on
a distributed system such as Coherence. The documentation for these utilities can be found in the official
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/administer/performing-network-performance-test.html#GUID-7267AB06-6353-416E-B9FD-A75F7FBFE523">Coherence Documentation</a>.</p>

<p>Whilst generally these tests would be run on server hardware, with more and more Coherence deployments moving into the
cloud and into Kubernetes these tests can also be performed in <code>Pods</code> to measure inter-Pod network performance.
This test can be used to see the impact of running <code>Pods</code> across different zones, or on different types of Kubernetes
networks, with different <code>Pod</code> resource settings, etc.</p>

</div>

<h2 id="_run_the_message_bus_test_in_pods">Run the Message Bus Test in Pods</h2>
<div class="section">
<p>The message bus test can easily be run using <code>Pods</code> in Kubernetes.
Using the example from the Coherence documentation there will need to be two <code>Pods</code>, a listener and a sender.
This example will create a <code>Service</code> for the listener so that the sender <code>Pod</code> can use the <code>Service</code> name
to resolve the listener <code>Pod</code> address.</p>


<h3 id="_run_the_listener_pod">Run the Listener Pod</h3>
<div class="section">
<p>Create a <code>yaml</code> file that will create the <code>Service</code> and <code>Pod</code> for the listener:</p>

<pre
lang="yaml"
title="message-bus-listener.yaml"
>apiVersion: v1
kind: Service
metadata:
  name: message-bus-listener
spec:
  selector:
    app: message-bus-listener
  ports:
  - protocol: TCP
    port: 8000
    targetPort: mbus
---
apiVersion: v1
kind: Pod
metadata:
  name: message-bus-listener
  labels:
    app: message-bus-listener
spec:
  restartPolicy: Never
  containers:
    - name: coherence
      image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0  <span class="conum" data-value="1" />
      ports:
        - name: mbus
          containerPort: 8000
          protocol: TCP
      command:
        - java                                                   <span class="conum" data-value="2" />
        - -cp
        - /u01/oracle/oracle_home/coherence/lib/coherence.jar
        - com.oracle.common.net.exabus.util.MessageBusTest
        - -bind
        - tmb://0.0.0.0:8000</pre>

<ul class="colist">
<li data-value="1">This example uses the Coherence image from Oracle container registry, but any image with <code>coherence.jar</code> in it
could be used.</li>
<li data-value="2">The command line that the container will execute is exactly the same as that for the listener process in the
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/administer/performing-network-performance-test.html#GUID-7267AB06-6353-416E-B9FD-A75F7FBFE523">Coherence Documentation</a>.</li>
</ul>
<p>Start the listener <code>Pod</code>:</p>

<pre
lang="bash"

>kubectl create -f message-bus-listener.yaml</pre>

<p>Retrieving the logs for the listener <code>Pod</code> the messages should show that the <code>Pod</code> has started:</p>

<pre
lang="bash"

>kubectl logs pod/message-bus-listener
OPEN event for tmb://message-bus-listener:8000</pre>

</div>

<h3 id="_run_the_sender_pod">Run the Sender Pod</h3>
<div class="section">
<pre
lang="yaml"
title="message-bus-sender.yaml"
>apiVersion: v1
kind: Pod
metadata:
  name: message-bus-sender
  labels:
    app: message-bus-sender
spec:
  restartPolicy: Never
  containers:
    - name: coherence
      image: container-registry.oracle.com/middleware/coherence:14.1.1.0.0
      command:
        - java                         <span class="conum" data-value="1" />
        - -cp
        - /u01/oracle/oracle_home/coherence/lib/coherence.jar
        - com.oracle.common.net.exabus.util.MessageBusTest
        - -bind
        - tmb://0.0.0.0:8000
        - -peer
        - tmb://message-bus-listener:8000  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">Again, the command line is the same as that for the sender process in the
<a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/administer/performing-network-performance-test.html#GUID-7267AB06-6353-416E-B9FD-A75F7FBFE523">Coherence Documentation</a>.</li>
<li data-value="2">The <code>peer</code> address uses the <code>Service</code> name <code>message-bus-listener</code> from the sender <code>yaml</code>.</li>
</ul>
<p>Start the sender <code>Pod</code>:</p>

<pre
lang="bash"

>kubectl create -f message-bus-sender.yaml</pre>

<p>Retrieving the logs for the sender <code>Pod</code> the messages should show that the <code>Pod</code> has started and show the test results:</p>

<pre
lang="bash"

>kubectl logs pod/message-bus-sender
OPEN event for tmb://message-bus-sender:8000
CONNECT event for tmb://message-bus-listener:8000 on tmb://message-bus-sender:8000
now:  throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0
now:  throughput(out 34805msg/s 1.14gb/s, in 348msg/s 11.3mb/s), latency(response(avg 25.31ms, effective 110.03ms, min 374.70us, max 158.10ms), receipt 25.47ms), backlog(out 77% 83/s 308KB, in 0% 0/s 0B), connections 1, errors 0</pre>

<div class="admonition note">
<p class="admonition-textlabel">Note</p>
<p ><p>Don&#8217;t forget to stop the <code>Pods</code> after obtaining the results:</p>

<pre
lang="bash"

>kubectl delete -f message-bus-sender.yaml
kubectl delete -f message-bus-listener.yaml</pre>
</p>
</div>
</div>

<h3 id="_run_pods_on_specific_nodes">Run Pods on Specific Nodes</h3>
<div class="section">
<p>In the example above the <code>Pods</code> will be scheduled wherever Kubernetes decides to put them. This could have a big impact
on the test result for different test runs. For example in a Kubernetes cluster that spans zones and data centres, if
the two <code>Pods</code> get scheduled in different data centres this will have worse results than if the two <code>Pods</code> get scheduled
onto the same node.</p>

<p>To get consistent results add node selectors, taints, tolerations etc, as covered in the Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">assign Pods to Nodes</a> documentation.</p>

</div>
</div>
</doc-view>
<!-- pages/metrics/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>Coherence metrics can be used in a cluster deployed in Kubernetes.</p>

<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/metrics/020_metrics"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Metrics</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Enabling the Coherence metrics feature.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/metrics/030_importing"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Importing Grafana Dashboards</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Importing the Coherence Grafana Dashboards.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/metrics/040_dashboards"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Using Grafana Dashboards</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Using the Coherence Grafana Dashboards.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/metrics/050_ssl"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">SSL</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Enable SSL on the metrics endpoint.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</doc-view>
<!-- pages/metrics/020_metrics.js -->
<doc-view>

<h2 id="_publish_metrics">Publish Metrics</h2>
<div class="section">
<p>Since version 12.2.1.4 Coherence has had the ability to expose a http endpoint that can be used to scrape metrics.
This would typically be used to expose metrics to something like Prometheus.</p>

<div class="admonition note">
<p class="admonition-inline">The description below is only applicable if metrics will be served by Coherence using the
<code>coherence-metrics</code> module. If Coherence metrics will be served from a different endpoint, for
example from a Helidon web-server using <code>coherence-mp-metrics</code> or using Coherence Micrometer integration,
then the documentation below does not apply.</p>
</div>
<div class="admonition note">
<p class="admonition-inline">The default metrics endpoint is <strong>disabled</strong> by default in Coherence clusters but can be enabled and configured by
setting the relevant fields in the <code>Coherence</code> CRD. This assumes that your application has included the
<code>coherence-metrics</code> module as a dependency. See the Coherence product documentation for more details on enabling metrics
in your application.</p>
</div>
<p>The example below shows how to enable and access Coherence metrics when served by the endpoint provided by the
<code>coherence-metrics</code> module. For the example below to work the application deployed must have the <code>coherence-metrics</code>
jar file and its dependencies on the classpath.</p>

<p>Once the metrics port has been exposed, for example via a load balancer or port-forward command, the metrics
endpoint is available at <code><a id="" title="" target="_blank" href="http://host:port/metrics">http://host:port/metrics</a></code>.</p>

<p>See the <a id="" title="" target="_blank" href="https://docs.oracle.com/en/middleware/standalone/coherence/14.1.1.0/manage/using-coherence-metrics.html">Using Coherence Metrics</a>
documentation for full details on the available metrics.</p>

<div class="admonition note">
<p class="admonition-inline">Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version.</p>
</div>

<h3 id="_deploy_coherence_with_metrics_enabled">Deploy Coherence with Metrics Enabled</h3>
<div class="section">
<p>To deploy a <code>Coherence</code> resource with metrics enabled and exposed on a port, the simplest yaml
would look like this:</p>

<pre
lang="yaml"
title="metrics-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: metrics-cluster
spec:
  coherence:
    metrics:
      enabled: true     <span class="conum" data-value="1" />
  ports:
    - name: metrics  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">Setting the <code>coherence.metrics.enabled</code> field to <code>true</code> will enable metrics</li>
<li data-value="2">To expose metrics via a <code>Service</code> it is added to the <code>ports</code> list.
The <code>metrics</code> port is a special case where the <code>port</code> number is optional so in this case metrics
will bind to the default port <code>9612</code>.
(see <router-link to="/ports/020_container_ports">Exposing Ports</router-link> for details)</li>
</ul>

<h4 id="_expose_metrics_on_a_different_port">Expose Metrics on a Different Port</h4>
<div class="section">
<p>To expose metrics on a different port the alternative port value can be set in the <code>coherence.metrics</code>
section, for example:</p>

<pre
lang="yaml"
title="metrics-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: metrics-cluster
spec:
  coherence:
    metrics:
      enabled: true
      port: 8080      <span class="conum" data-value="1" />
  ports:
    - name: metrics</pre>

<ul class="colist">
<li data-value="1">metrics will now be exposed on port <code>8080</code></li>
</ul>
</div>
</div>

<h3 id="_port_forward_the_metrics_port">Port-forward the Metrics Port</h3>
<div class="section">
<p>After installing the basic <code>metrics-cluster.yaml</code> from the first example above there would be a three member
Coherence cluster installed into Kubernetes.</p>

<p>For example, the cluster can be installed with <code>kubectl</code></p>

<pre
lang="bash"

>kubectl -n coherence-test create -f metrics-cluster.yaml

coherence.coherence.oracle.com/metrics-cluster created</pre>

<p>The <code>kubectl</code> CLI can be used to list <code>Pods</code> for the cluster:</p>

<pre
lang="bash"

>kubectl -n coherence-test get pod -l coherenceCluster=metrics-cluster

NAME                   READY   STATUS    RESTARTS   AGE
metrics-cluster-0   1/1     Running   0          36s
metrics-cluster-1   1/1     Running   0          36s
metrics-cluster-2   1/1     Running   0          36s</pre>

<p>In a test or development environment the simplest way to reach an exposed port is to use the <code>kubectl port-forward</code> command.
For example to connect to the first <code>Pod</code> in the deployment:</p>

<pre
lang="bash"

>kubectl -n coherence-test port-forward metrics-cluster-0 9612:9612

Forwarding from [::1]:9612 -&gt; 9612
Forwarding from 127.0.0.1:9612 -&gt; 9612</pre>

</div>

<h3 id="_access_the_metrics_endpoint">Access the Metrics Endpoint</h3>
<div class="section">
<p>Now that a port has been forwarded from localhost to a <code>Pod</code> in the cluster the metrics endpoint can be accessed.</p>

<p>Issue the following <code>curl</code> command to access the REST endpoint:</p>

<pre
lang="bash"

>curl http://127.0.0.1:9612/metrics</pre>

</div>
</div>

<h2 id="_prometheus_service_monitor">Prometheus Service Monitor</h2>
<div class="section">
<p>The operator can create a Prometheus <code>ServiceMonitor</code> for the metrics port so that Prometheus will automatically
scrape metrics from the <code>Pods</code> in a <code>Coherence</code> deployment.</p>

<pre
lang="yaml"
title="metrics-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: metrics-cluster
spec:
  coherence:
    metrics:
      enabled: true
  ports:
    - name: metrics
      serviceMonitor:
        enabled: true  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>serviceMonitor.enabled</code> field is set to <code>true</code> for the <code>metrics</code> port.</li>
</ul>
<p>See <router-link to="/ports/040_servicemonitors">Exposing ports and Services - Service Monitors</router-link> documentation for more details.</p>

</div>
</doc-view>
<!-- pages/metrics/030_importing.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Importing Grafana Dashboards</dt>
<dd slot="desc"><p>The Operator has a set of Grafana dashboards that can be imported into a Grafana instance.</p>

<div class="admonition note">
<p class="admonition-inline">Note: Use of metrics is available only when using the operator with clusters running
Coherence 12.2.1.4 or later version.</p>
</div></dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_obtain_the_coherence_dashboards">Obtain the Coherence Dashboards</h2>
<div class="section">
<p>The Coherence Operator provides a set of dashboards for Coherence that may be imported into Grafana.
There are two ways to obtain the dashboards:</p>

<ul class="ulist">
<li>
<p>Clone the Coherence Operator GitHub repo, checkout the branch or tag for the version you want to use and
then obtain the dashboards from the <code>dashboards/</code> directory.</p>

</li>
<li>
<p>Download the <code>.tar.gz</code> dashboards package for the release you want to use.</p>

</li>
</ul>
<pre
lang="bash"

>curl https://oracle.github.io/coherence-operator/dashboards/latest/coherence-dashboards.tar.gz \
    -o coherence-dashboards.tar.gz
tar -zxvf coherence-dashboards.tar.gz</pre>

<p>The above commands will download the <code>coherence-dashboards.tar.gz</code> file and unpack it resulting in a
directory named <code>dashboards/</code> in the current working directory. This <code>dashboards/</code> directory will contain
the various Coherence dashboard files.</p>

</div>

<h2 id="_importing_grafana_dashboards">Importing Grafana Dashboards.</h2>
<div class="section">
<p>This example shows you how to import the Grafana dashboards into your own Grafana instance.</p>

<p>By default, the Coherence dashboards require a datasource named <code>Prometheus</code> which
should also be the default datasource.</p>

<p>If this datasource is already used, and you cannot add another datasource as the default,
then please go to <router-link to="#different" @click.native="this.scrollFix('#different')">Importing with a different datasource</router-link>.</p>


<h3 id="importing">Manually Importing Using the Defaults</h3>
<div class="section">
<p>In your Grafana environment, ensure you either:</p>

<ul class="ulist">
<li>
<p>have a Prometheus datasource named <code>Prometheus</code> which is also marked as the default datasource</p>

</li>
<li>
<p>have added a new Prometheus datasource which you have set as the default</p>

</li>
</ul>
<p>Then continue below.</p>

<ul class="ulist">
<li>
<p>Clone the git repository using</p>

</li>
</ul>
<div class="listing">
<pre>git clone https://github.com/oracle/coherence-operator.git</pre>
</div>

<div class="admonition note">
<p class="admonition-textlabel">Note</p>
<p ><p>There are three sets of dashboards available</p>

<ul class="ulist">
<li>
<p>Default - these are dashboards under the <code>dashboards/grafana/</code> directory that are compatible with
the metric names produced by the Coherence metrics publisher</p>

</li>
<li>
<p>Microprofile - these are dashboards under the <code>dashboards/grafana-microprofile/</code> directory that are compatible with
the metric names produced by the Coherence MP Metrics module.</p>

</li>
<li>
<p>Micrometer - these are dashboards under the <code>dashboards/grafana-micrometer/</code> directory that are compatible with
the metric names produced by the Coherence Micrometer Metrics module. These are metrics commonly used with Microprofile applications such as Helidon.</p>

</li>
<li>
<p>Micrometer - these are dashboards under the <code>dashboards/grafana-micrometer/</code> directory that are compatible with the metric names produced by the Coherence Micrometer Metrics module. Micrometer is a common metrics framework used with applications such as Spring, Micronaut etc.</p>

</li>
</ul>
<p>If you do not see metrics on the dashboards as expected you might be using the wrong dashboards version for how
Coherence has been configured.
The simplest way to find out which version corresponds to your Coherence cluster
is to query the metrics endpoint with something like <code>curl</code>.
If the metric names are in the format <code>vendor:coherence_cluster_size</code>, i.e. prefixed with <code>vendor:</code> then this is
the default Coherence format.
If metric names are in the format <code>vendor_Coherence_Cluster_Size</code>, i.e. prefixed with <code>vendor_</code> then this is
Microprofile format.</p>
</p>
</div>
<ul class="ulist">
<li>
<p>Decide which dashboards you will import, depending on how metrics are being published (see the note above).</p>

</li>
<li>
<p>Login to Grafana and for each dashboard in the chosen dashboard directory carry out the
following to upload to Grafana:</p>
<ul class="ulist">
<li>
<p>Highlight the '+' (plus) icons and click on import</p>

</li>
<li>
<p>Click `Upload Json file' button to select a dashboard</p>

</li>
<li>
<p>Leave all the default values and click on <code>Import</code></p>

</li>
</ul>
</li>
</ul>
</div>

<h3 id="different">Manually Importing With a Different Datasource</h3>
<div class="section">
<p>If your Grafana environment has a default datasource that you cannot change or already has a
datasource named <code>Prometheus</code>, follow these steps to import the dashboards:</p>

<ul class="ulist">
<li>
<p>Login to Grafana</p>

</li>
<li>
<p>Create a new datasource named <code>Coherence-Prometheus</code> and point to your Prometheus endpoint</p>

</li>
<li>
<p>Create a temporary directory and copy all the dashboards from the cloned directory
<code>&lt;DIR&gt;/dashboards/grafana</code> to this temporary directory</p>

</li>
<li>
<p>Change to this temporary directory and run the following to update the datasource to <code>Coherence-Prometheus</code> or the
datasource of your own choice:</p>

</li>
</ul>
<div class="listing">
<pre>for file in *.json
do
    sed -i '' -e 's/"datasource": "Prometheus"/"datasource": "Coherence-Prometheus"/g' \
              -e 's/"datasource": null/"datasource": "Coherence-Prometheus"/g' \
              -e 's/"datasource": "-- Grafana --"/"datasource": "Coherence-Prometheus"/g' $file;
done</pre>
</div>

<ul class="ulist">
<li>
<p>Once you have completed the script, proceed to upload the dashboards as described above.</p>

</li>
</ul>
</div>
</div>

<h2 id="_automatically_importing_dashboards">Automatically Importing Dashboards</h2>
<div class="section">
<p>There are ways to automatically import dashboards into Grafana, the exact method would depend on how Grafana is to
be installed and run.
The Coherence Operator test pipeline, for example, uses the
<a id="" title="" target="_blank" href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a>
to install Prometheus and Grafana and automatically imports the Coherence dashboards from a <code>ConfigMap</code>.<br>
The examples below show how to create the dashboards as a <code>ConfigMap</code> and then install them into a Grafana
instances started with the Prometheus Operator.</p>

<p>There are two ways to create the <code>ConfigMap</code> containing the dashboard files:</p>

<ul class="ulist">
<li>
<p>Use the <code>ConfigMap</code> yaml available directly from GitHub</p>

</li>
<li>
<p>Obtain the dashboards as described above and create a <code>ConfigMap</code> from those files.</p>

</li>
</ul>

<h3 id="_create_a_configmap_from_github_yaml">Create a ConfigMap from GitHub Yaml</h3>
<div class="section">
<p>To create a <code>ConfigMap</code> with the Grafana dashboards directly from the <code>ConfigMap</code> yaml for a specific Operator release
the following commands can be used:</p>

<pre
lang="bash"

>kubectl -n monitoring create \
    -f https://oracle.github.io/coherence-operator/dashboards/latest/coherence-grafana-dashboards.yaml</pre>

<p>In this example the dashboards will be installed into the <code>monitoring</code> namespace.</p>

<p>The example above installs the dashboards configured to use the default Coherence metrics format.
Coherence provides integrations with Microprofile metrics and <a id="" title="" target="_blank" href="https://micrometer.io">Micrometer</a> metrics, which
produce metrics with slightly different name formats.
The operator provides dashboards compatible with both of these formats.</p>

<ul class="ulist">
<li>
<p>Microprofile change the URL to <code>coherence-grafana-microprofile-dashboards.yaml</code>, for example:</p>

</li>
</ul>
<pre
lang="bash"

>kubectl -n monitoring create \
    -f https://oracle.github.io/coherence-operator/dashboards/latest/coherence-grafana-microprofile-dashboards.yaml</pre>

<ul class="ulist">
<li>
<p>Micrometer change the URL to <code>coherence-grafana-micrometer-dashboards.yaml</code>, for example:</p>

</li>
</ul>
<pre
lang="bash"

>kubectl -n monitoring create \
    -f https://oracle.github.io/coherence-operator/dashboards/latest/coherence-grafana-micrometer-dashboards.yaml</pre>

</div>

<h3 id="_create_a_configmap_from_the_dashboard_package_file">Create a ConfigMap from the Dashboard Package File</h3>
<div class="section">
<p>To create a <code>ConfigMap</code> with the Grafana dashboards in directly from <code>.tar.gz</code> dashboard package for a specific
Operator release the following commands can be used:</p>

<pre
lang="bash"

>curl https://oracle.github.io/coherence-operator/dashboards/latest/coherence-dashboards.tar.gz \
    -o coherence-dashboards.tar.gz
tar -zxvf coherence-dashboards.tar.gz
kubectl -n monitoring create configmap coherence-grafana-dashboards --from-file=dashboards/grafana</pre>

<p>The <code>VERSION</code> variable has been set to the version of the dashboards to be used (this corresponds to an
Operator release version but dashboards can be used independently of the Operator).<br>
In this example the dashboards <code>ConfigMap</code> named <code>coherence-grafana-dashboards</code> will be installed into
the <code>monitoring</code> namespace.</p>

</div>

<h3 id="_label_the_configmap">Label the ConfigMap</h3>
<div class="section">
<p>In this example Grafana will be configured to import dashboards from <code>ConfigMaps</code> with the
label <code>grafana_dashboard</code>, so the <code>ConfigMap</code> created above needs to be labelled:</p>

<pre
lang="bash"

>kubectl -n monitoring label configmap coherence-grafana-dashboards grafana_dashboard=1</pre>

</div>

<h3 id="_install_the_prometheus_operator">Install the Prometheus Operator</h3>
<div class="section">
<p>The Prometheus Operator will be installed using its Helm chart.
Create a Helm values file like the following:</p>

<pre
lang="yaml"
title="prometheus-values.yaml"
>prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
alertmanager:
  enabled: false
nodeExporter:
  enabled: true
grafana:
  enabled: true                   <span class="conum" data-value="1" />
  sidecar:
    dashboards:                   <span class="conum" data-value="2" />
      enabled: true
      label: grafana_dashboard</pre>

<ul class="colist">
<li data-value="1">Grafana will be enabled.</li>
<li data-value="2">Grafana will automatically import dashboards from <code>ConfigMaps</code> that have the label <code>grafana_dashboard</code>
(which was given to the <code>ConfigMap</code> created above).</li>
</ul>
<p>Prometheus can be installed into the <code>monitoring</code> namespace using the Helm command:</p>

<pre
lang="bash"

>helm install --namespace monitoring \
    --values prometheus-values.yaml \
    prometheus stable/prometheus-operator</pre>

<p>To actually start Prometheus a <code>Prometheus</code> CRD resource needs to be added to Kubernetes.
Create a <code>Prometheus</code> resource yaml file suitable for testing:</p>

<pre
lang="yaml"
title="prometheus.yaml"
>apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      coherenceComponent: coherence-service-monitor  <span class="conum" data-value="1" />
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: true</pre>

<ul class="colist">
<li data-value="1">The <code>serviceMonitorSelector</code> tells Prometheus to use any <code>ServiceMonitor</code> that is created with the
<code>coherence-service-monitor</code> label, which is a label that the Coherence Operator adds to any <code>ServiceMonitor</code>
that it creates.</li>
</ul>
<p>Install the <code>prometheus.yaml</code> file into Kubernetes:</p>

<pre
lang="bash"

>kubectl -n monitoring create -f etc/prometheus.yaml</pre>

<p>In the <code>monitoring</code> namespace there should now be a number of <code>Pods</code> and <code>Services</code>, among them a <code>Prometheus</code>
instance, and a Grafana instance. It should be possible to reach the Grafana UI on the ports exposed by the <code>Pod</code>
and see the imported Coherence dashboards.</p>

<pre
lang="bash"

>GRAFANA_POD=$(kubectl -n monitoring get pod -l app.kubernetes.io/name=grafana -o name)
kubectl -n monitoring port-forward ${GRAFANA_POD} 3000:3000</pre>

<div class="admonition note">
<p class="admonition-inline">The default username for Grafana installed by the Prometheus Operator is <code>admin</code>
the default password is <code>prom-operator</code></p>
</div>
<p>If a Coherence cluster has been started with the Operator as described in the <router-link to="/metrics/020_metrics">Publish Metrics</router-link>
page, its metrics will eventually appear in Prometheus and Grafana. It can sometimes take a minute or so for
Prometheus to start scraping metrics and for them to appear in Grafana.</p>

</div>
</div>
</doc-view>
<!-- pages/metrics/040_dashboards.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Grafana Dashboards</dt>
<dd slot="desc"><p>The Coherence Operator provides detailed Grafana dashboards to provide insight into your running Coherence Clusters.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_grafana_dashboards">Grafana Dashboards</h2>
<div class="section">
<div class="admonition note">
<p class="admonition-inline">Note: Use of metrics is available only when using the operator with clusters running
Coherence 12.2.1.4 or later version.</p>
</div>
</div>

<h2 id="_table_of_contents">Table of Contents</h2>
<div class="section">
<ol style="margin-left: 15px;">
<li>
<router-link to="#navigation" @click.native="this.scrollFix('#navigation')">Navigation</router-link>

</li>
<li>
<router-link to="#dashboards" @click.native="this.scrollFix('#dashboards')">Dashboards</router-link>
<ol style="margin-left: 15px;">
<li>
<router-link to="#main" @click.native="this.scrollFix('#main')">Coherence Dashboard Main</router-link>

</li>
<li>
<router-link to="#members" @click.native="this.scrollFix('#members')">Members Summary &amp; Details Dashboards</router-link>

</li>
<li>
<router-link to="#services" @click.native="this.scrollFix('#services')">Services Summary &amp; Details Dashboards</router-link>

</li>
<li>
<router-link to="#caches" @click.native="this.scrollFix('#caches')">Caches Summary &amp; Detail Dashboards</router-link>

</li>
<li>
<router-link to="#proxies" @click.native="this.scrollFix('#proxies')">Proxy Servers Summary &amp; Detail Dashboards</router-link>

</li>
<li>
<router-link to="#persistence" @click.native="this.scrollFix('#persistence')">Persistence Summary Dashboard</router-link>

</li>
<li>
<router-link to="#federation" @click.native="this.scrollFix('#federation')">Federation Summary &amp; Details Dashboards</router-link>

</li>
<li>
<router-link to="#machines" @click.native="this.scrollFix('#machines')">Machines Summary Dashboard</router-link>

</li>
<li>
<router-link to="#http" @click.native="this.scrollFix('#http')">HTTP Servers Summary Dashboard</router-link>

</li>
<li>
<router-link to="#ed" @click.native="this.scrollFix('#ed')">Elastic Data Summary Dashboard</router-link>

</li>
</ol>
</li>
</ol>
</div>

<h2 id="navigation">Navigation</h2>
<div class="section">
<p>The pre-loaded Coherence Dashboards provide a number of common features and
navigation capabilities that appear at the top of most dashboards.</p>


<h3 id="_variables">Variables</h3>
<div class="section">
<p><img src="./images/grafana-variables.png" alt="Variables"width="250" />
</p>

<p>Allows for selection of information to be displayed where there is more than one item.</p>

<ol style="margin-left: 15px;">
<li>
Cluster Name - Allows selection of the cluster to view metrics for

</li>
<li>
Top N Limit - Limits the display of <code>Top</code> values for tables that support it

</li>
<li>
Service Name, Member Name, Cache Name - These will appear on various dashboards

</li>
</ol>
<p>See the <a id="" title="" target="_blank" href="https://grafana.com/docs/reference/templating/">Grafana Documentation</a> for more information on Variables.</p>

</div>

<h3 id="_annotations">Annotations</h3>
<div class="section">
<p><img src="./images/grafana-annotations.png" alt="Annotations"width="250" />
</p>

<p>Vertical red lines on a graph to indicate a change in a key markers such as:</p>

<ol style="margin-left: 15px;">
<li>
Show Cluster Size Changes - Displays when the cluster size has changed

</li>
<li>
Show Partition Transfers - Displays when partition transfers have occurred

</li>
</ol>
<p>See the <a id="" title="" target="_blank" href="https://grafana.com/docs/reference/annotations/">Grafana Documentation</a> for more information on Annotations.</p>

</div>

<h3 id="_navigation">Navigation</h3>
<div class="section">
<p><img src="./images/grafana-navigation.png" alt="Navigation"width="250" />
</p>

<ol style="margin-left: 15px;">
<li>
Select Dashboard - In the top right a drop down list of dashboards is available selection

</li>
<li>
Drill Through - Ability to drill through based upon service, member, node, etc.

</li>
</ol>
</div>
</div>

<h2 id="dashboards">Dashboards</h2>
<div class="section">

<h3 id="main">1. Coherence Dashboard Main</h3>
<div class="section">
<p>Shows a high-level overview of the selected Coherence cluster including metrics such as:</p>

<ul class="ulist">
<li>
<p>Cluster member count, services, memory and health</p>

</li>
<li>
<p>Top N loaded members, Top N heap usage and GC activity</p>

</li>
<li>
<p>Service backlogs and endangered or vulnerable services</p>

</li>
<li>
<p>Top query times, non-optimized queries</p>

</li>
<li>
<p>Guardian recoveries and terminations</p>

</li>
</ul>


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-main.png" alt="Dashboard Main"width="950" />
</v-card-text>
</v-card>

</div>

<h3 id="members">2. Members Summary &amp; Details Dashboards</h3>
<div class="section">
<p>Shows an overview of all cluster members that are enabled for metrics capture including metrics such as:</p>

<ul class="ulist">
<li>
<p>Member list include heap usage</p>

</li>
<li>
<p>Top N members for GC time and count</p>

</li>
<li>
<p>Total GC collection count and time by Member</p>

</li>
<li>
<p>Publisher and Receiver success rates</p>

</li>
<li>
<p>Guardian recoveries and send queue size</p>

</li>
</ul>

<h4 id="_members_summary">Members Summary</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-members.png" alt="Members"width="950" />
</v-card-text>
</v-card>

</div>

<h4 id="_member_details">Member Details</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-members.png" alt="Member Details"width="950" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="services">3. Services Summary &amp; Details Dashboards</h3>
<div class="section">
<p>Shows an overview of all cluster services including metrics such as:</p>

<ul class="ulist">
<li>
<p>Service members for storage and non-storage services</p>

</li>
<li>
<p>Service task count</p>

</li>
<li>
<p>StatusHA values as well as endangered, vulnerable and unbalanced partitions</p>

</li>
<li>
<p>Top N services by task count and backlog</p>

</li>
<li>
<p>Task rates, request pending counts and task and request averages</p>

</li>
</ul>

<h4 id="_services_summary">Services Summary</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-services.png" alt="Services"width="950" />
</v-card-text>
</v-card>

</div>

<h4 id="_service_details">Service Details</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-service.png" alt="Service Details"width="950" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="caches">4. Caches Summary &amp; Detail Dashboards</h3>
<div class="section">
<p>Shows an overview of all caches including metrics such as:</p>

<ul class="ulist">
<li>
<p>Cache entries, memory and index usage</p>

</li>
<li>
<p>Cache access counts including gets, puts and removed,  max query times</p>

</li>
<li>
<p>Front cache hit and miss rates</p>

</li>
</ul>

<h4 id="_caches_summary">Caches Summary</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-caches.png" alt="Caches"width="950" />
</v-card-text>
</v-card>

</div>

<h4 id="_cache_details">Cache Details</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-cache.png" alt="Cache Details"width="950" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="proxies">5. Proxy Servers Summary &amp; Detail Dashboards</h3>
<div class="section">
<p>Shows and overview of Proxy servers including metrics such as:</p>

<ul class="ulist">
<li>
<p>Active connection count and service member count</p>

</li>
<li>
<p>Total messages sent/ received</p>

</li>
<li>
<p>Proxy server data rates</p>

</li>
<li>
<p>Individual connection details abd byte backlogs</p>

</li>
</ul>

<h4 id="_proxy_servers_summary">Proxy Servers Summary</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-proxies.png" alt="Proxy Servers"width="950" />
</v-card-text>
</v-card>

</div>

<h4 id="_proxy_servers_detail">Proxy Servers Detail</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-proxy.png" alt="Proxy Server Details"width="950" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="persistence">6. Persistence Summary Dashboard</h3>
<div class="section">
<p>Shows and overview of Persistence including metrics such as:</p>

<ul class="ulist">
<li>
<p>Persistence enabled services</p>

</li>
<li>
<p>Maximum active persistence latency</p>

</li>
<li>
<p>Active space total usage and by service</p>

</li>
</ul>


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-persistence.png" alt="Persistence"width="950" />
</v-card-text>
</v-card>

</div>

<h3 id="federation">7. Federation Summary &amp; Details Dashboards</h3>
<div class="section">
<p>Shows overview of Federation including metrics such as:</p>

<ul class="ulist">
<li>
<p>Destination and Origins details</p>

</li>
<li>
<p>Entries, records and bytes send and received</p>

</li>
</ul>

<h4 id="_federation_summary">Federation Summary</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-federation-summary.png" alt="Federation Summary"width="950" />
</v-card-text>
</v-card>

</div>

<h4 id="_federation_details">Federation Details</h4>
<div class="section">


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-federation-detail.png" alt="Federation Details"width="950" />
</v-card-text>
</v-card>

</div>
</div>

<h3 id="machines">8. Machines Summary Dashboard</h3>
<div class="section">
<p>Shows an overview of all machines that make up the Kubernetes cluster underlying the Coherence cluster including metrics such as:</p>

<ul class="ulist">
<li>
<p>Machine processors, free swap space and physical memory</p>

</li>
<li>
<p>Load averages</p>

</li>
</ul>


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-machines.png" alt="Machines"width="950" />
</v-card-text>
</v-card>

</div>

<h3 id="http">9. HTTP Servers Summary Dashboard</h3>
<div class="section">
<p>Shows an overview of all HTTP Servers running in the cluster including metrics such as:</p>

<ul class="ulist">
<li>
<p>Service member count, requests, error count and average request time</p>

</li>
<li>
<p>HTTP Request rates and response codes</p>

</li>
</ul>


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-http.png" alt="HTTP Servers"width="950" />
</v-card-text>
</v-card>

</div>

<h3 id="ed">10. Elastic Data Summary Dashboard</h3>
<div class="section">
<p>Shows an overview of all HTTP Servers running in the cluster including metrics such as:</p>

<ul class="ulist">
<li>
<p>RAM and Flash journal files in use</p>

</li>
<li>
<p>RAM and Flash compactions</p>

</li>
</ul>


<v-card>
<v-card-text class="overflow-y-hidden" style="text-align:center">
<img src="./images/grafana-elastic-data.png" alt="Elastic Data"width="950" />
</v-card-text>
</v-card>

</div>
</div>
</doc-view>
<!-- pages/metrics/050_ssl.js -->
<doc-view>

<h2 id="_ssl_with_metrics">SSL with Metrics</h2>
<div class="section">
<p>It is possible to configure metrics endpoint to use SSL to secure the communication between server and
client. The SSL configuration is in the <code>coherence.metrics.ssl</code> section of the CRD spec.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  coherence:
    metrics:
      enabled: true
      ssl:
        enabled: true                            <span class="conum" data-value="1" />
        keyStore: metrics-keys.jks               <span class="conum" data-value="2" />
        keyStoreType: JKS                        <span class="conum" data-value="3" />
        keyStorePasswordFile: store-pass.txt     <span class="conum" data-value="4" />
        keyPasswordFile: key-pass.txt            <span class="conum" data-value="5" />
        keyStoreProvider:                        <span class="conum" data-value="6" />
        keyStoreAlgorithm: SunX509               <span class="conum" data-value="7" />
        trustStore: metrics-trust.jks            <span class="conum" data-value="8" />
        trustStoreType: JKS                      <span class="conum" data-value="9" />
        trustStorePasswordFile: trust-pass.txt   <span class="conum" data-value="10" />
        trustStoreProvider:                      <span class="conum" data-value="11" />
        trustStoreAlgorithm: SunX509             <span class="conum" data-value="12" />
        requireClientCert: true                  <span class="conum" data-value="13" />
        secrets: metrics-secret                  <span class="conum" data-value="14" /></pre>

<ul class="colist">
<li data-value="1">The <code>enabled</code> field when set to <code>true</code> enables SSL for metrics or when set to <code>false</code> disables SSL</li>
<li data-value="2">The <code>keyStore</code> field sets the name of the Java key store file that should be used to obtain the server&#8217;s key</li>
<li data-value="3">The optional <code>keyStoreType</code> field sets the type of the key store file, the default value is <code>JKS</code></li>
<li data-value="4">The optional <code>keyStorePasswordFile</code> sets the name of the text file containing the key store password</li>
<li data-value="5">The optional <code>keyPasswordFile</code> sets the name of the text file containing the password of the key in the key store</li>
<li data-value="6">The optional <code>keyStoreProvider</code> sets the provider name for the key store</li>
<li data-value="7">The optional <code>keyStoreAlgorithm</code> sets the algorithm name for the key store, the default value is <code>SunX509</code></li>
<li data-value="8">The <code>trustStore</code> field sets the name of the Java trust store file that should be used to obtain the server&#8217;s key</li>
<li data-value="9">The optional <code>trustStoreType</code> field sets the type of the trust store file, the default value is <code>JKS</code></li>
<li data-value="10">The optional <code>trustStorePasswordFile</code> sets the name of the text file containing the trust store password</li>
<li data-value="11">The optional <code>trustStoreProvider</code> sets the provider name for the trust store</li>
<li data-value="12">The optional <code>trustStoreAlgorithm</code> sets the algorithm name for the trust store, the default value is <code>SunX509</code></li>
<li data-value="13">The optional <code>requireClientCert</code> field if set to <code>true</code> enables two-way SSL where the client must also provide
a valid certificate</li>
<li data-value="14">The optional <code>secrets</code> field sets the name of the Kubernetes <code>Secret</code> to use to obtain the key store, truct store
and password files from.</li>
</ul>
<p>The various files and keystores referred to in the configuration above can be any location accessible in the image
used by the <code>coherence</code> container in the deployment&#8217;s <code>Pods</code>. Typically, for things such as SSL keys and certs,
these would be provided by obtained from <code>Secrets</code> loaded as additional <code>Pod</code> <code>Volumes</code>.
See <router-link to="/other/060_secret_volumes">Add Secrets Volumes</router-link> for the documentation on how to specify
secrets as additional volumes.</p>

</div>
</doc-view>
<!-- pages/ordering/010_overview.js -->
<doc-view>

<h2 id="_coherence_deployment_dependencies_and_start_order">Coherence Deployment Dependencies and Start Order</h2>
<div class="section">
<p>The default behaviour of the operator is to create the <code>StatefulSet</code> for a <code>Coherence</code> deployment immediately.
Sometimes this behaviour is not suitable if, for example, when application code running in one deployment depends on the
availability of another deployment.
Typically, this might be storage disabled members having functionality that relies on the storage members being ready first.
The <code>Coherence</code> CRD allows can be configured with a <code>startQuorum</code> that defines a deployment&#8217;s dependency on other
deployments in the cluster.</p>

<div class="admonition note">
<p class="admonition-inline">The <code>startQuorum</code> only applies when a cluster is initially being started by the operator, it does not apply in other
functions such as upgrades, scaling, shut down etc.</p>
</div>
<p>An individual deployment can depend on one or more other deployment. The dependency can be such that the deployment will
not be created until all of the <code>Pods</code> of the dependent deployment are ready, or it can be configured so that just a
single <code>Pod</code> of the dependent deployment must be ready.</p>

<p>For example:
In the yaml snippet below there are two <code>Coherence</code> deployments, <code>data</code> and <code>proxy</code></p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: data
spec:
  replicas: 3           <span class="conum" data-value="1" />
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: proxy
spec:
  startQuorum:          <span class="conum" data-value="2" />
    - deployment: data
      podCount: 1</pre>

<ul class="colist">
<li data-value="1">The <code>data</code> deployment does not specify a <code>startQuorum</code> so this role will be created immediately by the operator.</li>
<li data-value="2">The <code>proxy</code> deployment has a start quorum that means that the <code>proxy</code> deployment depends on the <code>data</code> deployment.
The <code>podCount</code> field has been set to <code>1</code> meaning the <code>proxy</code> deployment&#8217;s <code>StatefulSet</code> will not be created until at
least <code>1</code> of the <code>data</code> deployment&#8217;s <code>Pods</code> is in the <code>Ready</code> state.</li>
</ul>
<p>Omitting the <code>podCount</code> from the quorum means that the role will not start until all the configured replicas of the
dependent deployment are ready; for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: data
spec:
  replicas: 3
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: proxy
spec:
  startQuorum:          <span class="conum" data-value="1" />
    - deployment: data</pre>

<ul class="colist">
<li data-value="1">The <code>proxy</code> deployment&#8217;s <code>startQuorum</code> just specifies a dependency on the <code>data</code> deployment with no <code>podCount</code> so
all <code>3</code> of the <code>data</code> deployment&#8217;s <code>Pods</code> must be <code>Ready</code> before the <code>proxy</code> deployment&#8217;s <code>StatefulSet</code> is created by
the operator.</li>
</ul>
<div class="admonition note">
<p class="admonition-inline">Setting a <code>podCount</code> less than or equal to zero is the same as not specifying a count.</p>
</div>

<h3 id="_multiple_dependencies">Multiple Dependencies</h3>
<div class="section">
<p>The <code>startQuorum</code> can specify a dependency on more than on deployment; for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: data      <span class="conum" data-value="1" />
spec:
  replicas: 5
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: proxy        <span class="conum" data-value="1" />
spec:
  replicas: 3
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: web
spec:
  startQuorum:          <span class="conum" data-value="2" />
    - deployment: data
    - deployment: proxy
      podCount: 1</pre>

<ul class="colist">
<li data-value="1">The <code>data</code> and <code>proxy</code> deployments do not specify a <code>startQuorum</code>, so the <code>StatefulSets</code> for these deployments will
be created immediately by the operator.</li>
<li data-value="2">The <code>web</code> deployment has a <code>startQuorum</code> the defines a dependency on both the <code>data</code> deployment and the <code>proxy</code>
deployment. The <code>proxy</code> dependency also specifies a <code>podCount</code> of <code>1</code>.
This means that the operator wil not create the <code>web</code> role&#8217;s <code>StatefulSet</code> until all <code>5</code> replicas of the <code>data</code>
deployment are <code>Ready</code> and at least <code>1</code> of the <code>proxy</code> deployment&#8217;s <code>Pods</code> is <code>Ready</code>.</li>
</ul>
</div>

<h3 id="_chained_dependencies">Chained Dependencies</h3>
<div class="section">
<p>It is also possible to chain dependencies, for example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: data            <span class="conum" data-value="1" />
spec:
  replicas: 5
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: proxy
spec:
  replicas: 3
  startQuorum:          <span class="conum" data-value="2" />
    - deployment: data
---
apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: web
spec:
  startQuorum:          <span class="conum" data-value="3" />
    - deployment: proxy
      podCount: 1</pre>

<ul class="colist">
<li data-value="1">The <code>data</code> deployment does not specify a <code>startQuorum</code> so this deployment&#8217;s <code>StatefulSet</code> will be created immediately
by the operator.</li>
<li data-value="2">The <code>proxy</code> deployment defines a dependency on the <code>data</code> deployment without a <code>podCount</code> so all five <code>Pods</code> of the
<code>data</code> role must be in a <code>Ready</code> state before the operator will create the <code>proxy</code> deployment&#8217;s <code>StatefulSet</code>.</li>
<li data-value="3">The <code>web</code> deployment depends on the <code>proxy</code> deployment with a <code>podCount</code> of one, so the operator will not create the
<code>web</code> deployment&#8217;s <code>StatefulSet</code> until at least one <code>proxy</code> deployment <code>Pod</code> is in a <code>Ready</code> state.</li>
</ul>
<div class="admonition warning">
<p class="admonition-inline">The operator does not validate that a <code>startQuorum</code> makes sense. It is possible to declare a quorum with circular
dependencies, in which case the roles will never start. It would also be possible to create a quorum with a <code>podCount</code> greater
than the <code>replicas</code> value of the dependent deployment, in which case the quorum would never be met, and the role would not start.</p>
</div>
</div>
</div>
</doc-view>
<!-- pages/other/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>There are a number of miscellaneous configuration settings that can be added to containers and <code>Pods</code>
controlled by the Coherence Operator.</p>

<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/020_environment"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Environment Variables</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding environment variables to the Coherence container.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/030_labels"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Pod Labels</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding Pod labels.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/040_annotations"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Pod Annotations</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding Pod annotations.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>

<h3 id="_containers">Containers</h3>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/080_add_containers"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Add Containers</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding side-car containers and init-containers.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>

<h3 id="_volumes">Volumes</h3>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/070_add_volumes"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Add Volumes</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding Volumes and volume mounts.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/050_configmap_volumes"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Add ConfigMap Volumes</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding Volumes and volume mounts using ConfigMaps.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/060_secret_volumes"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Add Secret Volumes</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding Volumes and volume mounts using Secrets.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>

<h3 id="_pod_scheduling">Pod Scheduling</h3>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/090_pod_scheduling"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Pod Scheduling</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Taints, Tolerations and node selectors.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/other/100_resources"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Resources</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configuring Coherence container resource constraints.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</div>
</doc-view>
<!-- pages/other/020_environment.js -->
<doc-view>

<h2 id="_environment_variables">Environment Variables</h2>
<div class="section">
<p>Environment variables can be added to the Coherence container in the <code>Pods</code> managed by the Operator.
Additional variables should be added to the <code>env</code> list in the <code>Coherence</code> CRD spec.
The entries in the <code>env</code> list are Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvar-v1-core">EnvVar</a>
values, exactly the same as when adding environment variables to a container spec.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  env:
    - name: VAR_ONE            <span class="conum" data-value="1" />
      value: VALUE_ONE
    - name: VAR_TWO            <span class="conum" data-value="2" />
      valueFrom:
        secretKeyRef:
          name: test-secret
          key: secret-key</pre>

<ul class="colist">
<li data-value="1">The <code>VAR_ONE</code> environment variable is a simple variable with a value of <code>VALUE_ONE</code></li>
<li data-value="2">The <code>VAR_TWO</code> environment variable is variable that is loaded from a secret.</li>
</ul>
</div>
</doc-view>
<!-- pages/other/030_labels.js -->
<doc-view>

<h2 id="_pod_labels">Pod Labels</h2>
<div class="section">
<p>Additional labels can be added to the <code>Pods</code> managed by the Operator.
Additional labels should be added to the <code>labels</code> map in the <code>Coherence</code> CRD spec.
The entries in the <code>labels</code> map should confirm to the recommendations and rules in the Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels</a> documentation.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  labels:             <span class="conum" data-value="1" />
    tier: backend
    environment: dev</pre>

<ul class="colist">
<li data-value="1">Two labels will be added to the <code>Pods</code>, <code>tier=backend</code> and <code>environment=dev</code></li>
</ul>
</div>
</doc-view>
<!-- pages/other/040_annotations.js -->
<doc-view>

<h2 id="_pod_annotations">Pod Annotations</h2>
<div class="section">
<p>Additional annotations can be added to the <code>Pods</code> managed by the Operator.
Annotations should be added to the <code>annotations</code> map in the <code>Coherence</code> CRD spec.
The entries in the <code>annotations</code> map should confirm to the recommendations and rules in the Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations</a> documentation.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  annotations:                        <span class="conum" data-value="1" />
    prometheus.io/path: /metrics
    prometheus.io/port: "9612"
    prometheus.io/scheme: http
    prometheus.io/scrape: "true"</pre>

<ul class="colist">
<li data-value="1">A number of Prometheus annotations will be added to this <code>Coherence</code> deployment&#8217;s <code>Pods</code></li>
</ul>
</div>
</doc-view>
<!-- pages/other/050_configmap_volumes.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Add ConfigMap Volumes</dt>
<dd slot="desc"><p>Additional <code>Volumes</code> and <code>VolumeMounts</code> from <code>ConfigMaps</code> can easily be added to a <code>Coherence</code> resource.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_add_configmap_volumes">Add ConfigMap Volumes</h2>
<div class="section">
<p>To add a <code>ConfigMap</code> as an additional volume to the <code>Pods</code> of a Coherence deployment add entries to the
<code>configMapVolumes</code> list in the CRD spec.
Each entry in the list has a mandatory <code>name</code> and <code>mountPath</code> field, all other fields are optional.
The <code>name</code> field is the name of the <code>ConfigMap</code> to mount and is also used as the volume name.
The <code>mountPath</code> field is the path in the container to mount the volume to.</p>

<div class="admonition note">
<p class="admonition-inline">Additional volumes added in this way (either <code>ConfigMaps</code> shown here, or <code>Secrets</code> or plain <code>Volumes</code>) will be
added to all containers in the <code>Pod</code>.</p>
</div>
<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  configMapVolumes:
    - name: storage-config               <span class="conum" data-value="1" />
      mountPath: /home/coherence/config  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>ConfigMap</code> named <code>storage-config</code> will be mounted to the <code>Pod</code> as an additional <code>Volume</code> named <code>storage-config</code></li>
<li data-value="2">The <code>ConfigMap</code> will be mounted at <code>/home/coherence/config</code> in the containers.</li>
</ul>
<p>The yaml above would result in a <code>Pod</code> spec similar to the following (a lot of the <code>Pod</code> spec has been omitted to just
show the relevant volume information):</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Pod
metadata:
  name: storage-0
spec:
  containers:
    - name: coherence
      volumeMounts:
        - name: storage-config
          mountPath: /home/coherence/config
  volumes:
    - name: storage-config
      configMap:
        name: storage-config</pre>

<p>As already stated, if the <code>Coherence</code> resource has additional containers the <code>ConfigMap</code> will be mounted in all of them.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  sideCars:
    - name: fluentd
      image: "fluent/fluentd:v1.3.3"
  configMapVolumes:
    - name: storage-config
      mountPath: /home/coherence/config</pre>

<p>In this example the <code>storage-config</code> <code>ConfigMap</code> will be mounted as a <code>Volume</code> and mounted to both the <code>coherence</code>
container and the <code>fluentd</code> container.
The yaml above would result in a <code>Pod</code> spec similar to the following (a lot of the <code>Pod</code> spec has been omitted to just
show the relevant volume information):</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Pod
metadata:
  name: storage-0
spec:
  containers:
    - name: coherence
      volumeMounts:
        - name: storage-config
          mountPath: /home/coherence/config
    - name: fluentd
      image: "fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3"
      volumeMounts:
        - name: storage-config
          mountPath: /home/coherence/config
  volumes:
    - name: storage-config
      configMap:
        name: storage-config</pre>

</div>
</doc-view>
<!-- pages/other/060_secret_volumes.js -->
<doc-view>

<v-layout row wrap>
<v-flex xs12 sm10 lg10>
<v-card class="section-def" v-bind:color="$store.state.currentColor">
<v-card-text class="pa-3">
<v-card class="section-def__card">
<v-card-text>
<dl>
<dt slot=title>Add Secrets Volumes</dt>
<dd slot="desc"><p>Additional <code>Volumes</code> and <code>VolumeMounts</code> from <code>Secrets</code> can easily be added to a <code>Coherence</code> resource.</p>
</dd>
</dl>
</v-card-text>
</v-card>
</v-card-text>
</v-card>
</v-flex>
</v-layout>

<h2 id="_add_secrets_volumes">Add Secrets Volumes</h2>
<div class="section">
<p>To add a <code>Secret</code> as an additional volume to the <code>Pods</code> of a Coherence deployment add entries to the
<code>secretVolumes</code> list in the CRD spec.
Each entry in the list has a mandatory <code>name</code> and <code>mountPath</code> field, all other fields are optional.
The <code>name</code> field is the name of the <code>Secret</code> to mount and is also used as the volume name.
The <code>mountPath</code> field is the path in the container to mount the volume to.</p>

<div class="admonition note">
<p class="admonition-inline">Additional volumes added in this way (either <code>Secrets</code> shown here, or <code>Secrets</code> or plain <code>Volumes</code>) will be
added to all containers in the <code>Pod</code>.</p>
</div>
<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  secretVolumes:
    - name: storage-config               <span class="conum" data-value="1" />
      mountPath: /home/coherence/config  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>Secret</code> named <code>storage-config</code> will be mounted to the <code>Pod</code> as an additional <code>Volume</code> named <code>storage-config</code></li>
<li data-value="2">The <code>Secret</code> will be mounted at <code>/home/coherence/config</code> in the containers.</li>
</ul>
<p>The yaml above would result in a <code>Pod</code> spec similar to the following (a lot of the <code>Pod</code> spec has been omitted to just
show the relevant volume information):</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Pod
metadata:
  name: storage-0
spec:
  containers:
    - name: coherence
      volumeMounts:
        - name: storage-config
          mountPath: /home/coherence/config
  volumes:
    - name: storage-config
      secret:
        secretName: storage-config</pre>

<p>As already stated, if the <code>Coherence</code> resource has additional containers the <code>Secret</code> will be mounted in all of them.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  sideCars:
    - name: fluentd
      image: "fluent/fluentd:v1.3.3"
  secretVolumes:
    - name: storage-config
      mountPath: /home/coherence/config</pre>

<p>In this example the <code>storage-config</code> <code>Secret</code> will be mounted as a <code>Volume</code> and mounted to both the <code>coherence</code>
container and the <code>fluentd</code> container.
The yaml above would result in a <code>Pod</code> spec similar to the following (a lot of the <code>Pod</code> spec has been omitted to just
show the relevant volume information):</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Pod
metadata:
  name: storage-0
spec:
  containers:
    - name: coherence
      volumeMounts:
        - name: storage-config
          mountPath: /home/coherence/config
    - name: fluentd
      image: "fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3"
      volumeMounts:
        - name: storage-config
          mountPath: /home/coherence/config
  volumes:
    - name: storage-config
      secret:
        secretName: storage-config</pre>

</div>
</doc-view>
<!-- pages/other/070_add_volumes.js -->
<doc-view>

<h2 id="_add_pod_volumes">Add Pod Volumes</h2>
<div class="section">
<p>Volumes and volume mappings can easily be added to a <code>Coherence</code> resource Pod to allow application code
deployed in the Pods to access additional storage.</p>

<p>Volumes are added by adding configuration to the <code>volumes</code> list in the <code>Coherence</code> CRD spec.
The configuration of the volume can be any valid yaml that would be used when adding a <code>Volume</code> to a <code>Pod</code> spec.</p>

<p>Volume mounts are added by adding configuration to the <code>volumeMounts</code> list in the <code>Coherence</code> CRD spec.
The configuration of the volume mount can be any valid yaml that would be used when adding a volume mount to a
container in a <code>Pod</code> spec.</p>

<div class="admonition note">
<p class="admonition-inline">Additional volumes added in this way will be added to all containers in the <code>Pod</code>.</p>
</div>
<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  volumes:
    - name: data-volume      <span class="conum" data-value="1" />
      nfs:
        path: /shared/data
        server: nfs-server
  volumeMounts:
    - name: data-volume      <span class="conum" data-value="2" />
      mountPath: /data</pre>

<ul class="colist">
<li data-value="1">An additional <code>Volume</code> named <code>data-volume</code> has been added (in this case the volume is an NFS volume).</li>
<li data-value="2">An additional volume mount has been added tthat will mount the <code>data-volume</code> at the <code>/data</code> mount point.</li>
</ul>
<p>The yaml above would result in a <code>Pod</code> spec similar to the following (a lot of the <code>Pod</code> spec has been omitted to just
show the relevant volume information):</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Pod
metadata:
  name: storage-0
spec:
  containers:
    - name: coherence
      volumeMounts:
        - name: data-volume
          mountPath: /data
  volumes:
    - name: data-volume
      nfs:
        path: /shared/data
        server: nfs-server</pre>

</div>
</doc-view>
<!-- pages/other/080_add_containers.js -->
<doc-view>

<h2 id="_configure_additional_containers">Configure Additional Containers</h2>
<div class="section">
<p>Additional containers and init-containers can easily be added to a <code>Coherence</code> resource Pod.
There are two types of container that can be added, init-containers and normal containers.
An example use case for this would be to add something like a Fluentd side-car container to ship logs to Elasticsearch.</p>

<div class="admonition note">
<p class="admonition-inline">A note about Volumes:<br>
The Operator created a number of volumes and volume mounts by default. These default volume mounts will be added
to <strong>all</strong> containers in the <code>Pod</code> including containers added as described here.<br>
Any additional volumes and volume mounts added to the <code>Coherence</code> resource spec will also be added <strong>all</strong> containers.</p>
</div>

<h3 id="_add_a_container">Add a Container</h3>
<div class="section">
<p>To add a container to the <code>Pods</code> specify the container in the <code>sideCars</code> list in the <code>Coherence</code> CRD spec.</p>

<p>See the <router-link to="/logging/020_logging">Logging Documentation</router-link> for a bigger example of adding a side-car container.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  sideCars:
    - name: fluentd                   <span class="conum" data-value="1" />
      image: "fluent/fluentd:v1.3.3"</pre>

<ul class="colist">
<li data-value="1">An additional container named <code>fluentd</code> has been added to the CRD spec.</li>
</ul>
<p>The containers will added to the <code>sideCars</code> will be added to the <code>Pods</code> exactly as configured.
Any configuration that is valid in a Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core">Container Spec</a>
may be added to an entry in <code>sideCars</code></p>

</div>

<h3 id="_add_an_init_container">Add an Init-Container</h3>
<div class="section">
<p>Just like normal containers above, additional init-containers can also be added to the <code>Pods</code>.
To add an init-container to the <code>Pods</code> specify the container in the <code>initContainers</code> list in the <code>Coherence</code> CRD spec.
As with containers, for init-containers any configuration that is valid in a Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core">Container Spec</a>
may be added to an entry in <code>initContainers</code></p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  initContainers:
    - name: setup                   <span class="conum" data-value="1" />
      image: "app-setup:1.0.0"</pre>

<ul class="colist">
<li data-value="1">An additional init-container named <code>setup</code> has been added to the CRD spec with an image named <code>app-setup:1.0.0</code>.</li>
</ul>
</div>
</div>
</doc-view>
<!-- pages/other/090_pod_scheduling.js -->
<doc-view>

<h2 id="_configure_pod_scheduling">Configure Pod Scheduling</h2>
<div class="section">
<p>In Kubernetes <code>Pods</code> can be configured to control how, and onto which nodes, Kubernetes will schedule those <code>Pods</code>; the
Coherence Operator allows the same control for <code>Pods</code> owned by a <code>Coherence</code> resource.</p>

<p>The following settings can be configured:</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>nodeSelector</code></td>
<td class=""><code>nodeSelector</code> is the simplest recommended form of node selection constraint.
<code>nodeSelector</code> is a field of role spec, it specifies a map of key-value pairs.
For the <code>Pod</code> to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels
(it can have additional labels as well).
See <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">Assigning Pods to Nodes</a> in the
Kubernetes documentation</td>
</tr>
<tr>
<td class=""><code>affinity</code></td>
<td class="">The affinity/anti-affinity feature, greatly expands the types of constraints you can express over just using labels
in a <code>nodeSelector</code>.
See <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">Assigning Pods to Nodes</a> in the
Kubernetes documentation</td>
</tr>
<tr>
<td class=""><code>tolerations</code></td>
<td class=""><code>nodeSelector</code> and <code>affinity</code> are properties of <code>Pods</code> that attracts them to a set of nodes (either as a preference or
a hard requirement). Taints are the opposite  they allow a node to repel a set of <code>Pods</code>.
Taints and tolerations work together to ensure that <code>Pods</code> are not scheduled onto inappropriate nodes.
One or more taints are applied to a node; this marks that the node should not accept any <code>Pods</code> that do not tolerate
the taints. Tolerations are applied to <code>Pods</code>, and allow (but do not require) the <code>Pods</code> to schedule onto nodes with
matching taints.
See <a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations</a> in the Kubernetes
documentation.</td>
</tr>
</tbody>
</table>
</div>
<p>The <code>nodeSelector</code>, <code>affinity</code> and <code>tolerations</code> fields are all part of the <code>Coherence</code> CRD spec.
The format of the fields is that same as documented in the Kubernetes documentation
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">Assigning Pods to Nodes</a> and
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations</a></p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  tolerations:
    - key: "example-key"
      operator: "Exists"
      effect: "NoSchedule"
  nodeSelector:
    disktype: ssd
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
             - key: kubernetes.io/e2e-az-name
               operator: In
               values:
                 - e2e-az1
                 - e2e-az2</pre>

</div>
</doc-view>
<!-- pages/other/100_resources.js -->
<doc-view>

<h2 id="_container_resource_limits">Container Resource Limits</h2>
<div class="section">
<p>When creating a <code>Coherence</code> resource you can optionally specify how much CPU and memory (RAM) each Coherence Container
is allowed to consume. The container resources are specified in the <code>resources</code> section of the <code>Coherence</code> spec;
the format is exactly the same as documented in the Kubernetes documentation
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">Managing Compute Resources for Containers</a>.</p>

<div class="admonition warning">
<p class="admonition-inline">When setting resource limits, in particular memory limits, for a container it is important to ensure that the
Coherence JVM is properly configured so that it does not consume more memory than the limits. If the JVM attempts to
consume more memory than the resource limits allow the <code>Pod</code> can be killed by Kubernetes.
See <router-link to="/jvm/050_memory">Configuring the JVM Memory</router-link> for details on the different memory settings.</p>
</div>
<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  resources:           <span class="conum" data-value="1" />
    requests:
      memory: "64Mi"
      cpu: "250m"
    limits:
      memory: "128Mi"
      cpu: "500m"</pre>

<ul class="colist">
<li data-value="1">The <code>coherence</code> container in the <code>Pods</code> has a request of 0.25 cpu and 64MiB of memory.
The <code>coherence</code> container has a limit of 0.5 cpu and 128MiB of memory.</li>
</ul>
</div>
</doc-view>
<!-- pages/other/110_readiness.js -->
<doc-view>

<h2 id="_readiness_liveness_probes">Readiness &amp; Liveness Probes</h2>
<div class="section">
<p>The Coherence Operator injects a Readiness/Liveness endpoint into the Coherence container that is used as the default
readiness and liveness check for the <code>Pods</code> deployed by the operator.
This endpoint is suitable for most use-cases, but it is possible to configure a different readiness and liveness probe,
or just change the timings of the probes if required.</p>

<p>The readiness/liveness probe used by the Operator in the Coherence Pods checks a number of things to determine whether the Pods is ready, one of these is whether the JVM is a cluster member.
If your application uses a custom main class and is not properly bootstrapping Coherence then the Pod will not be ready until your application code actually touches a Coherence resource causing Coherence to start and join the cluster.</p>

<p>When running in clusters with the Operator using custom main classes it is advisable to properly bootstrap Coherence
from within your <code>main</code> method. This can be done using the new Coherence bootstrap API available from CE release 20.12
or by calling <code>com.tangosol.net.DefaultCacheServer.startServerDaemon().waitForServiceStart();</code></p>


<h3 id="_coherence_readiness">Coherence Readiness</h3>
<div class="section">
<p>The default endpoint used by the Operator for readiness checks that the <code>Pod</code> is a member of the Coherence cluster and
that none of the partitioned cache services have a StatusHA value of <code>endangered</code>.
If the <code>Pod</code> is the only cluster member at the time of the ready check the StatusHA check will be skipped.
If a partitioned service has a backup count of zero the StatusHA check will be skipped for that service.</p>

<p>There are scenarios where the StatusHA check can fail but should be ignored because the application does not care
about data loss for caches on that particular cache service. Normally in this case the backup count for the cache
service would be zero, and the service would automatically be skipped in the StatusHA test.</p>

<p>The ready check used by the Operator can be configured to skip the StatusHA test for certain services.
In the <code>Coherence</code> CRD the <code>coherence.allowEndangeredForStatusHA</code> is a list of string values that can be
set to the names of partitioned cache services that should not be included in the StatusHA check.
For a service to be skipped its name must exactly match one of the names in the <code>allowEndangeredForStatusHA</code> list.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  coherence:
    allowEndangeredForStatusHA:   <span class="conum" data-value="1" />
      - TempService</pre>

<ul class="colist">
<li data-value="1">The <code>allowEndangeredForStatusHA</code> field is a list of string values. In this case the <code>TempService</code> will not
be checked for StatusHA in the ready check.</li>
</ul>
</div>

<h3 id="_configure_readiness">Configure Readiness</h3>
<div class="section">
<p>The <code>Coherence</code> CRD <code>spec.readinessProbe</code> field is identical to configuring a readiness probe for a <code>Pod</code>
in Kubernetes; see <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness &amp; Readiness</a></p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  readinessProbe:
    httpGet:
      port: 8080
      path: "/ready"
    timeoutSeconds: 60
    initialDelaySeconds: 300
    periodSeconds: 120
    failureThreshold: 10
    successThreshold: 1</pre>

<p>The example above configures a http probe for readiness and sets different timings for the probe.
The <code>Coherence</code> CRD supports the other types of readiness probe too, <code>exec</code> and <code>tcpSocket</code>.</p>

</div>

<h3 id="_configure_liveness">Configure Liveness</h3>
<div class="section">
<p>The <code>Coherence</code> CRD <code>spec.livenessProbe</code> field is identical to configuring a liveness probe for a <code>Pod</code>
in Kubernetes; see <a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness &amp; Readiness</a></p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  livenessProbe:
    httpGet:
      port: 8080
      path: "/live"
    timeoutSeconds: 60
    initialDelaySeconds: 300
    periodSeconds: 120
    failureThreshold: 10
    successThreshold: 1</pre>

<p>The example above configures a http probe for liveness and sets different timings for the probe.
The <code>Coherence</code> CRD supports the other types of readiness probe too, <code>exec</code> and <code>tcpSocket</code>.</p>

</div>
</div>
</doc-view>
<!-- pages/performance/010_performance.js -->
<doc-view>

<h2 id="_performance_testing_in_kubernetes">Performance Testing in Kubernetes</h2>
<div class="section">
<p>Many customers use Coherence because they want access to data at in-memory speeds. Customers who want the best performance from their application typically embark on performance testing and load testing of their application. When doing this sort of testing on Kubernetes, it is useful to understand the ways that your Kubernetes environment can impact your test results.</p>

</div>

<h2 id="_where_are_your_nodes">Where are your Nodes?</h2>
<div class="section">
<p>When an application has been deployed into Kubernetes, pods will typically be distributed over many nodes in the Kubernetes cluster.
When deploying into Kubernetes cluster in the cloud, for example on Oracle OKE, the nodes can be distributed across different availability zones. These zones are effectively different data centers, meaning that the network speed can differ considerable between nodes in different zones.
Performance testing in this sort of environment can be difficult if you use default Pod scheduling. Different test runs could distribute Pods to different nodes, in different zones, and skew results depending on how "far" test clients and servers are from each other.
For example, when testing a simple Coherence <code>EntryProcessor</code> invocation in a Kubernetes cluster spread across zones, we saw the 95% response time when the client and server were in the same zone was 0.1 milli-seconds. When the client and server were in different zones, the 95% response time could be as high as 0.8 milli-seconds. This difference is purely down to the network distance between nodes. Depending on the actual use-cases being tested, this difference might not have much impact on overall response times, but for simple operations it can be a significant enough overhead to impact test results.</p>

<p>The solution to the issue described above is to use Pod scheduling to fix the location of the Pods to be used for tests. In a cluster like Oracle OKE, this would ensure all the Pods will be scheduled into the same availability zone.</p>


<h3 id="_finding_node_zones">Finding Node Zones</h3>
<div class="section">
<p>This example is going to talks about scheduling Pods to a single availability zone in a Kubernetes cluster in the cloud. Pod scheduling in this way uses Node labels, and in fact any label on the Nodes in your cluster could be used to fix the location of the Pods.</p>

<p>To schedule all the Coherence Pods into a single zone we first need to know what zones we have and what labels have used.
The standard Kubernetes Node label for the availability zone is <code>topology.kubernetes.io/zone</code> (as documented in the <a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/labels-annotations-taints/">Kubernetes Labels Annotations and Taints</a> documentation). To slightly confuse the situation, prior to Kubernetes 1.17 the label was <code>failure-domain.beta.kubernetes.io/zone</code>, which has now been deprecated. Some Kubernetes clusters, even after 1.17, still use the deprecated label, so you need to know what labels your Nodes have.</p>

<p>Run the following command so list the nodes in a Kubernetes cluster with the value of the two zone labels for each node:</p>

<pre
lang="bash"

>kubectl get nodes -L topology.kubernetes.io/zone,failure-domain.beta.kubernetes.io/zone</pre>

<p>The output will be something like this:</p>

<pre


>NAME      STATUS   ROLES   AGE   VERSION   ZONE             ZONE
node-1    Ready    node    66d   v1.19.7   US-ASHBURN-AD-1
node-2    Ready    node    66d   v1.19.7   US-ASHBURN-AD-2
node-3    Ready    node    66d   v1.19.7   US-ASHBURN-AD-3
node-4    Ready    node    66d   v1.19.7   US-ASHBURN-AD-2
node-5    Ready    node    66d   v1.19.7   US-ASHBURN-AD-3
node-6    Ready    node    66d   v1.19.7   US-ASHBURN-AD-1</pre>

<p>In the output above the first <code>Zone</code> column has values, and the second does not. This means that the zone label used is the first in the label list in our <code>kubectl</code> command, i.e., <code>topology.kubernetes.io/zone</code>.</p>

<p>If the nodes had been labeled with the second, deprecated, label in the <code>kubectl</code> command list <code>failure-domain.beta.kubernetes.io/zone</code> the output would look like this:</p>

<pre


>NAME      STATUS   ROLES   AGE   VERSION   ZONE   ZONE
node-1    Ready    node    66d   v1.19.7          US-ASHBURN-AD-1
node-2    Ready    node    66d   v1.19.7          US-ASHBURN-AD-2
node-3    Ready    node    66d   v1.19.7          US-ASHBURN-AD-3
node-4    Ready    node    66d   v1.19.7          US-ASHBURN-AD-2
node-5    Ready    node    66d   v1.19.7          US-ASHBURN-AD-3
node-6    Ready    node    66d   v1.19.7          US-ASHBURN-AD-1</pre>

<p>From the list of nodes above we can see that there are three zones, <code>US-ASHBURN-AD-1</code>, <code>US-ASHBURN-AD-2</code> and <code>US-ASHBURN-AD-3</code>.
In this example we will schedule all the Pods to zome <code>US-ASHBURN-AD-1</code>.</p>

</div>

<h3 id="_scheduling_pods_of_a_coherence_cluster">Scheduling Pods of a Coherence Cluster</h3>
<div class="section">
<p>The <code>Coherence</code> CRD supports a number of ways to schedule Pods, as described in the <router-link to="/other/090_pod_scheduling">Configure Pod Scheduling</router-link> documentation. Using node labels is the simplest of the scheduling methods.
In this case we need to schedule Pods onto nodes that have the label <code>topology.kubernetes.io/zone=US-ASHBURN-AD-1</code>.
In the <code>Coherence</code> yaml we use the <code>nodeSelector</code> field.</p>

<pre
lang="yaml"
title="coherence-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: storage
spec:
  replicas: 3
  nodeSelector:
    topology.kubernetes.io/zone: US-ASHBURN-AD-1</pre>

<p>When the yaml above is applied, a cluster of three Pods will be created, all scheduled onto nodes in the <code>US-ASHBURN-AD-1</code> availability zone.</p>

</div>

<h3 id="_other_performance_factors">Other Performance Factors</h3>
<div class="section">
<p>Depending on the Kubernetes cluster you are using there could be various other factors to bear in mind. Many Kubernetes clusters run on virtual machines, which can be poor for repeated performance comparisons unless you know what else might be running on the underlying hardware that the VM is on. If a test run happens at the same time as another VM is consuming a lot of the underlying hardware resource this can obviously impact the results. Unfortunately bear-metal hardware, the best for repeated performance tests, is not always available, so it is useful to bear this in mind if there is suddenly a strange outlier in the tests.</p>

</div>
</div>
</doc-view>
<!-- pages/ports/010_overview.js -->
<doc-view>

<h2 id="_overview">Overview</h2>
<div class="section">
<p>Almost every application deployed into a Kubernetes cluster needs to communicate with other processes to provide services
to other processes or consume services to other processes. This is achieved by exposing ports on containers in <code>Pods</code> and
optionally exposing those same ports using <code>Services</code> and ingress.
The <code>Coherence</code> CRD spec makes it simple to add ports to the Coherence container and configure <code>Services</code> to
expose those ports.</p>

<p>Each additional port configured is exposed via its own <code>Service</code>.</p>

<p>If the configuration of <code>Services</code> for ports provided by the <code>Coherence</code> CRD spec is not sufficient or cannot
provide the required <code>Service</code> configuration then it is always possible to just create your own <code>Services</code> in Kubernetes.</p>


<h3 id="_guides_to_adding_and_exposing_ports">Guides to Adding and Exposing Ports</h3>
<div class="section">
<v-layout row wrap class="mb-5">
<v-flex xs12>
<v-container fluid grid-list-md class="pa-0">
<v-layout row wrap class="pillars">
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/ports/020_container_ports"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Adding Ports</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding additional container ports to the Coherence container.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/ports/030_services"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Expose Ports via Services</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Configuring Services used to expose ports.</p>
</v-card-text>
</v-card>
</v-flex>
<v-flex xs12 sm4 lg3>
<v-card>
<router-link to="/ports/040_servicemonitors"><div class="card__link-hover"/>
</router-link>
<v-card-title primary class="headline layout justify-center">
<span style="text-align:center">Prometheus ServiceMonitors</span>
</v-card-title>
<v-card-text class="caption">
<p></p>
<p>Adding Prometheus ServiceMonitors to expose ports to be scraped for metrics.</p>
</v-card-text>
</v-card>
</v-flex>
</v-layout>
</v-container>
</v-flex>
</v-layout>
</div>
</div>
</doc-view>
<!-- pages/ports/020_container_ports.js -->
<doc-view>

<h2 id="_additional_container_ports">Additional Container Ports</h2>
<div class="section">
<p>Except for rare cases most applications deployed into a Kubernetes cluster will need to expose ports that
they provide services on to other applications.
This is covered in the Kubernetes documentation,
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">Connect Applications with Services</a></p>

<p>The <code>Coherence</code> CRD makes it simple to expose ports and configure their services.
The CRD contains a field named <code>ports</code>, which is an array of named ports.
In the most basic configuration the only required values are the name and port to expose, for example:</p>

<pre
lang="yaml"
title="test-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest  <span class="conum" data-value="1" />
      port: 8080</pre>

<ul class="colist">
<li data-value="1">This example exposes a single port named <code>rest</code> on port <code>8080</code>.</li>
</ul>
<p>When the example above is deployed the Coherence Operator will add configure the ports for the
Coherence container in the <code>Pods</code> to expose that port and will also create a <code>Service</code> for the port.</p>

<p>For example, the relevant snippet of the <code>StatefulSet</code> configuration would be:</p>

<pre
lang="yaml"

>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: test-cluster
spec:
  template:
    spec:
      containers:
      - name: coherence
        ports:
          - name: rest           <span class="conum" data-value="1" />
            containerPort: 8080  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The Operator has added the <code>rest</code> port to the <code>coherence</code> containers port list.
The <code>name</code> field in the <code>Coherence</code> CRD&#8217;s port spec maps to the <code>name</code> field in the Container port spec.</li>
<li data-value="2">The <code>port</code> field in the <code>Coherence</code> CRD&#8217;s port spec maps to the <code>containerPort</code> in the Container port spec.</li>
</ul>
<p>For each additional port the Operator will create a <code>Service</code> of tyep <code>ClusterIP</code> with a default configuration.
The name of the service will be the <code>Coherence</code> resource&#8217;s name with the port name appended to it,
so in this case it will be <code>test-cluster-rest</code>. The <code>Service</code> might look like this:</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Service
metadata:
  name: test-cluster-rest                 <span class="conum" data-value="1" />
spec:
  ports:
    - name: rest                          <span class="conum" data-value="2" />
      port: 8080                          <span class="conum" data-value="3" />
      targetPort: rest                    <span class="conum" data-value="4" />
  type: ClusterIP                         <span class="conum" data-value="5" />
  selector:
    coherenceDeployment: test-cluster     <span class="conum" data-value="6" />
    coherenceCluster: test-cluster
    coherenceRole: storage
    coherenceComponent: coherencePod</pre>

<ul class="colist">
<li data-value="1">The <code>Service</code> name will be automatically generated (this can be overridden).</li>
<li data-value="2">The <code>ports</code> section will have just the single port being exposed by this service with the same name as the port.</li>
<li data-value="3">The <code>port</code> exposed by the <code>Service</code> will be the same as the container port value (this can be overridden).</li>
<li data-value="4">The target port will be set to the port being exposed from the container.</li>
<li data-value="5">The default <code>Service</code> type is <code>ClusterIP</code> (this can be overridden).</li>
<li data-value="6">A selector will be created to match the <code>Pods</code> in the <code>Coherence</code> resource.</li>
</ul>
<p>The <code>Coherence</code> CRD spec allows port and service to be further configured and allows a
Prometheus <code>ServiceMonitor</code> to be created for the port if that port is to expose metrics.</p>

<p>See also:</p>

<ul class="ulist">
<li>
<p><router-link to="/ports/030_services">Configure Services for Ports</router-link></p>

</li>
<li>
<p><router-link to="/ports/040_servicemonitors">Prometheus ServiceMonitors</router-link></p>

</li>
</ul>

<h3 id="_metrics_management_ports">Metrics &amp; Management Ports</h3>
<div class="section">
<p>Exposing the Coherence metrics port or Coherence Management over REST port are treated as a special case in the
configuration. Normally both the port&#8217;s <code>name</code> and <code>port</code> value are required fields. If the port name is <code>metrics</code>
or <code>management</code> the Operator already knows the <code>port</code> values (either from the defaults or from the metrics or
management configuration) so these do not need to be specified again.</p>

<p>For example, if the <code>Coherence</code> resource above also exposed Coherence metrics and management it might look like this:</p>

<pre
lang="yaml"
title="test-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  coherence:
    metrics:
      enabled: true
      port: 9876
    management:
      enabled: true
      port: 1234
  ports:
    - name: rest         <span class="conum" data-value="1" />
      port: 8080
    - name: metrics      <span class="conum" data-value="2" />
    - name: management   <span class="conum" data-value="3" /></pre>

<ul class="colist">
<li data-value="1">The <code>rest</code> port is not a special case and must have a port defined, in this case <code>8080</code>.</li>
<li data-value="2">The <code>metrics</code> port is exposed, but the port is not required as the Operator already knows the port value,
which is configured in the <code>coherence.metrics</code> section to be 9876.</li>
<li data-value="3">The <code>management</code> port is exposed, but the port is not required as the Operator already knows the port value,
which is configured in the <code>coherence.management</code> section to be 1234.</li>
</ul>
<p>If the port value is not set in <code>coherence.metrics.port</code> or in <code>coherence.management.port</code> then the Operator will
use the defaults for these values, 9612 for metrics and 30000 for management.</p>

</div>
</div>

<h2 id="_configuring_the_port">Configuring the Port</h2>
<div class="section">
<p>The only mandatory fields when adding a port to a <code>Coherence</code> resource are the name and port number.
There are a number of optional fields, which when not specified use the Kubernetes default values.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest
      port: 8080
      protocol: TCP
      hostIP: 10.10.1.19
      hostPort: 1000
      nodePort: 5000</pre>

<p>The additional fields, <code>protocol</code>, <code>hostIP</code>, <code>hostPort</code> have the same meaning and same defaults in the
<code>Coherence</code> CRD port spec as they have in a Kubernetes container port
(see the Kubernetes <a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#containerport-v1-core">ContainerPort</a> API reference).
These fields map directly from the <code>Coherence</code> CRD port spec to the container port spec.</p>

<p>The example above would create a container port shown below:</p>

<pre
lang="yaml"

>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: test-cluster
spec:
  template:
    spec:
      containers:
      - name: coherence
        ports:
          - name: rest
            containerPort: 8080
            protocol: TCP
            hostIP: 10.10.1.19
            hostPort: 1000</pre>

<p>The <code>nodePort</code> field in the <code>Coherence</code> CRD port spec maps to the <code>nodePort</code> field in the <code>Service</code> port spec.
The <code>nodePort</code> is described in the Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#serviceport-v1-core">ServicePort</a> API reference.</p>

<p>The <code>Coherence</code> CRD example above with <code>nodePort</code> set would create a <code>Service</code> with the same <code>nodePort</code> value:</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Service
metadata:
  name: test-cluster-rest
spec:
  ports:
    - name: rest
      port: 8080
      targetPort: rest
      nodePort: 5000
  type: ClusterIP
  selector:
    coherenceDeployment: test-cluster
    coherenceCluster: test-cluster
    coherenceRole: storage
    coherenceComponent: coherencePod</pre>

</div>
</doc-view>
<!-- pages/ports/030_services.js -->
<doc-view>

<h2 id="_configure_services_for_ports">Configure Services for Ports</h2>
<div class="section">
<p>As described in the <router-link to="/ports/020_container_ports">Additional Container Ports</router-link> documentation,
it is possible to expose additional ports on the Coherence container in the Pods of a <code>Coherence</code> resource.
The Coherence Operator will create a <code>Service</code> to expose each additional port.
By default, the name of the service is the combination of the <code>Coherence</code> resource name and the port name
(this can default behaviour can be overridden as shown below in the <router-link to="#_override_the_service_name" @click.native="this.scrollFix('#_override_the_service_name')"></router-link> section).
The configuration of the <code>Service</code> can be altered using fields in the port spec&#8217;s <code>service</code> section.</p>

<p>For example:</p>

<pre
lang="yaml"
title="test-cluster.yaml"
>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest   <span class="conum" data-value="1" />
      port: 8080
      service: {}  <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">This example exposes a single port named <code>rest</code> on port <code>8080</code>.</li>
<li data-value="2">The <code>service</code> section of the port spec is empty so the Operator will use its default behaviour
to create a <code>Service</code> in the same namespace with the name <code>test-cluster-rest</code>.</li>
</ul>
</div>

<h2 id="_override_the_service_name">Override the Service Name</h2>
<div class="section">
<p>Sometimes it is useful to use a different name than the default for a <code>Service</code> for a port,
for example, when the port is exposing functionality that other applications want to consume on a fixed well know endpoint.
To override the generated service name with another name the <code>service.name</code> field can be set.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest
      port: 8080
      service:
        name: payments  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">By setting the <code>service.name</code> field the <code>Service</code> for this port will be named <code>payments</code>.</li>
</ul>
<p>The service for the above example would look like this:</p>

<pre
lang="yaml"

>apiVersion: v1
kind: Service
metadata:
  name: payments  <span class="conum" data-value="1" />
spec:
  ports:
    - name: rest
      port: 8080
      targetPort: rest
  type: ClusterIP
  selector:
    coherenceDeployment: test-cluster
    coherenceCluster: test-cluster
    coherenceRole: storage
    coherenceComponent: coherencePod</pre>

<ul class="colist">
<li data-value="1">The <code>Service</code> name is <code>payments</code> instead of <code>test-cluster-rest</code></li>
</ul>
</div>

<h2 id="_override_the_service_port">Override the Service Port</h2>
<div class="section">
<p>It is sometimes useful to be able to expose a service on a different port on the <code>Service</code> to that being used by the container.
One use-case for this would be where the <code>Coherence</code> deployment is providing a http service where the container
exposes the service on port <code>8080</code> whereas the <code>Service</code> can use port <code>80</code>.</p>

<p>For example, using the same example payemnts service above:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest
      port: 8080
      service:
        name: payments  <span class="conum" data-value="1" />
        port: 80        <span class="conum" data-value="2" /></pre>

<ul class="colist">
<li data-value="1">The <code>Service</code> name will be <code>payments</code></li>
<li data-value="2">The <code>Service</code> port will be <code>80</code></li>
</ul>
<p>This then allows the payments service to be accessed on a simple url of <code><a id="" title="" target="_blank" href="http://payments">http://payments</a></code></p>

</div>

<h2 id="_disable_service_creation">Disable Service Creation</h2>
<div class="section">
<p>Sometimes it may be desirable to expose a port on the Coherence container but not have the Operator automatically
create a <code>Service</code> to expose the port. For example, maybe the port is to be exposed via some other load balancer
service controlled by another system.
To disable automatic service creation set the <code>service.enabled</code> field to <code>false</code>.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest
      port: 8080
      service:
        enabled: false  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">With the <code>service.enabled</code> field set to <code>false</code> no <code>Service</code> will be created.</li>
</ul>
</div>

<h2 id="_other_service_configuration">Other Service Configuration</h2>
<div class="section">
<p>The <code>Coherence</code> resource CRD allows many other settings to be configured on the <code>Service</code>.
These fields are identical to the corresponding fields in the Kubernetes <code>Service</code> spec.</p>

<p>See the <code>Coherence</code> CRD <router-link :to="{path: '/about/04_coherence_spec', hash: '#_servicespec'}">Service Spec</router-link> documentation
and the Kubernetes
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#servicespec-v1-core">Service API reference</a>.</p>

</div>
</doc-view>
<!-- pages/ports/040_servicemonitors.js -->
<doc-view>

<h2 id="_prometheus_servicemonitors">Prometheus ServiceMonitors</h2>
<div class="section">
<p>When a port exposed on a container is to be used to serve metrics to Prometheus this often requires the addition of
a Prometheus <code>ServiceMonitor</code> resource. The Coherence Operator makes it simple to add a <code>ServiceMonitor</code> for an exposed
port. The advantage of specifying the <code>ServiceMonitor</code> configuration in the <code>Coherence</code> CRD spec is that the
<code>ServiceMonitor</code> resource will be created, updated and deleted as part of the lifecycle of the <code>Coherence</code> resource,
and does not need to be managed separately.</p>

<p>A <code>ServiceMonitor</code> is created for an exposed port by setting the <code>serviceMonitor.enabled</code> field to <code>true</code>.
The Operator will create a <code>ServiceMonitor</code> with the same name as the <code>Service</code>.
The <code>ServiceMonitor</code> created will have a single endpoint for the port being exposed.</p>

<p>For example:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  ports:
    - name: rest
      port: 8080
      serviceMonitor:
        enabled: true  <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">With the <code>serviceMonitor.enabled</code> field set to <code>true</code> a <code>ServiceMonitor</code> resource will be created.</li>
</ul>
<p>The <code>ServiceMonitor</code> created from the spec above will look like this:
For example:</p>

<pre
lang="yaml"

>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: test-cluster-rest
  labels:
    coherenceCluster: test-cluster
    coherenceComponent: coherence-service-monitor
    coherenceDeployment: test-cluster
    coherenceRole: test-cluster
spec:
  endpoints:
    - port: rest
      relabelings:
        - action: labeldrop
          regex: (endpoint|instance|job|service)
  selector:
    matchLabels:
      coherenceCluster: test-cluster
      coherenceComponent: coherence-service
      coherenceDeployment: test-cluster
      coherencePort: rest
      coherenceRole: test-cluster</pre>


<h3 id="_configure_the_servicemonitor">Configure the ServiceMonitor</h3>
<div class="section">
<p>The <code>Coherence</code> CRD <router-link :to="{path: '/about/04_coherence_spec', hash: '#_servicemonitorspec'}">ServiceMonitorSpec</router-link>
contains many of the fields from the
<a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#servicemonitorspec">Prometheus <code>ServiceMonitorSpec</code></a>
and <a id="" title="" target="_blank" href="https://coreos.com/operators/prometheus/docs/latest/api.html#endpoint">Prometheus Endpoint</a>
to allow the <code>ServiceMonitor</code> to be configured for most use-cases.</p>

<p>In situations where the <code>Coherence</code> CRD does not have the required fields, for example when a different version
of Prometheus has been installed to that used to build the Coherence Operator, then the solution would be to
manually create <code>ServiceMonitors</code> instead of letting them be created by the Coherence Operator.</p>

</div>
</div>
</doc-view>
<!-- pages/scaling/010_overview.js -->
<doc-view>

<h2 id="_scale_coherence_deployments">Scale Coherence Deployments</h2>
<div class="section">
<p>The Coherence Operator provides the ability to safely scale up and down a <code>Coherence</code> deployment.
A <code>Coherence</code> deployment is backed by a <code>StatefulSet</code>, which can easily be scaled using existing Kubernetes features.
The problem with directly scaling down the <code>StatefulSet</code> is that Kubernetes will immediately kill the required number
of <code>Pods</code>. This is obviously very bad for Coherence as killing multiple storage enabled members would almost certainly
cause data loss.</p>

<p>The Coherence Operator supports scaling by applying the scaling update directly to <code>Coherence</code> deployment rather than
to the underlying <code>StatefulSet</code>. There are two methods to scale a <code>Coherence</code> deployment:</p>

<ul class="ulist">
<li>
<p>Update the <code>replicas</code> field in the <code>Coherence</code> CRD spec.</p>

</li>
<li>
<p>Use the <code>kubectl scale</code> command</p>

</li>
</ul>
<p>When either of these methods is used the Operator will detect that a change to the size of the deployment is required
and ensure that the change will be applied safely. The logical steps the Operator will perform are:</p>

<ol style="margin-left: 15px;">
<li>
Detect desired replicas is different to current replicas

</li>
<li>
Check the cluster is StatusHA - i.e. no cache services are endangered. If any service is not StatusHA requeue the
scale request  (go back to step one).

</li>
<li>
If scaling up, add the required number of members.

</li>
<li>
If scaling down, scale down by one member and requeue the request (go back to step one).

</li>
</ol>
<p>What these steps ensure is that the deployment will not be resized unless the cluster is in a safe state.
When scaling down only a single member will be removed at a time, ensuring that the cluster is in a safe state before
removing the next member.</p>

<div class="admonition note">
<p class="admonition-inline">The Operator will only apply safe scaling functionality to deployments that are storage enabled.
If a deployment is storage disabled then it can be scaled up or down by the required number of members
in one step as there is no fear of data loss in a storage disabled member.</p>
</div>
</div>

<h2 id="_controlling_safe_scaling">Controlling Safe Scaling</h2>
<div class="section">
<p>The <code>Coherence</code> CRD has a number of fields that control the behaviour of scaling.</p>


<h3 id="_scaling_policy">Scaling Policy</h3>
<div class="section">
<p>The <code>Coherence</code> CRD spec has a field <code>scaling.policy</code> that can be used to override the default scaling
behaviour. The scaling policy has three possible values:</p>


<div class="table__overflow elevation-1  ">
<table class="datatable table">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class=""><code>ParallelUpSafeDown</code></td>
<td class="">This is the default scaling policy.
With this policy when scaling up <code>Pods</code> are added in parallel (the same as using the <code>Parallel</code> <code>podManagementPolicy</code>
in a <a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#statefulsetspec-v1-apps">StatefulSet</a>) and
when scaling down <code>Pods</code> are removed one at a time (the same as the <code>OrderedReady</code> <code>podManagementPolicy</code> for a
StatefulSet). When scaling down a check is done to ensure that the members of the cluster have a safe StatusHA value
before a <code>Pod</code> is removed (i.e. none of the Coherence cache services have an endangered status).
This policy offers faster scaling up and start-up because pods are added in parallel as data should not be lost when
adding members, but offers safe, albeit slower,  scaling down as <code>Pods</code> are removed one by one.</td>
</tr>
<tr>
<td class=""><code>Parallel</code></td>
<td class="">With this policy when scaling up <code>Pods</code> are added in parallel (the same as using the <code>Parallel</code> <code>podManagementPolicy</code>
in a <a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#statefulsetspec-v1-apps">StatefulSet</a>).
With this policy no StatusHA check is performed either when scaling up or when scaling down.
This policy allows faster start and scaling times but at the cost of no data safety; it is ideal for deployments that are
storage disabled.</td>
</tr>
<tr>
<td class=""><code>Safe</code></td>
<td class="">With this policy when scaling up and down <code>Pods</code> are removed one at a time (the same as the <code>OrderedReady</code>
<code>podManagementPolicy</code> for a StatefulSet). When scaling down a check is done to ensure that the members of the deployment
have a safe StatusHA value before a <code>Pod</code> is removed (i.e. none of the Coherence cache services have an endangered status).
This policy is slower to start, scale up and scale down.</td>
</tr>
</tbody>
</table>
</div>
<p>Both the <code>ParallelUpSafeDown</code> and <code>Safe</code> policies will ensure no data loss when scaling a deployment.</p>

<p>The policy can be set as shown below:</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    policy: Safe <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">This deployment will scale both up and down with StatusHA checks.</li>
</ul>
</div>

<h3 id="_scaling_statusha_probe">Scaling StatusHA Probe</h3>
<div class="section">
<p>The StatusHA check performed by the Operator uses a http endpoint that the Operator runs on a well-known port in the
Coherence JVM. This endpoint does performs a simple check to verify that none of the partitioned cache services known
about by Coherence have an endangered status. If an application has a different concept of what "safe" means it can
implement a different method to check the status during scaling.</p>

<p>The operator supports different types of safety check probes, these are exactly the same as those supported by
Kubernetes for readiness and liveness probes. The <code>scaling.probe</code> section of the <code>Coherence</code> CRD allows different
types of probe to be configured.</p>


<h4 id="_using_a_http_get_probe">Using a HTTP Get Probe</h4>
<div class="section">
<p>A HTTP get probe works the same way as a
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-liveness-http-request">Kubernetes liveness http request</a></p>

<p>The probe can be configured as follows</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    probe:
      httpGet:             <span class="conum" data-value="1" />
        port: 8080
        path: /statusha</pre>

<ul class="colist">
<li data-value="1">This deployment will check the status of the services by performing a http GET on <code><a id="" title="" target="_blank" href="http://&lt;pod-ip&gt;:8080/statusha">http://&lt;pod-ip&gt;:8080/statusha</a></code>.
If the response is <code>200</code> the check will pass, any other response the check is assumed to be false.</li>
</ul>
</div>

<h4 id="_using_a_tcp_probe">Using a TCP Probe</h4>
<div class="section">
<p>A TCP probe works the same way as a
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-tcp-liveness-probe">Kubernetes TCP liveness probe</a></p>

<p>The probe can be configured as follows</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    probe:
      tcpSocket:    <span class="conum" data-value="1" />
        port: 7000</pre>

<ul class="colist">
<li data-value="1">This deployment will check the status of the services by connecting to the socket on port <code>7000</code>.</li>
</ul>
</div>

<h4 id="_using_an_exec_command_probe">Using an Exec Command Probe</h4>
<div class="section">
<p>A TCP probe works the same way as a
<a id="" title="" target="_blank" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-liveness-command">Kubernetes Exec liveness probe</a></p>

<p>The probe can be configured as follows</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    probe:
      exec:
        command:      <span class="conum" data-value="1" />
          - /bin/ah
          - safe.sh</pre>

<ul class="colist">
<li data-value="1">This deployment will check the status of the services by running the <code>sh safe.sh</code> command in the <code>Pod</code>.</li>
</ul>
</div>
</div>
</div>
</doc-view>
<!-- pages/troubleshooting/01_trouble-shooting.js -->
<doc-view>

<h2 id="_troubleshooting_guide">Troubleshooting Guide</h2>
<div class="section">
<p>The purpose of this page is to list troubleshooting guides and work-arounds for issues that you may run into when using the Coherence Operator.
This paged will be updated and maintained over time to include common issues we see from customers</p>

</div>

<h2 id="_contents">Contents</h2>
<div class="section">
<ul class="ulist">
<li>
<p><router-link to="#restart" @click.native="this.scrollFix('#restart')">Why Does the Operator Pod Restart</router-link></p>

</li>
<li>
<p><router-link to="#ready" @click.native="this.scrollFix('#ready')">Why are the Coherence Pods not reaching ready</router-link></p>

</li>
<li>
<p><router-link to="#stuck-pending" @click.native="this.scrollFix('#stuck-pending')">My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down</router-link></p>

</li>
<li>
<p><router-link to="#stuck-delete" @click.native="this.scrollFix('#stuck-delete')">My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment</router-link></p>

</li>
<li>
<p><router-link to="#site-safe" @click.native="this.scrollFix('#site-safe')">My cache services will not reach Site Safe</router-link></p>

</li>
<li>
<p><router-link to="#dashboards" @click.native="this.scrollFix('#dashboards')">My Grafana Dashboards do not display any metrics</router-link></p>

</li>
</ul>
</div>

<h2 id="_issues">Issues</h2>
<div class="section">

<h3 id="restart">Why Does the Operator Pod Restart After Installation</h3>
<div class="section">
<p>You might notice that when the Operator is installed that the Operator Pod starts, dies and is then restarted.
This is expected behaviour. The Operator uses a K8s web-hook for defaulting and validation of the Coherence resources.
A web-hook requires certificates to be present in a <code>Secret</code> mounted to the Operator Pod as a volume.
If this Secret is not present the Operator creates it and populates it with self-signed certs.
In order for the Secret to then be mounted correctly the Pod must be restarted.
See the <router-link to="/webhooks/01_introduction">the WebHooks</router-link> documentation.</p>

</div>

<h3 id="ready">Why are the Coherence Pods not reaching ready</h3>
<div class="section">
<p>The readiness/liveness probe used by the Operator in the Coherence Pods checks a number of things to determine whether the Pods is ready, one of these is whether the JVM is a cluster member.
If your application uses a custom main class and is not properly bootstrapping Coherence then the Pod will not be ready until your application code actually touches a Coherence resource causing Coherence to start and join the cluster.</p>

<p>When running in clusters with the Operator using custom main classes it is advisable to properly bootstrap Coherence
from within your <code>main</code> method. This can be done using the new Coherence bootstrap API available from CE release 20.12
or by calling <code>com.tangosol.net.DefaultCacheServer.startServerDaemon().waitForServiceStart();</code></p>

</div>

<h3 id="stuck-pending">My Coherence cluster is stuck with some running Pods and some pending Pods, I want to scale down</h3>
<div class="section">
<p>If you try to create a Coherence deployment that has a replica count that is greater than your k8s cluster can actually
provision then one or more Pods will fail to be created or can be left in a pending state.
The obvious solution to this is to just scale down your Coherence deployment to a smaller size that can be provisioned.
The issue here is that the safe scaling functionality built into the operator will not allow scaling down to take place
because it cannot guarantee no parition/data loss. The Coherence deployment is now stuck in this state.</p>

<p>The simplest solution would be to completely delete the the Coherence deployment and redeploy with a lower replica count.</p>

<p>If this is not possible then the following steps will allow the deployment to be scaled down.</p>

<p>1 Update the stuck Coherence deployment&#8217;s scaling policy to be <code>Parallel</code></p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test
spec:
  scaling:
    policy: Parallel</pre>

<p>2 Scale down the cluster to the required size using whatever scaling commands you want, i.e <code>kubectl scale</code>
or just update the replica value of the Coherence deployment yaml. Note: If updating the Coherence yaml, this
should not be done as part of step 1, above.</p>

<p>3 Once the Coherence deployment has scaled to the required size then change the scaling policy value back to the
default by updating the Coherence yaml to have no scaling policy value in it.</p>

<div class="admonition warning">
<p class="admonition-inline">When using this work around to scale down a stuck deployment that contains data it is important that
only the missing or pending Pods are removed. For example if a Coherence deployment is deployed with a replica count
of 100 and 90 Pods are ready, but the other 10 are either missing or stuck pending then the replica value used in
step 2 above must be 90. Because the scaling policy has been set to <code>Parallel</code> the operator will not check any
Status HA values before scaling down Pods, so removing "ready" Pods that contain data will almost certainly result
in data loss. To safely scale down lower, then first follow the three steps above then after changing the scaling policy
back to the default further scaling down can be done as normal.</p>
</div>
</div>

<h3 id="stuck-delete">My Coherence cluster is stuck with all pending/crashing Pods, I cannot delete the deployment</h3>
<div class="section">
<p>A Coherence deployment can become stuck where none of the Pods can start, for example the image used is incorrect
and all Pods are stuck in ImagePullBackoff. It can then become impossible to delete the broken deployment.
This is because the Operator has installed a finalizer but this finalizer cannot execute.</p>

<p>For example, suppose we have deployed a Coherence deployment named <code>my-cluster</code> into namespace <code>coherence-test</code>.</p>

<p>First try to delete the deployment as normal:</p>

<pre
lang="console"

>kubectl -n coherence-test delete coherence/my-cluster</pre>

<p>If this command hangs, then press <code>ctrl-c</code> to exit and then run the following patch command.</p>

<pre
lang="console"

>kubectl -n coherence-test patch coherence/my-cluster -p '{"metadata":{"finalizers":[]}}' --type=merge</pre>

<p>This will remove the Operator&#8217;s finalizer from the Coherence deployment.</p>

<p>At this point the <code>my-cluster</code> Coherence deployment might already have been removed,
if not try the delete command again.</p>

</div>

<h3 id="site-safe">My cache services will not reach Site-Safe</h3>
<div class="section">
<p>Coherence distributes data in a cluster to achieve the highest status HA value that it can, the best being site-safe.
This is done using the various values configured for the site, rack, machine, and member names.
The Coherence Operator configures these values for the Pods in a Coherence deployment.
By default, the values for the site and rack names are taken from standard k8s labels applied to the Nodes in the k8s cluster.
If the Nodes in the cluster do not have these labels set then the site and rack names will be unset and Coherence
will not be able to reach rack or site safe.</p>

<p>There are a few possible solutions to this, see the explanation in the
documentation explaining <router-link to="/coherence/021_member_identity">Member Identity</router-link></p>

</div>

<h3 id="dashboards">My Grafana Dashboards do not display any metrics</h3>
<div class="section">
<p>If you have imported the Grafana dashboards provided by the Operator into Grafana, but they are not displaying any metric
values, it may be that you have imported the wrong format dashboards. The Operator has multiple sets of dashboards,
one for the default Coherence metric name format, one for Microprofile metric name format, and one for
<a id="" title="" target="_blank" href="https://micrometer.io">Micrometer</a> metric name format.</p>

<p>The simplest way to find out which version corresponds to your Coherence cluster
is to query the metrics endpoint with something like <code>curl</code>.
If the metric names are in the format <code>vendor:coherence_cluster_size</code>, i.e. prefixed with <code>vendor:</code> then this is
the default Coherence format.
If metric names are in the format <code>vendor_Coherence_Cluster_Size</code>, i.e. prefixed with <code>vendor_</code> then this is
Microprofile format.
If the metric name has no <code>vendor</code> prefix then it is using Micrometer metrics.</p>

<p>See: the <router-link to="/metrics/030_importing">Importing Grafana Dashboards</router-link> documentation.</p>

</div>
</div>
</doc-view>
<!-- pages/troubleshooting/02_heap_dump.js -->
<doc-view>

<h2 id="_capture_heap_dumps">Capture Heap Dumps</h2>
<div class="section">
<p>Heap dumps can be very useful when debugging but generating and downloading a heap dump from a container in Kubernetes can be tricky. When you are running minimal images without an O/S or full JDK (such as the distroless images used by JIB) this becomes even more tricky.</p>

</div>

<h2 id="_ephemeral_containers">Ephemeral Containers</h2>
<div class="section">
<p>Ephemeral containers were introduced in k8s v1.16 and at the time of writing are still in alpha.
Ephemeral containers is a feature gate that must be enabled for your cluster.
If you have the <code>EphemeralContainers</code> feature gate enabled, then obtaining a heap dump is not so difficult.</p>


<h4 id="_enable_ephemeralcontainers_in_kind">Enable EphemeralContainers in KinD</h4>
<div class="section">
<p>We use <a id="" title="" target="_blank" href="https://kind.sigs.k8s.io">KinD</a> for a lot of our CI builds and testing, enabling the <code>EphemeralContainers</code> feature gate in KinD is very easy.</p>

<p>For example, this KinD configuration enables the <code>EphemeralContainers</code> feature gate</p>

<pre
lang="yaml"

>kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  EphemeralContainers: true <span class="conum" data-value="1" />
nodes:
- role: control-plane
- role: worker
- role: worker</pre>

<ul class="colist">
<li data-value="1">The <code>EphemeralContainers</code> feature gate is set to <code>true</code></li>
</ul>
</div>

<h3 id="_shared_process_namespace">Shared Process Namespace</h3>
<div class="section">
<p>In this example we are going to use the <code>jps</code> and <code>jcmd</code> tools to generate the heap dump from an ephemeral container.
For this to work the ephemeral container must be able to see the processes running in the <code>coherence</code> container.
The <code>Coherence</code> CRD spec has a field named <code>ShareProcessNamespace</code>, which sets the corresponding field in the Coherence Pods that will be created for the deployment.</p>

<pre
lang="yaml"

>apiVersion: coherence.oracle.com/v1
kind: Coherence
metadata:
  name: test-cluster
spec:
  shareProcessNamespace: true   <span class="conum" data-value="1" /></pre>

<ul class="colist">
<li data-value="1">The <code>shareProcessNamespace</code> must be set to <code>true</code>.</li>
</ul>
<p>If you have some other way to trigger a heap dump to a specific location without requiring the ephemeral container to see the Coherence container processes then the technique below can still be used without setting <code>shareProcessNamespace</code> to <code>true</code>.</p>

</div>

<h3 id="_create_an_ephemeral_container">Create an Ephemeral Container</h3>
<div class="section">
<p>Let&#8217;s say we have a Coherence cluster deployed named <code>test-cluster</code> in a namespace named <code>coherence-test</code>.
There will be a number of Pods created for this deployment, named <code>test-cluster-0</code>, <code>test-cluster-1</code> and so on.
For this example we will obtain a heap dump from Pod <code>test-cluster-1</code>.</p>

<p>The purpose of using an ephemeral container is because the Coherence container we are running does not contain any of the tools and programs we require for debugging, e.g. <code>jps</code>, <code>jcmd</code> etc.
The ephemeral container we run obviously needs to have all the required tools. You could create a custom image with what you need in it, but for this example we will use the <code>openjdk:11</code> image, as it has a full JDK and other tools we need in it.
You should obviously use a JDK version that matches the version in the Coherence container.</p>

<p>We can use the <code>kubectl debug</code> command that can be used to create an ephemeral containers.
For our purposes we cannot use this command as we will require volume mounts to share storage between the ephemeral container and the Coherence container so that the ephemeral container can see the heap dump file.</p>

<p>Instead of the <code>kubectl debug</code> command we can create ephemeral containers using the <code>kubectl --raw</code> API.
Ephemeral containers are a sub-resource of the Pod API.</p>

<ul class="ulist">
<li>
<p>First obtain the current ephemeral containers sub-resource for the Pod.
We do this using the <code>kubectl get --raw</code> command with the URL path in the format <code>/api/v1/namespaces/&lt;namespace&gt;&gt;/pods/&lt;pod&gt;/ephemeralcontainers</code>, where <code>&lt;namespace&gt;</code> is the namespace that the Pod is deployed into and <code>&lt;pod&gt;</code> is the name of the Pod.</p>

</li>
</ul>
<p>So in our example the command would be:</p>

<pre
lang="bash"

>kubectl get --raw /api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers</pre>

<p>Which will output json similar to this, which we will save to a file named <code>ec.json</code>:</p>

<pre
lang="json"
title="ec.json"
>{
  "kind": "EphemeralContainers",
  "apiVersion": "v1",
  "metadata": {
    "name": "test-cluster-1",
    "namespace": "coherence-test",
    "selfLink": "/api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers",
    "uid": "731ca9a9-332f-4999-821d-adfea2e1d2d4",
    "resourceVersion": "24921",
    "creationTimestamp": "2021-03-12T10:41:35Z"
  },
  "ephemeralContainers": []
}</pre>

<p>The <code>"ephemeralContainers"</code> field is an empty array as we have not created any previous containers.</p>

<p>We now need to edit this yaml to define the ephemeral container we want to create.
The Pod created by the Operator contains an empty directory volume with a volume mount at <code>/coherence-operator/jvm</code>, which is where the JVM is configured to dump debug information, such as heap dumps.
We will create an ephemeral container with the same mount so that the <code>/coherence-operator/jvm</code> directory will be shared between the Coherence container and the ephemeral container.</p>

<p>Another thing to note is that the default entrypoint in the <code>openjdk:11</code> image we are using in this example is JShell.
This is obviously not what we want, so we will make sure we specify <code>/bin/sh</code> as the entry point as we want a command line shell.</p>

<p>Our edited <code>ec.json</code> file looks like this:</p>

<pre
lang="json"
title="ec.json"
>{
  "kind": "EphemeralContainers",
  "apiVersion": "v1",
  "metadata": {
    "name": "test-cluster-1",
    "namespace": "coherence-test",
    "selfLink": "/api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers",
    "uid": "731ca9a9-332f-4999-821d-adfea2e1d2d4",
    "resourceVersion": "24921",
    "creationTimestamp": "2021-03-12T10:41:35Z"
  },
  "ephemeralContainers": [
    {
      "name": "debug",                                 <span class="conum" data-value="1" />
      "image": "openjdk:11",                           <span class="conum" data-value="2" />
      "command": [
          "bin/sh"                                     <span class="conum" data-value="3" />
      ],
      "imagePullPolicy": "IfNotPresent",               <span class="conum" data-value="4" />
      "terminationMessagePolicy":"File",
      "stdin": true,                                   <span class="conum" data-value="5" />
      "tty": true,
      "volumeMounts": [
          {
              "mountPath": "/coherence-operator/jvm",  <span class="conum" data-value="6" />
              "name": "jvm"
          }
      ]
    }
  ]
}</pre>

<ul class="colist">
<li data-value="1">We add an ephemeral container named <code>debug</code>. The name can be anything as long as it is unique in the Pod.</li>
<li data-value="2">We specify that the image used for the container is <code>openjdk:11</code></li>
<li data-value="3">Specify <code>/bin/sh</code> as the container entry point so that we get a command line shell</li>
<li data-value="4">We must specify an image pull policy</li>
<li data-value="5">We want an interactive container, so we specify <code>stdin</code> and <code>tty</code></li>
<li data-value="6">We create the same volume mount to <code>/coherence-operator/jvm</code> that the Coherence container has.</li>
</ul>
<p>We can now re-apply the json to add the new ephemeral container using the <code>kubectl replace --raw</code> command to the same URL path we used for the <code>get</code> command above, this time using <code>-f ec.json</code> to specify the json we want to replace.</p>

<pre
lang="bash"

>kubectl replace --raw /api/v1/namespaces/coherence-test/pods/test-cluster-1/ephemeralcontainers -f ec.json</pre>

<p>After executing the above command the ephemeral container should have been created, we can now attach to it.</p>

</div>

<h3 id="_attach_to_the_ephemeral_container">Attach to the Ephemeral Container</h3>
<div class="section">
<p>We now have an ephemeral container named <code>debug</code> in the Pod <code>test-cluster-1</code>.
We need to attach to the container so that we can create the heap dump.</p>

<pre
lang="bash"

>kubectl attach test-cluster-1 -c debug -it -n coherence-test</pre>

<p>The command above will attach an interactive (<code>-it</code>) session to the <code>debug</code> container (specified with <code>-c debug</code>) in Pod <code>test-cluster-1</code>, in the namespace <code>coherence-test</code>.
Displaying something like this:</p>

<pre
lang="bash"

>If you don't see a command prompt, try pressing enter.

#</pre>

</div>

<h3 id="_trigger_the_heap_dump">Trigger the Heap Dump</h3>
<div class="section">
<p>We can now generate the heap dump for the Coherence process using <code>jcmd</code>, but first we need to find its PID using <code>jps</code>.</p>

<pre
lang="bash"

>jps -l</pre>

<p>Which will display something like this:</p>

<pre
lang="bash"

>117 jdk.jcmd/sun.tools.jps.Jps
55 com.oracle.coherence.k8s.Main</pre>

<p>The main class run by the Operator is <code>com.oracle.coherence.k8s.Main</code> so the PID of the Coherence process is <code>55</code>.
We can now use <code>jcmd</code> to generate the heap dump. We need to make sure that the heap dump is created in the <code>/coherence-operator/jvm/</code> directory, as this is shared between both containers.</p>

<pre
lang="bash"

>jcmd 55 GC.heap_dump /coherence-operator/jvm/heap-dump.hprof</pre>

<p>After running the command above, we will have a heap dump file that we can access from the ephemeral <code>Pod</code>.
We have a number of choices about how to get the file out of the Pod and somewhere that we can analyze it.
We could use <code>sftp</code> to ship it somewhere, or some tools to copy it to cloud storage or just simply use <code>kubectl cp</code> to copy it.</p>

<div class="admonition note">
<p class="admonition-inline">Do not exit out of the ephemeral container session until you have copied the heap dump.</p>
</div>
<p>The <code>kubectl cp</code> command is in the form <code>kubectl cp &lt;namespace&gt;/&lt;pod&gt;/&lt;file&gt; &lt;local-file&gt; -c &lt;container&gt;</code>.
So to use <code>kubectl cp</code> we can execute a command like the following:</p>

<pre
lang="bash"

>kubectl cp coherence-test/test-cluster-1:/coherence-operator/jvm/heap-dump.hprof \
    $(pwd)/heap-dump.hprof -c debug</pre>

<p>We will now have a file called <code>heap-dump.hprof</code> in the current directory.
We can now exit out of the ephemeral container.</p>

</div>
</div>
</doc-view>
<!-- pages/webhooks/01_introduction.js -->
<doc-view>

<h2 id="_coherence_operator_kubernetes_web_hooks">Coherence Operator Kubernetes Web-Hooks</h2>
<div class="section">
<p>The Coherence Operator uses Kubernetes admission control webhooks to validate and provide default values for
Coherence resources
(see the <a id="" title="" target="_blank" href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">Kubernetes documentation</a>
for more details on web-hooks).</p>

<p>The Coherence Operator webhooks will validate a <code>Coherence</code> resources when it is created or updated contain.
For example, the <code>replicas</code> count is not negative. If a <code>Coherence</code> resource is invalid it will be rejected before it
gets stored into Kubernetes.</p>


<h3 id="_webhook_scope">Webhook Scope</h3>
<div class="section">
<p>Webhooks in Kubernetes are a cluster resource, not a namespaced scoped resource, so consequently there is typically only
a single webhook installed for a given resource type. If the Coherence Operator is installed as a cluster scoped operator
then this is not a problem but if multiple Coherence Operators are deployed then they could all attempt to install the
webhooks and update or overwrite a previous configuration. This might not be an issue if all of the operators deployed
in a Kubernetes cluster are the same version but different versions could cause issues.</p>

</div>
</div>

<h2 id="_webhook_certificates">Webhook Certificates</h2>
<div class="section">
<p>Kubernetes requires webhooks to expose an API over https and consequently this requires certificates to be created.
By default, the Coherence Operator will create a self-signed CA certificate and key for use with its webhooks.
Alternatively it is possible to use an external certificate manager such as the commonly used
<a id="" title="" target="_blank" href="https://github.com/jetstack/cert-manager">Cert Manager</a>.
Configuring and using Cert Manager is beyond the scope of this documentation.</p>

</div>
</doc-view>
